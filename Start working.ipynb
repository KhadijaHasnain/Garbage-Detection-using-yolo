{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1556326,"sourceType":"datasetVersion","datasetId":918769}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"L4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"909c0fdb959d40328d3724326e57cb67":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_cde69577ceab4dabaff29253c3e75344","IPY_MODEL_5be9d30694b24e339516f6dcbf62d850"],"layout":"IPY_MODEL_b72ae68a4fc44485bbf14d450d97b208"}},"cde69577ceab4dabaff29253c3e75344":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a18c78746ecf4eb299ce4d98ef8e6291","placeholder":"â€‹","style":"IPY_MODEL_eb96752987ed44479393043601c5d77a","value":"0.016 MB of 0.016 MB uploaded\r"}},"5be9d30694b24e339516f6dcbf62d850":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8cc742c44fe4fdf8e5a7b62f9af3f7c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e82dc19fbc634ae788e261c0b3d36ede","value":1}},"b72ae68a4fc44485bbf14d450d97b208":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a18c78746ecf4eb299ce4d98ef8e6291":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb96752987ed44479393043601c5d77a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8cc742c44fe4fdf8e5a7b62f9af3f7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e82dc19fbc634ae788e261c0b3d36ede":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image, ImageFile\nfrom collections import Counter\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport wandb  # Import wandb\nfrom transformers import BertTokenizer\n\n# Login with the API KEY\nwandb.login(key=\"ab35ea8191eba471c2b58a844910531625b00550\")\n\n# Initialize wandb\nwandb.init(project=\"Working wandb\", entity=\"mblogge785-work\")\n\n# Initialize the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Define configuration for YOLOv3\nconfig = [\n    (32, 3, 1),\n    (128, 3, 1),\n    (64, 3, 2),\n    [\"list\", 1],\n    (128, 3, 2),\n    [\"list\", 2],\n    (256, 3, 2),\n    [\"list\", 8],\n    (512, 3, 2),\n    [\"list\", 8],\n    (1024, 3, 2),\n    [\"list\", 4],\n    (512, 1, 1),\n    (1024, 3, 1),\n    \"sp\",\n    (256, 1, 1),\n    \"up\",\n    (256, 1, 1),\n    (512, 3, 1),\n    \"sp\",\n    (128, 1, 1),\n    \"up\",\n    (128, 1, 1),\n    (256, 3, 1),\n    \"sp\",\n]\n\n# Define constants\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nNumClasses = 20\nImageSize = 416\nRateOfLearning = 1e-5\nepochsno = 150\nThresholdConf = 0.8\nThreshMap = 0.5\nThreshNms = 0.45\nSizeOfBatch = 32\nDirImage = \"/kaggle/input/pascalvoc-yolo/images\"\nDirLable = \"/kaggle/input/pascalvoc-yolo/labels\"\n\n# Define anchors and classes\nANCHORS = [\n    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n]\n\nAllClacess = [\n    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\",\n    \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n]\n\n# Define model hyperparameters\nENC_EMB_DIM = 512\nDEC_EMB_DIM = 512\nHID_DIM = 1024\nN_LAYERS = 2\nENC_DROPOUT = 0.3\nDEC_DROPOUT = 0.3\n\nwandb.config.update({\n    \"learning_rate\": 1e-3,\n    \"epochs\": 30,\n    \"batch_size\": 64,\n    \"encoder_embedding_dim\": ENC_EMB_DIM,\n    \"decoder_embedding_dim\": DEC_EMB_DIM,\n    \"hidden_dim\": HID_DIM,\n    \"num_layers\": N_LAYERS,\n    \"encoder_dropout\": ENC_DROPOUT,\n    \"decoder_dropout\": DEC_DROPOUT\n})\n\nclass CNN_Block(nn.Module):\n    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n        super(CNN_Block, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.leaky = nn.LeakyReLU(0.1)\n        self.use_bn_act = bn_act\n\n    def forward(self, x):\n        if self.use_bn_act:\n            return self.leaky(self.bn(self.conv(x)))\n        else:\n            return self.conv(x)\n\nclass Residual_Block(nn.Module):\n    def __init__(self, channels, use_residual=True, num_repeats=1):\n        super(Residual_Block, self).__init__()\n        self.layers = nn.ModuleList()\n        for repeat in range(num_repeats):\n            self.layers += [\n                nn.Sequential(\n                    CNN_Block(channels, channels//2, kernel_size=1),\n                    CNN_Block(channels//2, channels, kernel_size=3, padding=1)\n                )\n            ]\n        self.use_residual = use_residual\n        self.num_repeats = num_repeats\n\n    def forward(self, x):\n        for layer in self.layers:\n            if self.use_residual:\n                x = x + layer(x)\n            else:\n                x = layer(x)\n        return x\n\nclass Prediction_Scale(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super(Prediction_Scale, self).__init__()\n        self.pred = nn.Sequential(\n            CNN_Block(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n            CNN_Block(2 * in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1),\n        )\n        self.num_classes = num_classes\n\n    def forward(self, x):\n        return (\n            self.pred(x)\n            .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])\n            .permute(0, 1, 3, 4, 2)\n        )\n\nclass YOLOv3(nn.Module):\n    def __init__(self, in_channels=3, num_classes=20):\n        super(YOLOv3, self).__init__()\n        self.num_classes = num_classes\n        self.in_channels = in_channels\n        self.layers = self._create_conv_layers()\n\n    def forward(self, x):\n        outputs = []\n        route_connections = []\n        for layer in self.layers:\n            if isinstance(layer, Prediction_Scale):\n                outputs.append(layer(x))\n                continue\n            x = layer(x)\n            if isinstance(layer, Residual_Block) and layer.num_repeats == 8:\n                route_connections.append(x)\n            elif isinstance(layer, nn.Upsample):\n                x = torch.cat([x, route_connections.pop()], dim=1)\n        return outputs\n\n    def _create_conv_layers(self):\n        layers = nn.ModuleList()\n        in_channels = self.in_channels\n        for module in config:\n            if isinstance(module, tuple):\n                out_channels, kernel_size, stride = module\n                layers.append(CNN_Block(\n                    in_channels, out_channels,\n                    kernel_size=kernel_size, stride=stride,\n                    padding=1 if kernel_size == 3 else 0\n                ))\n                in_channels = out_channels\n            elif isinstance(module, list):\n                layers.append(Residual_Block(in_channels, num_repeats=module[1]))\n            elif isinstance(module, str):\n                if module == \"sp\":\n                    layers += [\n                        Residual_Block(in_channels, use_residual=False, num_repeats=1),\n                        CNN_Block(in_channels, in_channels//2, kernel_size=1),\n                        Prediction_Scale(in_channels//2, num_classes=self.num_classes)\n                    ]\n                    in_channels = in_channels // 2\n                elif module == \"up\":\n                    layers.append(nn.Upsample(scale_factor=2))\n                    in_channels = in_channels * 3\n        return layers\n\nclass YOLODataset(Dataset):\n    def __init__(self, csv_file, ImgDir, LableDir, anchors,\n                 ImageSize=416, sp=[13,26,52], cp=20, transform=None):\n        self.annotations = pd.read_csv(csv_file)\n        self.ImgDir = ImgDir\n        self.LableDir = LableDir\n        self.transform = transform\n        self.sp = sp\n        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])\n        self.num_anchors = self.anchors.shape[0]\n        self.num_anchors_per_scale = self.num_anchors // 3\n        self.cp = cp\n        self.ignore_iou_thresh = 0.5\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        label_path = os.path.join(self.LableDir, self.annotations.iloc[index, 1])\n        boxx = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n        img_path = os.path.join(self.ImgDir, self.annotations.iloc[index, 0])\n        image = Image.open(img_path)\n        if self.transform:\n            image = self.transform(image)\n        targets = [torch.zeros((self.num_anchors // 3, sp, sp, 6)) for sp in self.sp]\n        for box in boxx:\n            iou_anchors = WeidthHeight(torch.tensor(box[2:4]), self.anchors)\n            anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n            x, y, width, height, class_label = box\n            has_anchor = [False, False, False]\n            for anchor_idx in anchor_indices:\n                scale_idx = anchor_idx // self.num_anchors_per_scale\n                anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n                sp = self.sp[scale_idx]\n                i, j = int(sp*y), int(sp*x)\n                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n                if not anchor_taken and not has_anchor[scale_idx]:\n                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n                    x_cell, y_cell = sp*x - j, sp*y - i\n                    width_cell, height_cell = width*sp, height*sp\n                    box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n                    has_anchor[scale_idx] = True\n                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1\n        return image, tuple(targets)\n\nclass YoloLoss(nn.Module):\n    def __init__(self):\n        super(YoloLoss, self).__init__()\n        self.mse = nn.MSELoss()\n        self.bce = nn.BCEWithLogitsLoss()\n        self.entropy = nn.CrossEntropyLoss()\n        self.sigmoid = nn.Sigmoid()\n        self.lambda_class = 1\n        self.lambda_noobj = 10\n        self.lambda_obj = 1\n        self.lambda_box = 10\n\n    def forward(self, predictions, target, anchors):\n        obj = target[..., 0] == 1\n        noobj = target[..., 0] == 0\n        no_object_loss = self.bce((predictions[..., 0:1][noobj]), (target[..., 0:1][noobj]))\n        anchors = anchors.reshape(1,3,1,1,2)\n        box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n        ious = InterctionOverUnion(box_preds[obj], target[..., 1:5][obj]).detach()\n        object_loss = self.bce((predictions[..., 0:1][obj]), (ious * target[..., 0:1][obj]))\n        predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3])\n        target[..., 3:5] = torch.log((1e-6 + target[..., 3:5] / anchors))\n        box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n        class_loss = self.entropy((predictions[..., 5:][obj]), (target[..., 5][obj].long()))\n        return (self.lambda_box * box_loss + self.lambda_obj * object_loss + self.lambda_noobj * no_object_loss + self.lambda_class * class_loss)\n\n# Utility functions\ndef WeidthHeight(boxa, boxb):\n    intersection = torch.min(boxa[..., 0], boxb[..., 0]) * torch.min(boxa[..., 1], boxb[..., 1])\n    union = (boxa[..., 0] * boxa[..., 1] + boxb[..., 0] * boxb[..., 1] - intersection)\n    return intersection / union\n\ndef non_max_suppression(boxx, iou_threshold, threshold, box_format=\"corners\"):\n    assert type(boxx) == list\n    boxx = [box for box in boxx if box[1] > threshold]\n    boxx = sorted(boxx, key=lambda x: x[1], reverse=True)\n    boxx_after_nms = []\n    while boxx:\n        chosen_box = boxx.pop(0)\n        boxx = [box for box in boxx if box[0] != chosen_box[0] or InterctionOverUnion(\n            torch.tensor(chosen_box[2:]), torch.tensor(box[2:]), box_format=box_format) < iou_threshold]\n        boxx_after_nms.append(chosen_box)\n    return boxx_after_nms\n\ndef InterctionOverUnion(PredsBox, lableBox, box_format=\"midpoint\"):\n    if box_format == \"midpoint\":\n        box1_a1 = PredsBox[..., 0:1] - PredsBox[..., 2:3] / 2\n        box1_b1 = PredsBox[..., 1:2] - PredsBox[..., 3:4] / 2\n        box1_a2 = PredsBox[..., 0:1] + PredsBox[..., 2:3] / 2\n        box1_b2 = PredsBox[..., 1:2] + PredsBox[..., 3:4] / 2\n        box2_a1 = lableBox[..., 0:1] - lableBox[..., 2:3] / 2\n        box2_y1 = lableBox[..., 1:2] - lableBox[..., 3:4] / 2\n        box2_a2 = lableBox[..., 0:1] + lableBox[..., 2:3] / 2\n        box2_y2 = lableBox[..., 1:2] + lableBox[..., 3:4] / 2\n    if box_format == \"corners\":\n        box1_a1 = PredsBox[..., 0:1]\n        box1_b1 = PredsBox[..., 1:2]\n        box1_a2 = PredsBox[..., 2:3]\n        box1_b2 = PredsBox[..., 3:4]\n        box2_a1 = lableBox[..., 0:1]\n        box2_y1 = lableBox[..., 1:2]\n        box2_a2 = lableBox[..., 2:3]\n        box2_y2 = lableBox[..., 3:4]\n    x1 = torch.max(box1_a1, box2_a1)\n    y1 = torch.max(box1_b1, box2_y1)\n    x2 = torch.min(box1_a2, box2_a2)\n    y2 = torch.min(box1_b2, box2_y2)\n    intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n    box1_area = (box1_a2 - box1_a1) * (box1_b2 - box1_b1)\n    box2_area = (box2_a2 - box2_a1) * (box2_y2 - box2_y1)\n    return intersection / (box1_area + box2_area - intersection)\n\ndef mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", NumClasses=4):\n    average_precisions = []\n    epsilon = 1e-6\n    for c in range(NumClasses):\n        detections = []\n        ground_truths = []\n        for detection in pred_boxes:\n            if detection[1] == c:\n                detections.append(detection)\n        for true_box in true_boxes:\n            if true_box[1] == c:\n                ground_truths.append(true_box)\n        amount_boxx = Counter([gt[0] for gt in ground_truths])\n        for key, val in amount_boxx.items():\n            amount_boxx[key] = torch.zeros(val)\n        detections.sort(key=lambda x: x[2], reverse=True)\n        TP = torch.zeros((len(detections)))\n        FP = torch.zeros((len(detections)))\n        total_true_boxx = len(ground_truths)\n        if total_true_boxx == 0:\n            continue\n        for detection_idx, detection in enumerate(detections):\n            ground_truth_img = [bbox for bbox in ground_truths if bbox[0] == detection[0]]\n            num_gts = len(ground_truth_img)\n            best_iou = 0\n            for idx, gt in enumerate(ground_truth_img):\n                iou = InterctionOverUnion(torch.tensor(detection[3:]), torch.tensor(gt[3:]), box_format=box_format)\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_idx = idx\n            if best_iou > iou_threshold:\n                if amount_boxx[detection[0]][best_gt_idx] == 0:\n                    TP[detection_idx] = 1\n                    amount_boxx[detection[0]][best_gt_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n            else:\n                FP[detection_idx] = 1\n        TP_cumsum = torch.cumsum(TP, dim=0)\n        FP_cumsum = torch.cumsum(FP, dim=0)\n        recalls = TP_cumsum / (total_true_boxx + epsilon)\n        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n        average_precisions.append(torch.trapz(precisions, recalls))\n    return sum(average_precisions) / len(average_precisions)\n\ndef get_evaluation_boxx(loader, model, iou_threshold, anchors, threshold, box_format=\"midpoint\"):\n    model.eval()\n    train_idx = 0\n    all_pred_boxes = []\n    all_true_boxes = []\n    for batch_idx, (x, labels) in enumerate(loader):\n        x = x.float().to(DEVICE)\n        with torch.no_grad():\n            predictions = model(x)\n        batch_size = x.shape[0]\n        boxx = [[] for _ in range(batch_size)]\n        for i in range(3):\n            sp = predictions[i].shape[2]\n            anchor = torch.tensor([*anchors[i]]).to(DEVICE) * sp\n            boxes_scale_i = cells_to_boxx(predictions[i], anchor, sp=sp, is_preds=True)\n            for idx, box in enumerate(boxes_scale_i):\n                boxx[idx] += box\n        true_boxx = cells_to_boxx(labels[2], anchor, sp=sp, is_preds=False)\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(boxx[idx], iou_threshold=iou_threshold, threshold=threshold, box_format=box_format)\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n            for box in true_boxx[idx]:\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n            train_idx += 1\n    model.train()\n    return all_pred_boxes, all_true_boxes\n\ndef cells_to_boxx(predictions, anchors, sp, is_preds=True):\n    SizeOfBatch = predictions.shape[0]\n    num_anchors = len(anchors)\n    box_predictions = predictions[..., 1:5]\n    if is_preds:\n        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n        scores = torch.sigmoid(predictions[..., 0:1])\n        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n    else:\n        scores = predictions[..., 0:1]\n        best_class = predictions[..., 5:6]\n    cell_indices = (\n        torch.arange(sp)\n        .repeat(predictions.shape[0], 3, sp, 1)\n        .unsqueeze(-1)\n        .to(predictions.device)\n    )\n    x = 1 / sp * (box_predictions[..., 0:1] + cell_indices)\n    y = 1 / sp * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n    w_h = 1 / sp * box_predictions[..., 2:4]\n    converted_boxx = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(SizeOfBatch, num_anchors * sp * sp, 6)\n    return converted_boxx.tolist()\n\n# Define the function to compute precision, recall, and f1-score\ndef compute_metrics(pred_boxes, true_boxes, iou_threshold=0.5):\n    pred_classes = [box[0] for box in pred_boxes]\n    pred_iou = [box[1] for box in pred_boxes]\n\n    tp, fp, fn = 0, 0, 0\n\n    for pred, iou in zip(pred_classes, pred_iou):\n        if iou > iou_threshold:\n            tp += 1\n        else:\n            fp += 1\n\n    true_classes = [box[0] for box in true_boxes]\n    for true in true_classes:\n        if not any(pred == true for pred in pred_classes):\n            fn += 1\n\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n    return precision, recall, f1\n\ndef plot_image(image, boxes):\n    cmap = plt.get_cmap(\"tab20b\")\n    class_labels = AllClacess\n    colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n    im = np.array(image)\n    height, width, _ = im.shape\n    fig, ax = plt.subplots(1)\n    ax.imshow(im)\n    for box in boxes:\n        assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n        class_pred = box[0]\n        box = box[2:]\n        UpperLeft_x = box[0] - box[2] / 2\n        UpperLeft_y = box[1] - box[3] / 2\n        rect = patches.Rectangle(\n            (UpperLeft_x * width, UpperLeft_y * height),\n            box[2] * width, box[3] * height,\n            linewidth=2, edgecolor=colors[int(class_pred)], facecolor=\"none\",\n        )\n        ax.add_patch(rect)\n        plt.text(\n            UpperLeft_x * width,\n            UpperLeft_y * height,\n            s=class_labels[int(class_pred)],\n            color=\"white\",\n            verticalalignment=\"top\",\n            bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n        )\n    plt.show()\n\n# Instantiate the model\nmodel = YOLOv3(num_classes=NumClasses).to(DEVICE)\n\n# Compile the model\noptimizer = torch.optim.Adam(model.parameters(), lr=RateOfLearning)\nloss_fn = YoloLoss()\n\n# Scaler\nscaler = torch.cuda.amp.GradScaler()\n\n# Transform\ntransform = transforms.Compose([transforms.Resize((416, 416)), transforms.ToTensor()])\n\n# Train-Test Loader\ndef get_loaders(train_csv_path, test_csv_path):\n    train_dataset = YOLODataset(train_csv_path, transform=transform, sp=[ImageSize // 32, ImageSize // 16, ImageSize // 8],\n                                ImgDir=DirImage, LableDir=DirLable, anchors=ANCHORS)\n    test_dataset = YOLODataset(test_csv_path, transform=transform, sp=[ImageSize // 32, ImageSize // 16, ImageSize // 8],\n                               ImgDir=DirImage, LableDir=DirLable, anchors=ANCHORS)\n    train_loader = DataLoader(dataset=train_dataset, batch_size=SizeOfBatch, shuffle=True, drop_last=False)\n    test_loader = DataLoader(dataset=test_dataset, batch_size=SizeOfBatch, shuffle=False, drop_last=False)\n    return train_loader, test_loader\n\ntrain_loader, test_loader = get_loaders(\n    train_csv_path='/kaggle/input/pascalvoc-yolo/100examples.csv', test_csv_path='/kaggle/input/pascalvoc-yolo/8examples.csv'\n)\n\n# Anchors\nscaled_anchors = (\n    torch.tensor(ANCHORS) * torch.tensor([13,26,52]).unsqueeze(1).unsqueeze(1).repeat(1,3,2)\n).to(DEVICE)\n\n# Training loop\nhistory_loss = []\n\nfor epoch in tqdm(range(epochsno), desc=\"Epochs\"):\n    model.train()\n    losses = []\n    start_time = time.time()\n    all_ious = []\n    for batch_idx, (x,y) in enumerate(train_loader):\n        x = x.to(DEVICE)\n        y0, y1, y2 = (y[0].to(DEVICE), y[1].to(DEVICE), y[2].to(DEVICE))\n        with torch.cuda.amp.autocast():\n            out = model(x)\n            loss = (loss_fn(out[0], y0, scaled_anchors[0]) +\n                    loss_fn(out[1], y1, scaled_anchors[1]) +\n                    loss_fn(out[2], y2, scaled_anchors[2]))\n        losses.append(loss.item())\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        with torch.no_grad():\n            ious = InterctionOverUnion(out[0], y0)\n            all_ious.append(ious.mean().item())\n    end_time = time.time()\n    epoch_duration = end_time - start_time\n    history_loss.append(sum(losses)/len(losses))\n    pred_boxes, true_boxes = get_evaluation_boxx(train_loader, model, ThreshMap, ANCHORS, ThresholdConf)\n    mAP = mean_average_precision(pred_boxes, true_boxes, iou_threshold=ThreshMap, box_format=\"midpoint\", NumClasses=NumClasses)\n    precision, recall, f1 = compute_metrics(pred_boxes, true_boxes)\n    if (epoch+1) % 10 == 0:\n        tqdm.write(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds\")\n        print(f\"Epoch [{epoch+1}/{epochsno}], Loss: {sum(losses)/len(losses):.4f}\")\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"loss\": sum(losses)/len(losses),\n            \"epoch_duration\": epoch_duration,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1_score\": f1,\n            \"mean_average_precision\": mAP,\n            \"iou\": np.mean(all_ious)\n        })\n        torch.save(model.state_dict(), f'/kaggle/working/Yolov3_epoch{epoch+1}.pth')\n\nwandb.finish()\n","metadata":{"id":"oVY5p7PX1Hay","editable":false,"trusted":true},"execution_count":null,"outputs":[]}]}