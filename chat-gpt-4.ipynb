{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1556326,"sourceType":"datasetVersion","datasetId":918769}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"L4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"909c0fdb959d40328d3724326e57cb67":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_cde69577ceab4dabaff29253c3e75344","IPY_MODEL_5be9d30694b24e339516f6dcbf62d850"],"layout":"IPY_MODEL_b72ae68a4fc44485bbf14d450d97b208"}},"cde69577ceab4dabaff29253c3e75344":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a18c78746ecf4eb299ce4d98ef8e6291","placeholder":"​","style":"IPY_MODEL_eb96752987ed44479393043601c5d77a","value":"0.016 MB of 0.016 MB uploaded\r"}},"5be9d30694b24e339516f6dcbf62d850":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8cc742c44fe4fdf8e5a7b62f9af3f7c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e82dc19fbc634ae788e261c0b3d36ede","value":1}},"b72ae68a4fc44485bbf14d450d97b208":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a18c78746ecf4eb299ce4d98ef8e6291":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb96752987ed44479393043601c5d77a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8cc742c44fe4fdf8e5a7b62f9af3f7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e82dc19fbc634ae788e261c0b3d36ede":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image, ImageFile\nfrom collections import Counter\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport wandb  # Import wandb\nfrom transformers import BertTokenizer\n\n# Login with the API KEY\nwandb.login(key=\"ab35ea8191eba471c2b58a844910531625b00550\")\n\n# Initialize wandb\nwandb.init(project=\"Working wandb\", entity=\"mblogge785-work\")\n\n# Initialize the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Define configuration for YOLOv3\nconfig = [\n    (32, 3, 1),\n    (128, 3, 1),\n    (64, 3, 2),\n    [\"list\", 1],\n    (128, 3, 2),\n    [\"list\", 2],\n    (256, 3, 2),\n    [\"list\", 8],\n    (512, 3, 2),\n    [\"list\", 8],\n    (1024, 3, 2),\n    [\"list\", 4],\n    (512, 1, 1),\n    (1024, 3, 1),\n    \"sp\",\n    (256, 1, 1),\n    \"up\",\n    (256, 1, 1),\n    (512, 3, 1),\n    \"sp\",\n    (128, 1, 1),\n    \"up\",\n    (128, 1, 1),\n    (256, 3, 1),\n    \"sp\",\n]\n\n# Define constants\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nNumClasses = 20\nImageSize = 416\nRateOfLearning = 1e-5\nepochsno = 150\nThresholdConf = 0.8\nThreshMap = 0.5\nThreshNms = 0.45\nSizeOfBatch = 32\nDirImage = \"/kaggle/input/pascalvoc-yolo/images\"\nDirLable = \"/kaggle/input/pascalvoc-yolo/labels\"\n\n# Define anchors and classes\nANCHORS = [\n    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n]\n\nAllClacess = [\n    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\",\n    \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n]\n\n# Define model hyperparameters\nENC_EMB_DIM = 512\nDEC_EMB_DIM = 512\nHID_DIM = 1024\nN_LAYERS = 2\nENC_DROPOUT = 0.3\nDEC_DROPOUT = 0.3\n\nwandb.config.update({\n    \"learning_rate\": 1e-3,\n    \"epochs\": 30,\n    \"batch_size\": 64,\n    \"encoder_embedding_dim\": ENC_EMB_DIM,\n    \"decoder_embedding_dim\": DEC_EMB_DIM,\n    \"hidden_dim\": HID_DIM,\n    \"num_layers\": N_LAYERS,\n    \"encoder_dropout\": ENC_DROPOUT,\n    \"decoder_dropout\": DEC_DROPOUT\n})\n\nclass CNN_Block(nn.Module):\n    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n        super(CNN_Block, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.leaky = nn.LeakyReLU(0.1)\n        self.use_bn_act = bn_act\n\n    def forward(self, x):\n        if self.use_bn_act:\n            return self.leaky(self.bn(self.conv(x)))\n        else:\n            return self.conv(x)\n\nclass Residual_Block(nn.Module):\n    def __init__(self, channels, use_residual=True, num_repeats=1):\n        super(Residual_Block, self).__init__()\n        self.layers = nn.ModuleList()\n        for repeat in range(num_repeats):\n            self.layers += [\n                nn.Sequential(\n                    CNN_Block(channels, channels//2, kernel_size=1),\n                    CNN_Block(channels//2, channels, kernel_size=3, padding=1)\n                )\n            ]\n        self.use_residual = use_residual\n        self.num_repeats = num_repeats\n\n    def forward(self, x):\n        for layer in self.layers:\n            if self.use_residual:\n                x = x + layer(x)\n            else:\n                x = layer(x)\n        return x\n\nclass Prediction_Scale(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super(Prediction_Scale, self).__init__()\n        self.pred = nn.Sequential(\n            CNN_Block(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n            CNN_Block(2 * in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1),\n        )\n        self.num_classes = num_classes\n\n    def forward(self, x):\n        return (\n            self.pred(x)\n            .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])\n            .permute(0, 1, 3, 4, 2)\n        )\n\nclass YOLOv3(nn.Module):\n    def __init__(self, in_channels=3, num_classes=20):\n        super(YOLOv3, self).__init__()\n        self.num_classes = num_classes\n        self.in_channels = in_channels\n        self.layers = self._create_conv_layers()\n\n    def forward(self, x):\n        outputs = []\n        route_connections = []\n        for layer in self.layers:\n            if isinstance(layer, Prediction_Scale):\n                outputs.append(layer(x))\n                continue\n            x = layer(x)\n            if isinstance(layer, Residual_Block) and layer.num_repeats == 8:\n                route_connections.append(x)\n            elif isinstance(layer, nn.Upsample):\n                x = torch.cat([x, route_connections.pop()], dim=1)\n        return outputs\n\n    def _create_conv_layers(self):\n        layers = nn.ModuleList()\n        in_channels = self.in_channels\n        for module in config:\n            if isinstance(module, tuple):\n                out_channels, kernel_size, stride = module\n                layers.append(CNN_Block(\n                    in_channels, out_channels,\n                    kernel_size=kernel_size, stride=stride,\n                    padding=1 if kernel_size == 3 else 0\n                ))\n                in_channels = out_channels\n            elif isinstance(module, list):\n                layers.append(Residual_Block(in_channels, num_repeats=module[1]))\n            elif isinstance(module, str):\n                if module == \"sp\":\n                    layers += [\n                        Residual_Block(in_channels, use_residual=False, num_repeats=1),\n                        CNN_Block(in_channels, in_channels//2, kernel_size=1),\n                        Prediction_Scale(in_channels//2, num_classes=self.num_classes)\n                    ]\n                    in_channels = in_channels // 2\n                elif module == \"up\":\n                    layers.append(nn.Upsample(scale_factor=2))\n                    in_channels = in_channels * 3\n        return layers\n\nclass YOLODataset(Dataset):\n    def __init__(self, csv_file, ImgDir, LableDir, anchors,\n                 ImageSize=416, sp=[13,26,52], cp=20, transform=None):\n        self.annotations = pd.read_csv(csv_file)\n        self.ImgDir = ImgDir\n        self.LableDir = LableDir\n        self.transform = transform\n        self.sp = sp\n        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])\n        self.num_anchors = self.anchors.shape[0]\n        self.num_anchors_per_scale = self.num_anchors // 3\n        self.cp = cp\n        self.ignore_iou_thresh = 0.5\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        label_path = os.path.join(self.LableDir, self.annotations.iloc[index, 1])\n        boxx = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n        img_path = os.path.join(self.ImgDir, self.annotations.iloc[index, 0])\n        image = Image.open(img_path)\n        if self.transform:\n            image = self.transform(image)\n        targets = [torch.zeros((self.num_anchors // 3, sp, sp, 6)) for sp in self.sp]\n        for box in boxx:\n            iou_anchors = WeidthHeight(torch.tensor(box[2:4]), self.anchors)\n            anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n            x, y, width, height, class_label = box\n            has_anchor = [False, False, False]\n            for anchor_idx in anchor_indices:\n                scale_idx = anchor_idx // self.num_anchors_per_scale\n                anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n                sp = self.sp[scale_idx]\n                i, j = int(sp*y), int(sp*x)\n                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n                if not anchor_taken and not has_anchor[scale_idx]:\n                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n                    x_cell, y_cell = sp*x - j, sp*y - i\n                    width_cell, height_cell = width*sp, height*sp\n                    box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n                    has_anchor[scale_idx] = True\n                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1\n        return image, tuple(targets)\n\nclass YoloLoss(nn.Module):\n    def __init__(self):\n        super(YoloLoss, self).__init__()\n        self.mse = nn.MSELoss()\n        self.bce = nn.BCEWithLogitsLoss()\n        self.entropy = nn.CrossEntropyLoss()\n        self.sigmoid = nn.Sigmoid()\n        self.lambda_class = 1\n        self.lambda_noobj = 10\n        self.lambda_obj = 1\n        self.lambda_box = 10\n\n    def forward(self, predictions, target, anchors):\n        obj = target[..., 0] == 1\n        noobj = target[..., 0] == 0\n        no_object_loss = self.bce((predictions[..., 0:1][noobj]), (target[..., 0:1][noobj]))\n        anchors = anchors.reshape(1,3,1,1,2)\n        box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n        ious = InterctionOverUnion(box_preds[obj], target[..., 1:5][obj]).detach()\n        object_loss = self.bce((predictions[..., 0:1][obj]), (ious * target[..., 0:1][obj]))\n        predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3])\n        target[..., 3:5] = torch.log((1e-6 + target[..., 3:5] / anchors))\n        box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n        class_loss = self.entropy((predictions[..., 5:][obj]), (target[..., 5][obj].long()))\n        return (self.lambda_box * box_loss + self.lambda_obj * object_loss + self.lambda_noobj * no_object_loss + self.lambda_class * class_loss)\n\n# Utility functions\ndef WeidthHeight(boxa, boxb):\n    intersection = torch.min(boxa[..., 0], boxb[..., 0]) * torch.min(boxa[..., 1], boxb[..., 1])\n    union = (boxa[..., 0] * boxa[..., 1] + boxb[..., 0] * boxb[..., 1] - intersection)\n    return intersection / union\n\ndef non_max_suppression(boxx, iou_threshold, threshold, box_format=\"corners\"):\n    assert type(boxx) == list\n    boxx = [box for box in boxx if box[1] > threshold]\n    boxx = sorted(boxx, key=lambda x: x[1], reverse=True)\n    boxx_after_nms = []\n    while boxx:\n        chosen_box = boxx.pop(0)\n        boxx = [box for box in boxx if box[0] != chosen_box[0] or InterctionOverUnion(\n            torch.tensor(chosen_box[2:]), torch.tensor(box[2:]), box_format=box_format) < iou_threshold]\n        boxx_after_nms.append(chosen_box)\n    return boxx_after_nms\n\ndef InterctionOverUnion(PredsBox, lableBox, box_format=\"midpoint\"):\n    if box_format == \"midpoint\":\n        box1_a1 = PredsBox[..., 0:1] - PredsBox[..., 2:3] / 2\n        box1_b1 = PredsBox[..., 1:2] - PredsBox[..., 3:4] / 2\n        box1_a2 = PredsBox[..., 0:1] + PredsBox[..., 2:3] / 2\n        box1_b2 = PredsBox[..., 1:2] + PredsBox[..., 3:4] / 2\n        box2_a1 = lableBox[..., 0:1] - lableBox[..., 2:3] / 2\n        box2_y1 = lableBox[..., 1:2] - lableBox[..., 3:4] / 2\n        box2_a2 = lableBox[..., 0:1] + lableBox[..., 2:3] / 2\n        box2_y2 = lableBox[..., 1:2] + lableBox[..., 3:4] / 2\n    if box_format == \"corners\":\n        box1_a1 = PredsBox[..., 0:1]\n        box1_b1 = PredsBox[..., 1:2]\n        box1_a2 = PredsBox[..., 2:3]\n        box1_b2 = PredsBox[..., 3:4]\n        box2_a1 = lableBox[..., 0:1]\n        box2_y1 = lableBox[..., 1:2]\n        box2_a2 = lableBox[..., 2:3]\n        box2_y2 = lableBox[..., 3:4]\n    x1 = torch.max(box1_a1, box2_a1)\n    y1 = torch.max(box1_b1, box2_y1)\n    x2 = torch.min(box1_a2, box2_a2)\n    y2 = torch.min(box1_b2, box2_y2)\n    intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n    box1_area = (box1_a2 - box1_a1) * (box1_b2 - box1_b1)\n    box2_area = (box2_a2 - box2_a1) * (box2_y2 - box2_y1)\n    return intersection / (box1_area + box2_area - intersection)\n\ndef mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", NumClasses=4):\n    average_precisions = []\n    epsilon = 1e-6\n    for c in range(NumClasses):\n        detections = []\n        ground_truths = []\n        for detection in pred_boxes:\n            if detection[1] == c:\n                detections.append(detection)\n        for true_box in true_boxes:\n            if true_box[1] == c:\n                ground_truths.append(true_box)\n        amount_boxx = Counter([gt[0] for gt in ground_truths])\n        for key, val in amount_boxx.items():\n            amount_boxx[key] = torch.zeros(val)\n        detections.sort(key=lambda x: x[2], reverse=True)\n        TP = torch.zeros((len(detections)))\n        FP = torch.zeros((len(detections)))\n        total_true_boxx = len(ground_truths)\n        if total_true_boxx == 0:\n            continue\n        for detection_idx, detection in enumerate(detections):\n            ground_truth_img = [bbox for bbox in ground_truths if bbox[0] == detection[0]]\n            num_gts = len(ground_truth_img)\n            best_iou = 0\n            for idx, gt in enumerate(ground_truth_img):\n                iou = InterctionOverUnion(torch.tensor(detection[3:]), torch.tensor(gt[3:]), box_format=box_format)\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_idx = idx\n            if best_iou > iou_threshold:\n                if amount_boxx[detection[0]][best_gt_idx] == 0:\n                    TP[detection_idx] = 1\n                    amount_boxx[detection[0]][best_gt_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n            else:\n                FP[detection_idx] = 1\n        TP_cumsum = torch.cumsum(TP, dim=0)\n        FP_cumsum = torch.cumsum(FP, dim=0)\n        recalls = TP_cumsum / (total_true_boxx + epsilon)\n        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n        average_precisions.append(torch.trapz(precisions, recalls))\n    return sum(average_precisions) / len(average_precisions)\n\ndef get_evaluation_boxx(loader, model, iou_threshold, anchors, threshold, box_format=\"midpoint\"):\n    model.eval()\n    train_idx = 0\n    all_pred_boxes = []\n    all_true_boxes = []\n    for batch_idx, (x, labels) in enumerate(loader):\n        x = x.float().to(DEVICE)\n        with torch.no_grad():\n            predictions = model(x)\n        batch_size = x.shape[0]\n        boxx = [[] for _ in range(batch_size)]\n        for i in range(3):\n            sp = predictions[i].shape[2]\n            anchor = torch.tensor([*anchors[i]]).to(DEVICE) * sp\n            boxes_scale_i = cells_to_boxx(predictions[i], anchor, sp=sp, is_preds=True)\n            for idx, box in enumerate(boxes_scale_i):\n                boxx[idx] += box\n        true_boxx = cells_to_boxx(labels[2], anchor, sp=sp, is_preds=False)\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(boxx[idx], iou_threshold=iou_threshold, threshold=threshold, box_format=box_format)\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n            for box in true_boxx[idx]:\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n            train_idx += 1\n    model.train()\n    return all_pred_boxes, all_true_boxes\n\ndef cells_to_boxx(predictions, anchors, sp, is_preds=True):\n    SizeOfBatch = predictions.shape[0]\n    num_anchors = len(anchors)\n    box_predictions = predictions[..., 1:5]\n    if is_preds:\n        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n        scores = torch.sigmoid(predictions[..., 0:1])\n        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n    else:\n        scores = predictions[..., 0:1]\n        best_class = predictions[..., 5:6]\n    cell_indices = (\n        torch.arange(sp)\n        .repeat(predictions.shape[0], 3, sp, 1)\n        .unsqueeze(-1)\n        .to(predictions.device)\n    )\n    x = 1 / sp * (box_predictions[..., 0:1] + cell_indices)\n    y = 1 / sp * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n    w_h = 1 / sp * box_predictions[..., 2:4]\n    converted_boxx = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(SizeOfBatch, num_anchors * sp * sp, 6)\n    return converted_boxx.tolist()\n\n# Define the function to compute precision, recall, and f1-score\ndef compute_metrics(pred_boxes, true_boxes, iou_threshold=0.5):\n    pred_classes = [box[0] for box in pred_boxes]\n    pred_iou = [box[1] for box in pred_boxes]\n\n    tp, fp, fn = 0, 0, 0\n\n    for pred, iou in zip(pred_classes, pred_iou):\n        if iou > iou_threshold:\n            tp += 1\n        else:\n            fp += 1\n\n    true_classes = [box[0] for box in true_boxes]\n    for true in true_classes:\n        if not any(pred == true for pred in pred_classes):\n            fn += 1\n\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n    return precision, recall, f1\n\ndef plot_image(image, boxes):\n    cmap = plt.get_cmap(\"tab20b\")\n    class_labels = AllClacess\n    colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n    im = np.array(image)\n    height, width, _ = im.shape\n    fig, ax = plt.subplots(1)\n    ax.imshow(im)\n    for box in boxes:\n        assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n        class_pred = box[0]\n        box = box[2:]\n        UpperLeft_x = box[0] - box[2] / 2\n        UpperLeft_y = box[1] - box[3] / 2\n        rect = patches.Rectangle(\n            (UpperLeft_x * width, UpperLeft_y * height),\n            box[2] * width, box[3] * height,\n            linewidth=2, edgecolor=colors[int(class_pred)], facecolor=\"none\",\n        )\n        ax.add_patch(rect)\n        plt.text(\n            UpperLeft_x * width,\n            UpperLeft_y * height,\n            s=class_labels[int(class_pred)],\n            color=\"white\",\n            verticalalignment=\"top\",\n            bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n        )\n    plt.show()\n\n# Instantiate the model\nmodel = YOLOv3(num_classes=NumClasses).to(DEVICE)\n\n# Compile the model\noptimizer = torch.optim.Adam(model.parameters(), lr=RateOfLearning)\nloss_fn = YoloLoss()\n\n# Scaler\nscaler = torch.cuda.amp.GradScaler()\n\n# Transform\ntransform = transforms.Compose([transforms.Resize((416, 416)), transforms.ToTensor()])\n\n# Train-Test Loader\ndef get_loaders(train_csv_path, test_csv_path):\n    train_dataset = YOLODataset(train_csv_path, transform=transform, sp=[ImageSize // 32, ImageSize // 16, ImageSize // 8],\n                                ImgDir=DirImage, LableDir=DirLable, anchors=ANCHORS)\n    test_dataset = YOLODataset(test_csv_path, transform=transform, sp=[ImageSize // 32, ImageSize // 16, ImageSize // 8],\n                               ImgDir=DirImage, LableDir=DirLable, anchors=ANCHORS)\n    train_loader = DataLoader(dataset=train_dataset, batch_size=SizeOfBatch, shuffle=True, drop_last=False)\n    test_loader = DataLoader(dataset=test_dataset, batch_size=SizeOfBatch, shuffle=False, drop_last=False)\n    return train_loader, test_loader\n\ntrain_loader, test_loader = get_loaders(\n    train_csv_path='/kaggle/input/pascalvoc-yolo/100examples.csv', test_csv_path='/kaggle/input/pascalvoc-yolo/8examples.csv'\n)\n\n# Anchors\nscaled_anchors = (\n    torch.tensor(ANCHORS) * torch.tensor([13,26,52]).unsqueeze(1).unsqueeze(1).repeat(1,3,2)\n).to(DEVICE)\n\n# Training loop\nhistory_loss = []\n\nfor epoch in tqdm(range(epochsno), desc=\"Epochs\"):\n    model.train()\n    losses = []\n    start_time = time.time()\n    all_ious = []\n    for batch_idx, (x,y) in enumerate(train_loader):\n        x = x.to(DEVICE)\n        y0, y1, y2 = (y[0].to(DEVICE), y[1].to(DEVICE), y[2].to(DEVICE))\n        with torch.cuda.amp.autocast():\n            out = model(x)\n            loss = (loss_fn(out[0], y0, scaled_anchors[0]) +\n                    loss_fn(out[1], y1, scaled_anchors[1]) +\n                    loss_fn(out[2], y2, scaled_anchors[2]))\n        losses.append(loss.item())\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        with torch.no_grad():\n            ious = InterctionOverUnion(out[0], y0)\n            all_ious.append(ious.mean().item())\n    end_time = time.time()\n    epoch_duration = end_time - start_time\n    history_loss.append(sum(losses)/len(losses))\n    pred_boxes, true_boxes = get_evaluation_boxx(train_loader, model, ThreshMap, ANCHORS, ThresholdConf)\n    mAP = mean_average_precision(pred_boxes, true_boxes, iou_threshold=ThreshMap, box_format=\"midpoint\", NumClasses=NumClasses)\n    precision, recall, f1 = compute_metrics(pred_boxes, true_boxes)\n    if (epoch+1) % 10 == 0:\n        tqdm.write(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds\")\n        print(f\"Epoch [{epoch+1}/{epochsno}], Loss: {sum(losses)/len(losses):.4f}\")\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"loss\": sum(losses)/len(losses),\n            \"epoch_duration\": epoch_duration,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1_score\": f1,\n            \"mean_average_precision\": mAP,\n            \"iou\": np.mean(all_ious)\n        })\n        torch.save(model.state_dict(), f'/kaggle/working/Yolov3_epoch{epoch+1}.pth')\n\nwandb.finish()\n","metadata":{"id":"oVY5p7PX1Hay","execution":{"iopub.status.busy":"2024-08-08T11:01:31.559793Z","iopub.execute_input":"2024-08-08T11:01:31.560150Z","iopub.status.idle":"2024-08-08T11:07:24.597951Z","shell.execute_reply.started":"2024-08-08T11:01:31.560120Z","shell.execute_reply":"2024-08-08T11:07:24.596304Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:2kmt3imh) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.018 MB of 0.018 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fancy-snow-120</strong> at: <a href='https://wandb.ai/mblogge785-work/Untitled10/runs/2kmt3imh' target=\"_blank\">https://wandb.ai/mblogge785-work/Untitled10/runs/2kmt3imh</a><br/> View project at: <a href='https://wandb.ai/mblogge785-work/Untitled10' target=\"_blank\">https://wandb.ai/mblogge785-work/Untitled10</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240808_105152-2kmt3imh/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:2kmt3imh). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112495366665422, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6441b31f7eec4aaab6bc4d5dcf640afd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240808_110131-u3d2oici</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mblogge785-work/Untitled10/runs/u3d2oici' target=\"_blank\">mild-frost-121</a></strong> to <a href='https://wandb.ai/mblogge785-work/Untitled10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mblogge785-work/Untitled10' target=\"_blank\">https://wandb.ai/mblogge785-work/Untitled10</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mblogge785-work/Untitled10/runs/u3d2oici' target=\"_blank\">https://wandb.ai/mblogge785-work/Untitled10/runs/u3d2oici</a>"},"metadata":{}},{"name":"stderr","text":"Epochs:  13%|█▎        | 19/150 [04:26<41:06, 18.83s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 20 completed in 4.85 seconds\nEpoch [20/150], Loss: 44.8826\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  15%|█▍        | 22/150 [05:29<31:54, 14.96s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 536\u001b[0m\n\u001b[1;32m    534\u001b[0m epoch_duration \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    535\u001b[0m history_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msum\u001b[39m(losses)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(losses))\n\u001b[0;32m--> 536\u001b[0m pred_boxes, true_boxes \u001b[38;5;241m=\u001b[39m \u001b[43mget_evaluation_boxx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mThreshMap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mANCHORS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mThresholdConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m mAP \u001b[38;5;241m=\u001b[39m mean_average_precision(pred_boxes, true_boxes, iou_threshold\u001b[38;5;241m=\u001b[39mThreshMap, box_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmidpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, NumClasses\u001b[38;5;241m=\u001b[39mNumClasses)\n\u001b[1;32m    538\u001b[0m precision, recall, f1 \u001b[38;5;241m=\u001b[39m compute_metrics(pred_boxes, true_boxes)\n","Cell \u001b[0;32mIn[3], line 377\u001b[0m, in \u001b[0;36mget_evaluation_boxx\u001b[0;34m(loader, model, iou_threshold, anchors, threshold, box_format)\u001b[0m\n\u001b[1;32m    375\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 377\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    379\u001b[0m boxx \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size)]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[3], line 168\u001b[0m, in \u001b[0;36mYOLOv3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    166\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(layer(x))\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Residual_Block) \u001b[38;5;129;01mand\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mnum_repeats \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m8\u001b[39m:\n\u001b[1;32m    170\u001b[0m     route_connections\u001b[38;5;241m.\u001b[39mappend(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[3], line 112\u001b[0m, in \u001b[0;36mCNN_Block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bn_act:\n\u001b[0;32m--> 112\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleaky(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 0 has a total capacty of 14.74 GiB of which 618.12 MiB is free. Process 3042 has 14.13 GiB memory in use. Of the allocated memory 4.32 GiB is allocated by PyTorch, and 9.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 2.64 GiB. GPU 0 has a total capacty of 14.74 GiB of which 618.12 MiB is free. Process 3042 has 14.13 GiB memory in use. Of the allocated memory 4.32 GiB is allocated by PyTorch, and 9.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]}]}