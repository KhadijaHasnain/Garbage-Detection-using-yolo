{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import os\n",
        "# import sys\n",
        "# from tempfile import NamedTemporaryFile\n",
        "# from urllib.request import urlopen\n",
        "# from urllib.parse import unquote, urlparse\n",
        "# from urllib.error import HTTPError\n",
        "# from zipfile import ZipFile\n",
        "# import tarfile\n",
        "# import shutil\n",
        "\n",
        "# CHUNK_SIZE = 40960\n",
        "# DATA_SOURCE_MAPPING = 'pascalvoc-yolo:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F918769%2F1556326%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240803%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240803T180619Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D60245d936b12665b478461fae2417841ba2c7203275a0ae79659cb349615d122fac7b15d780d8891bb36e299c6bdf680fe6859446ca67e70d8d5432d2125cdeaf545729e8416e98cee7d35e9b170ab7f6cc74a90bfe13132a89fb9d1e010d24a968018b5af2820483069c50f9e19a704c76552339d4d6b9e706cd97f54082ed42e35db0153b33745f56f4ba8add1879563830b759076dc443bdaf5059a6d9b14ac3fffea278ebe72b2599b5f1aa4ac69e61b10489dd613fa15191ff298552939ad0b28066478f9d2aea9cb4559e8b32b431070c89b1d3ea223d49a482bffa1dc5916f86a5a8d4457e1b12276d4f19e028cfbc32bff990ed382347814dff79301'\n",
        "\n",
        "# KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "# KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "# KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "# !umount /kaggle/input/ 2> /dev/null\n",
        "# shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "# os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "# os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "# try:\n",
        "#   os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "# except FileExistsError:\n",
        "#   pass\n",
        "# try:\n",
        "#   os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "# except FileExistsError:\n",
        "#   pass\n",
        "\n",
        "# for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "#     directory, download_url_encoded = data_source_mapping.split(':')\n",
        "#     download_url = unquote(download_url_encoded)\n",
        "#     filename = urlparse(download_url).path\n",
        "#     destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "#     try:\n",
        "#         with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "#             total_length = fileres.headers['content-length']\n",
        "#             print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "#             dl = 0\n",
        "#             data = fileres.read(CHUNK_SIZE)\n",
        "#             while len(data) > 0:\n",
        "#                 dl += len(data)\n",
        "#                 tfile.write(data)\n",
        "#                 done = int(50 * dl / int(total_length))\n",
        "#                 sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "#                 sys.stdout.flush()\n",
        "#                 data = fileres.read(CHUNK_SIZE)\n",
        "#             if filename.endswith('.zip'):\n",
        "#               with ZipFile(tfile) as zfile:\n",
        "#                 zfile.extractall(destination_path)\n",
        "#             else:\n",
        "#               with tarfile.open(tfile.name) as tarfile:\n",
        "#                 tarfile.extractall(destination_path)\n",
        "#             print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "#     except HTTPError as e:\n",
        "#         print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "#         continue\n",
        "#     except OSError as e:\n",
        "#         print(f'Failed to load {download_url} to path {destination_path}')\n",
        "#         continue\n",
        "\n",
        "# print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "WOmORtUQkq4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install wandb"
      ],
      "metadata": {
        "id": "jrfeB5wtBC0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image, ImageFile\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import wandb  # Import wandb\n",
        "from transformers import BertTokenizer"
      ],
      "metadata": {
        "id": "hSixTi4uBIfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login with the API KEY\n",
        "wandb.login(key=\"ab35ea8191eba471c2b58a844910531625b00550\")\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"Untitled10\", entity=\"mblogge785-work\")  # Replace with your wandb username"
      ],
      "metadata": {
        "id": "sY8DenwZBPCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "j-EQyFuGpdiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define configuration for YOLOv3\n",
        "config = [\n",
        "    (32, 3, 1),\n",
        "    (128, 3, 1),\n",
        "    (64, 3, 2),\n",
        "    [\"list\", 1],\n",
        "    (128, 3, 2),\n",
        "    [\"list\", 2],\n",
        "    (256, 3, 2),\n",
        "    [\"list\", 8],\n",
        "    (512, 3, 2),\n",
        "    [\"list\", 8],\n",
        "    (1024, 3, 2),\n",
        "    [\"list\", 4],\n",
        "    (512, 1, 1),\n",
        "    (1024, 3, 1),\n",
        "    \"sp\",\n",
        "    (256, 1, 1),\n",
        "    \"up\",\n",
        "    (256, 1, 1),\n",
        "    (512, 3, 1),\n",
        "    \"sp\",\n",
        "    (128, 1, 1),\n",
        "    \"up\",\n",
        "    (128, 1, 1),\n",
        "    (256, 3, 1),\n",
        "    \"sp\",\n",
        "]"
      ],
      "metadata": {
        "id": "PFiBWZrpDg1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CNN block\n",
        "class CNN_Block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
        "        super(CNN_Block, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.leaky = nn.LeakyReLU(0.1)\n",
        "        self.use_bn_act = bn_act\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_bn_act:\n",
        "            return self.leaky(self.bn(self.conv(x)))\n",
        "        else:\n",
        "            return self.conv(x)"
      ],
      "metadata": {
        "id": "xgUPQ25bD73i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Residual block\n",
        "class Residual_Block(nn.Module):\n",
        "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
        "        super(Residual_Block, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for repeat in range(num_repeats):\n",
        "            self.layers += [\n",
        "                nn.Sequential(\n",
        "                    CNN_Block(channels, channels//2, kernel_size=1),\n",
        "                    CNN_Block(channels//2, channels, kernel_size=3, padding=1)\n",
        "                )\n",
        "            ]\n",
        "\n",
        "        self.use_residual = use_residual\n",
        "        self.num_repeats = num_repeats\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            if self.use_residual:\n",
        "                x = x + layer(x)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "30BGaEvUEAG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Prediction Scale\n",
        "class Prediction_Scale(nn.Module):\n",
        "    def __init__(self, in_channels, NumClasses):\n",
        "        super(Prediction_Scale, self).__init__()\n",
        "        self.pred = nn.Sequential(\n",
        "            CNN_Block(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n",
        "            CNN_Block(2 * in_channels, (NumClasses + 5) * 3, bn_act=False, kernel_size=1),\n",
        "        )\n",
        "        self.NumClasses = NumClasses\n",
        "\n",
        "    def forward(self, x):\n",
        "        return (\n",
        "            self.pred(x)\n",
        "            .reshape(x.shape[0], 3, self.NumClasses + 5, x.shape[2], x.shape[3])\n",
        "            .permute(0, 1, 3, 4, 2)\n",
        "        )"
      ],
      "metadata": {
        "id": "WmWG3n0TEKR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YOLOv3(nn.Module):\n",
        "  def __init__(self, in_channels=3, NumClasses=20):\n",
        "    super(YOLOv3, self).__init__()\n",
        "    self.NumClasses = NumClasses\n",
        "    self.in_channels = in_channels\n",
        "    self.layers = self._create_conv_layers()\n",
        "\n",
        "  def forward(self, x):\n",
        "    outputs = []\n",
        "    route_connections = []\n",
        "\n",
        "    for layer in self.layers:\n",
        "      if isinstance(layer, Prediction_Scale):\n",
        "        outputs.append(layer(x))\n",
        "        continue\n",
        "\n",
        "      x = layer(x)\n",
        "\n",
        "      if isinstance(layer, Residual_Block) and layer.num_repeats == 8:\n",
        "        route_connections.append(x)\n",
        "\n",
        "      elif isinstance(layer, nn.Upsample):\n",
        "        x = torch.cat([x, route_connections[-1]], dim=1)\n",
        "        route_connections.pop()\n",
        "\n",
        "    return outputs\n",
        "\n",
        "  def _create_conv_layers(self):\n",
        "    layers = nn.ModuleList()\n",
        "    in_channels = self.in_channels\n",
        "\n",
        "    for module in config:\n",
        "      if isinstance(module, tuple):\n",
        "        out_channels, kernel_size, stride = module\n",
        "        layers.append(CNN_Block(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=1 if kernel_size == 3 else 0\n",
        "        ))\n",
        "        in_channels = out_channels\n",
        "\n",
        "      elif isinstance(module, list):\n",
        "        num_repeats = module[1]\n",
        "        layers.append(Residual_Block(in_channels, num_repeats=num_repeats))\n",
        "\n",
        "      elif isinstance(module, str):\n",
        "        if module == \"sp\":\n",
        "          layers += [\n",
        "              Residual_Block(in_channels, use_residual=False, num_repeats=1),\n",
        "              CNN_Block(in_channels, in_channels//2, kernel_size=1),\n",
        "              Prediction_Scale(in_channels//2, NumClasses = self.NumClasses)\n",
        "          ]\n",
        "          in_channels = in_channels // 2\n",
        "\n",
        "        elif module == \"up\":\n",
        "          layers.append(nn.Upsample(scale_factor=2))\n",
        "          in_channels = in_channels * 3\n",
        "\n",
        "    return layers"
      ],
      "metadata": {
        "id": "xkCgT8d4EaWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model hyperparameters\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "sp = [ImageSize // 32, ImageSize // 16, ImageSize // 8]\n",
        "\n",
        "INPUT_DIM = tokenizer.vocab_size\n",
        "OUTPUT_DIM = tokenizer.vocab_size\n",
        "ImageSize = 512\n",
        "WorkersNo = 4\n",
        "HID_DIM = sp\n",
        "ThresholdConf = 2\n",
        "ThreshMap = 0.3\n",
        "ThreshNms = 0.3\n",
        "\n",
        "wandb.config.update({\n",
        "    \"RateOfLearning\": 1e-3,\n",
        "    \"epochsno\": 30,\n",
        "    \"SizeOfBatch\": 64,\n",
        "    \"encoder_embedding_dim\": ImageSize,\n",
        "    \"decoder_embedding_dim\": WorkersNo,\n",
        "    \"hidden_dim\": HID_DIM,\n",
        "    \"num_layers\": ThresholdConf,\n",
        "    \"encoder_dropout\": ThreshMap,\n",
        "    \"decoder_dropout\": ThreshNms\n",
        "})\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
        "TRG_PAD_IDX = tokenizer.pad_token_id\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
      ],
      "metadata": {
        "id": "lwpV0sCyuvF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NumClasses = 20\n",
        "ImageSize = 416\n",
        "model = YOLOv3(NumClasses=NumClasses)\n",
        "x = torch.randn((2, 3, ImageSize, ImageSize))\n",
        "out = model(x)\n",
        "assert model(x)[0].shape == (2, 3, ImageSize//32, ImageSize//32, NumClasses + 5)\n",
        "assert model(x)[1].shape == (2, 3, ImageSize//16, ImageSize//16, NumClasses + 5)\n",
        "assert model(x)[2].shape == (2, 3, ImageSize//8, ImageSize//8, NumClasses + 5)\n",
        "print(\"Success!\")"
      ],
      "metadata": {
        "id": "iVML2Ku2EnO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define utility functions\n",
        "def WeidthHeight(boxa, boxb):\n",
        "    intersection = torch.min(boxa[..., 0], boxb[..., 0]) * torch.min(\n",
        "        boxa[..., 1], boxb[..., 1]\n",
        "    )\n",
        "    union = (\n",
        "        boxa[..., 0] * boxa[..., 1] + boxb[..., 0] * boxb[..., 1] - intersection\n",
        "    )\n",
        "    return intersection / union\n",
        "\n",
        "def non_max_suppression(boxx, iou_threshold, threshold, box_format=\"corners\"):\n",
        "    assert type(boxx) == list\n",
        "\n",
        "    boxx = [box for box in boxx if box[1] > threshold]\n",
        "    boxx = sorted(boxx, key=lambda x: x[1], reverse=True)\n",
        "    boxx_after_nms = []\n",
        "\n",
        "    while boxx:\n",
        "        chosen_box = boxx.pop(0)\n",
        "\n",
        "        boxx = [\n",
        "            box\n",
        "            for box in boxx\n",
        "            if box[0] != chosen_box[0]\n",
        "            or InterctionOverUnion(\n",
        "                torch.tensor(chosen_box[2:]),\n",
        "                torch.tensor(box[2:]),\n",
        "                box_format=box_format,\n",
        "            )\n",
        "            < iou_threshold\n",
        "        ]\n",
        "\n",
        "        boxx_after_nms.append(chosen_box)\n",
        "\n",
        "    return boxx_after_nms\n",
        "\n",
        "def InterctionOverUnion(PredsBox, lableBox, box_format=\"midpoint\"):\n",
        "    if box_format == \"midpoint\":\n",
        "        box1_a1 = PredsBox[..., 0:1] - PredsBox[..., 2:3] / 2\n",
        "        box1_b1 = PredsBox[..., 1:2] - PredsBox[..., 3:4] / 2\n",
        "        box1_a2 = PredsBox[..., 0:1] + PredsBox[..., 2:3] / 2\n",
        "        box1_b2 = PredsBox[..., 1:2] + PredsBox[..., 3:4] / 2\n",
        "        box2_a1 = lableBox[..., 0:1] - lableBox[..., 2:3] / 2\n",
        "        box2_y1 = lableBox[..., 1:2] - lableBox[..., 3:4] / 2\n",
        "        box2_a2 = lableBox[..., 0:1] + lableBox[..., 2:3] / 2\n",
        "        box2_y2 = lableBox[..., 1:2] + lableBox[..., 3:4] / 2\n",
        "\n",
        "    if box_format == \"corners\":\n",
        "        box1_a1 = PredsBox[..., 0:1]\n",
        "        box1_b1 = PredsBox[..., 1:2]\n",
        "        box1_a2 = PredsBox[..., 2:3]\n",
        "        box1_b2 = PredsBox[..., 3:4]\n",
        "        box2_a1 = lableBox[..., 0:1]\n",
        "        box2_y1 = lableBox[..., 1:2]\n",
        "        box2_a2 = lableBox[..., 2:3]\n",
        "        box2_y2 = lableBox[..., 3:4]\n",
        "\n",
        "    x1 = torch.max(box1_a1, box2_a1)\n",
        "    y1 = torch.max(box1_b1, box2_y1)\n",
        "    x2 = torch.min(box1_a2, box2_a2)\n",
        "    y2 = torch.min(box1_b2, box2_y2)\n",
        "    intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n",
        "    box1_area = (box1_a2 - box1_a1) * (box1_b2 - box1_b1)\n",
        "    box2_area = (box2_a2 - box2_a1) * (box2_y2 - box2_y1)\n",
        "    iou = intersection / (box1_area + box2_area - intersection)\n",
        "\n",
        "    return iou"
      ],
      "metadata": {
        "id": "uuvFaoFbEwfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#added own\n",
        "# allows PIL to load images even if they are truncated or incomplete\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "metadata": {
        "id": "nOAqW-MGFJXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#added own imp\n",
        "class YOLODataset(Dataset):\n",
        "  def __init__(self, csv_file, ImgDir, LableDir, anchors,\n",
        "               ImageSize=416, sp=[13,26,52], cp=20, transform=None):\n",
        "    self.annotations = pd.read_csv(csv_file)\n",
        "    self.ImgDir = ImgDir\n",
        "    self.LableDir = LableDir\n",
        "    self.transform = transform\n",
        "    self.sp = sp\n",
        "\n",
        "    self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2]) # For all 3 scales\n",
        "    self.num_anchors = self.anchors.shape[0]\n",
        "    self.num_anchors_per_scale = self.num_anchors // 3\n",
        "\n",
        "    self.cp = cp\n",
        "\n",
        "    self.ignore_iou_thresh = 0.5\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    label_path = os.path.join(self.LableDir, self.annotations.iloc[index, 1])\n",
        "    boxx = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist() # np.roll with shift 4 on axis 1: [class, x, y, w, h] --> [x, y, w, h, class]\n",
        "\n",
        "    img_path = os.path.join(self.ImgDir, self.annotations.iloc[index, 0])\n",
        "    image = Image.open(img_path)\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    targets = [torch.zeros((self.num_anchors // 3, sp, sp, 6)) for sp in self.sp] # 6 because objectness score, bounding box coordinates (x, y, w, h), class label\n",
        "\n",
        "    for box in boxx:\n",
        "      iou_anchors = WeidthHeight(torch.tensor(box[2:4]), self.anchors) # IOU from height and width\n",
        "      anchor_indices = iou_anchors.argsort(descending=True, dim=0) # Sorting sucht that the first is the best anchor\n",
        "\n",
        "      x, y, width, height, class_label = box\n",
        "      has_anchor = [False, False, False] # Make sure there is an anchor for each of three scales for each bounding box\n",
        "\n",
        "      for anchor_idx in anchor_indices:\n",
        "        scale_idx = anchor_idx // self.num_anchors_per_scale # scale_idx is either 0,1,2: 0-->13x13, 1:-->26x26, 2:-->52x52\n",
        "        anchor_on_scale = anchor_idx % self.num_anchors_per_scale # In each scale, choosing the anchor thats either 0,1,2\n",
        "\n",
        "        sp = self.sp[scale_idx]\n",
        "        i, j = int(sp*y), int(sp*x) # x=0.5, sp=13 --> int(6.5) = 6 | i=y cell, j=x cell\n",
        "        anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
        "\n",
        "        if not anchor_taken and not has_anchor[scale_idx]:\n",
        "          targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
        "          x_cell, y_cell = sp*x - j, sp*y - i # 6.5 - 6 = 0.5 such that they are between [0,1]\n",
        "          width_cell, height_cell = (\n",
        "              width*sp, # sp=13, width=0.5, 6.5\n",
        "              height*sp\n",
        "          )\n",
        "\n",
        "          box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n",
        "\n",
        "          targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
        "          targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
        "          has_anchor[scale_idx] = True\n",
        "\n",
        "        # Even if the same grid shares another anchor having iou>ignore_iou_thresh then,\n",
        "        elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
        "          targets[scale_idx][anchor_on_scale, i, j, 0] = -1 # ignore this prediction\n",
        "\n",
        "    return image, tuple(targets)\n",
        "\n",
        "    # Define dataset\n",
        "# class YOLODataset(Dataset):\n",
        "#     def __init__(self, csv_file, transform=None):\n",
        "#         self.data_frame = pd.read_csv(csv_file)\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data_frame)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         img_name = os.path.join(self.data_frame.iloc[idx, 0])\n",
        "#         image = Image.open(img_name)\n",
        "#         boxes = self.data_frame.iloc[idx, 1:]\n",
        "\n",
        "#         if self.transform:\n",
        "#             image = self.transform(image)\n",
        "\n",
        "#         return image, boxes\n"
      ],
      "metadata": {
        "id": "mmYZNNBFGQuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.Resize((416, 416)), transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "Zj_rMGdgkSMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#added own imp\n",
        "def get_loaders(train_csv_path, test_csv_path):\n",
        "\n",
        "    train_dataset = YOLODataset(\n",
        "        train_csv_path,\n",
        "        transform=transform,\n",
        "        sp=[ImageSize // 32, ImageSize // 16, ImageSize // 8],\n",
        "        ImgDir=DirImage,\n",
        "        LableDir=DirLable,\n",
        "        anchors=ANCHORS,\n",
        "    )\n",
        "    test_dataset = YOLODataset(\n",
        "        test_csv_path,\n",
        "        transform=transform,\n",
        "        sp=[ImageSize // 32, ImageSize // 16, ImageSize // 8],\n",
        "        ImgDir=DirImage,\n",
        "        LableDir=DirLable,\n",
        "        anchors=ANCHORS,\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=SizeOfBatch,\n",
        "        shuffle=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        dataset=test_dataset,\n",
        "        batch_size=SizeOfBatch,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "#     # Define data loaders\n",
        "# def get_loaders(csv_file, batch_size, num_workers=0, shuffle=True):\n",
        "#     transform = transforms.Compose([\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Resize((416, 416)),\n",
        "#     ])\n",
        "\n",
        "#     dataset = YOLODataset(csv_file=csv_file, transform=transform)\n",
        "#     return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "KrFm4bATG-Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_average_precision(\n",
        "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", NumClasses=4\n",
        "):\n",
        "\n",
        "    # list storing all AP for respective classes\n",
        "    average_precisions = []\n",
        "\n",
        "    # used for numerical stability later on\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    for c in range(NumClasses):\n",
        "        detections = []\n",
        "        ground_truths = []\n",
        "\n",
        "        for detection in pred_boxes:\n",
        "            if detection[1] == c:\n",
        "                detections.append(detection)\n",
        "\n",
        "        for true_box in true_boxes:\n",
        "            if true_box[1] == c:\n",
        "                ground_truths.append(true_box)\n",
        "\n",
        "        amount_boxx = Counter([gt[0] for gt in ground_truths])\n",
        "\n",
        "        for key, val in amount_boxx.items():\n",
        "            amount_boxx[key] = torch.zeros(val)\n",
        "\n",
        "        # sort by box probabilities which is index 2\n",
        "        detections.sort(key=lambda x: x[2], reverse=True)\n",
        "        TP = torch.zeros((len(detections)))\n",
        "        FP = torch.zeros((len(detections)))\n",
        "        total_true_boxx = len(ground_truths)\n",
        "\n",
        "        # If none exists for this class then we can safely skip\n",
        "        if total_true_boxx == 0:\n",
        "            continue\n",
        "\n",
        "        for detection_idx, detection in enumerate(detections):\n",
        "            ground_truth_img = [\n",
        "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
        "            ]\n",
        "\n",
        "            num_gts = len(ground_truth_img)\n",
        "            best_iou = 0\n",
        "\n",
        "            for idx, gt in enumerate(ground_truth_img):\n",
        "                iou = InterctionOverUnion(\n",
        "                    torch.tensor(detection[3:]),\n",
        "                    torch.tensor(gt[3:]),\n",
        "                    box_format=box_format,\n",
        "                )\n",
        "\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_idx = idx\n",
        "\n",
        "            if best_iou > iou_threshold:\n",
        "                # only detect ground truth detection once\n",
        "                if amount_boxx[detection[0]][best_gt_idx] == 0:\n",
        "                    # true positive and add this bounding box to seen\n",
        "                    TP[detection_idx] = 1\n",
        "                    amount_boxx[detection[0]][best_gt_idx] = 1\n",
        "                else:\n",
        "                    FP[detection_idx] = 1\n",
        "\n",
        "            # if IOU is lower then the detection is a false positive\n",
        "            else:\n",
        "                FP[detection_idx] = 1\n",
        "\n",
        "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
        "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
        "        recalls = TP_cumsum / (total_true_boxx + epsilon)\n",
        "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
        "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "        # torch.trapz for numerical integration\n",
        "        average_precisions.append(torch.trapz(precisions, recalls))\n",
        "\n",
        "    return sum(average_precisions) / len(average_precisions)"
      ],
      "metadata": {
        "id": "XqvAW5UoizM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#added own\n",
        "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# WorkersNo = 4\n",
        "# SizeOfBatch = 32\n",
        "# ImageSize = 416\n",
        "# ClassesNo = 20\n",
        "# RateOfLearning = 1e-5\n",
        "# epochsno = 150\n",
        "# ThresholdConf = 0.8\n",
        "# ThreshMap = 0.5\n",
        "# ThreshNms = 0.45\n",
        "# sp = [ImageSize // 32, ImageSize // 16, ImageSize // 8]\n",
        "\n",
        "DirImage = \"/kaggle/input/pascalvoc-yolo/images\"\n",
        "DirLable = \"/kaggle/input/pascalvoc-yolo/labels\"\n",
        "\n",
        "ANCHORS = [\n",
        "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
        "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
        "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
        "]\n",
        "\n",
        "\n",
        "AllClacess = [\n",
        "    \"aeroplane\",\n",
        "    \"bicycle\",\n",
        "    \"bird\",\n",
        "    \"boat\",\n",
        "    \"bottle\",\n",
        "    \"bus\",\n",
        "    \"car\",\n",
        "    \"cat\",\n",
        "    \"chair\",\n",
        "    \"cow\",\n",
        "    \"diningtable\",\n",
        "    \"dog\",\n",
        "    \"horse\",\n",
        "    \"motorbike\",\n",
        "    \"person\",\n",
        "    \"pottedplant\",\n",
        "    \"sheep\",\n",
        "    \"sofa\",\n",
        "    \"train\",\n",
        "    \"tvmonitor\"\n",
        "]"
      ],
      "metadata": {
        "id": "lRTkY-6e6Cb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_evaluation_boxx(\n",
        "    loader,\n",
        "    model,\n",
        "    iou_threshold,\n",
        "    anchors,\n",
        "    threshold,\n",
        "    box_format=\"midpoint\"\n",
        "):\n",
        "\n",
        "    # make sure model is in eval before get boxx\n",
        "    model.eval()\n",
        "    train_idx = 0\n",
        "    all_pred_boxes = []\n",
        "    all_true_boxes = []\n",
        "    for batch_idx, (x, labels) in enumerate(loader):\n",
        "        x = x.float().to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = model(x)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        boxx = [[] for _ in range(batch_size)]\n",
        "        for i in range(3):\n",
        "            sp = predictions[i].shape[2] # grid cell size for each predictions\n",
        "            anchor = torch.tensor([*anchors[i]]).to(DEVICE) * sp # anchor for each grid, prediction type\n",
        "            boxes_scale_i = cells_to_boxx( # get boxx for each image in the batch\n",
        "                predictions[i], anchor, sp=sp, is_preds=True\n",
        "            )\n",
        "            for idx, (box) in enumerate(boxes_scale_i): # for each image, append the bbox to corr. boxx[idx]\n",
        "                boxx[idx] += box\n",
        "\n",
        "        # we just want one bbox for each label, not one for each scale\n",
        "        true_boxx = cells_to_boxx(\n",
        "            labels[2], anchor, sp=sp, is_preds=False\n",
        "        )\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "            nms_boxes = non_max_suppression(\n",
        "                boxx[idx],\n",
        "                iou_threshold=iou_threshold,\n",
        "                threshold=threshold,\n",
        "                box_format=box_format,\n",
        "            )\n",
        "\n",
        "            for nms_box in nms_boxes:\n",
        "                all_pred_boxes.append([train_idx] + nms_box)\n",
        "\n",
        "            for box in true_boxx[idx]:\n",
        "                if box[1] > threshold:\n",
        "                    all_true_boxes.append([train_idx] + box)\n",
        "\n",
        "            train_idx += 1\n",
        "\n",
        "    model.train()\n",
        "    return all_pred_boxes, all_true_boxes"
      ],
      "metadata": {
        "id": "drSirQw4jJNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#added own imp\n",
        "def cells_to_boxx(predictions, anchors, sp, is_preds=True):\n",
        "\n",
        "    SizeOfBatch = predictions.shape[0]\n",
        "    num_anchors = len(anchors)\n",
        "    box_predictions = predictions[..., 1:5]\n",
        "    if is_preds:\n",
        "        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n",
        "        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n",
        "        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n",
        "        scores = torch.sigmoid(predictions[..., 0:1])\n",
        "        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n",
        "    else:\n",
        "        scores = predictions[..., 0:1]\n",
        "        best_class = predictions[..., 5:6]\n",
        "\n",
        "    cell_indices = (\n",
        "        torch.arange(sp)\n",
        "        .repeat(predictions.shape[0], 3, sp, 1)\n",
        "        .unsqueeze(-1)\n",
        "        .to(predictions.DEVICE)\n",
        "    )\n",
        "    x = 1 / sp * (box_predictions[..., 0:1] + cell_indices)\n",
        "    y = 1 / sp * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n",
        "    w_h = 1 / sp * box_predictions[..., 2:4]\n",
        "    converted_boxx = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(SizeOfBatch, num_anchors * sp * sp, 6)\n",
        "    return converted_boxx.tolist()\n",
        "\n",
        "# Define bounding box conversion\n",
        "# def cells_to_boxx(prediction, anchors, img_size=416):\n",
        "#     grid_size = prediction.shape[2]\n",
        "#     box_x, box_y = torch.meshgrid([torch.arange(grid_size)] * 2, indexing='ij')\n",
        "#     box_x = box_x.to(prediction.DEVICE)\n",
        "#     box_y = box_y.to(prediction.DEVICE)\n",
        "\n",
        "#     prediction = prediction.reshape(-1, grid_size, grid_size, 3, 5 + 20)\n",
        "#     prediction[..., :2] = torch.sigmoid(prediction[..., :2]) + torch.cat([box_x.unsqueeze(-1), box_y.unsqueeze(-1)], dim=-1).float()\n",
        "#     prediction[..., 2:4] = torch.exp(prediction[..., 2:4]) * torch.tensor(anchors).to(prediction.DEVICE).unsqueeze(0).unsqueeze(0)\n",
        "#     prediction[..., :4] *= img_size\n",
        "\n",
        "#     return prediction"
      ],
      "metadata": {
        "id": "9sWupTfzIDnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#added own imp\n",
        "class YoloLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(YoloLoss, self).__init__()\n",
        "    self.mse = nn.MSELoss() # For bounding box loss\n",
        "    self.bce = nn.BCEWithLogitsLoss() # For multi-label prediction: Binary cross entropy\n",
        "    self.entropy = nn.CrossEntropyLoss() # For classification\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Constants for significance of obj, or no obj.\n",
        "    self.lambda_class = 1\n",
        "    self.lambda_noobj = 10\n",
        "    self.lambda_obj = 1\n",
        "    self.lambda_box = 10\n",
        "\n",
        "  def forward(self, predictions, target, anchors):\n",
        "    obj = target[..., 0] == 1\n",
        "    noobj = target[..., 0] == 0\n",
        "\n",
        "    no_object_loss = self.bce(\n",
        "        (predictions[..., 0:1][noobj]), (target[..., 0:1][noobj])\n",
        "    )\n",
        "\n",
        "    anchors = anchors.reshape(1,3,1,1,2) # Anchors initial shape 3x2 --> 3 anchor boxes each of certain hxw (2)\n",
        "\n",
        "    # box_preds = [..., sigmoid(x), sigmoid(y), [p_w * exp(t_w)], [p_h * exp(t_h)], ...]\n",
        "    box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n",
        "\n",
        "    # iou between predicted box and target box\n",
        "    ious = InterctionOverUnion(box_preds[obj], target[..., 1:5][obj]).detach()\n",
        "\n",
        "    object_loss = self.bce(\n",
        "        (predictions[..., 0:1][obj]), (ious * target[..., 0:1][obj]) # target * iou because only intersected part object loss calc\n",
        "    )\n",
        "\n",
        "    predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3]) # x, y to be between [0,1]\n",
        "    target[..., 3:5] = torch.log(\n",
        "        (1e-6 + target[..., 3:5] / anchors)\n",
        "    ) # Exponential of hxw (taking log because opp. of exp)\n",
        "\n",
        "    box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n",
        "\n",
        "    class_loss = self.entropy(\n",
        "        (predictions[..., 5:][obj]), (target[..., 5][obj].long())\n",
        "    )\n",
        "\n",
        "    return(\n",
        "        self.lambda_box * box_loss\n",
        "        + self.lambda_obj * object_loss\n",
        "        + self.lambda_noobj * no_object_loss\n",
        "        + self.lambda_class * class_loss\n",
        "    )\n",
        "\n",
        "# Define loss function for YOLOv3\n",
        "# class YoloLoss(nn.Module):\n",
        "#     def __init__(self, anchors, num_classes):\n",
        "#         super(YoloLoss, self).__init__()\n",
        "#         self.anchors = anchors\n",
        "#         self.num_classes = num_classes\n",
        "\n",
        "#     def forward(self, predictions, targets):\n",
        "#         # Implement loss calculation here\n",
        "#         return loss\n"
      ],
      "metadata": {
        "id": "Ewuf1zjJINfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#added own imp\n",
        "def plot_image(image, boxes):\n",
        "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
        "    cmap = plt.get_cmap(\"tab20b\")\n",
        "    class_labels = AllClacess\n",
        "    colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n",
        "    im = np.array(image)\n",
        "    height, width, _ = im.shape\n",
        "\n",
        "    # Create figure and axes\n",
        "    fig, ax = plt.subplots(1)\n",
        "    # Display the image\n",
        "    ax.imshow(im)\n",
        "\n",
        "    # Create a Rectangle patch\n",
        "    for box in boxes:\n",
        "        assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n",
        "        class_pred = box[0]\n",
        "        box = box[2:]\n",
        "        UpperLeft_x = box[0] - box[2] / 2\n",
        "        UpperLeft_y = box[1] - box[3] / 2\n",
        "        rect = patches.Rectangle(\n",
        "            (UpperLeft_x * width, UpperLeft_y * height),\n",
        "            box[2] * width,\n",
        "            box[3] * height,\n",
        "            linewidth=2,\n",
        "            edgecolor=colors[int(class_pred)],\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "        # Add the patch to the Axes\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(\n",
        "            UpperLeft_x * width,\n",
        "            UpperLeft_y * height,\n",
        "            s=class_labels[int(class_pred)],\n",
        "            color=\"white\",\n",
        "            verticalalignment=\"top\",\n",
        "            bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n",
        "        )\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# # Define plotting function\n",
        "#     def plot_image(img, boxes):\n",
        "#     fig, ax = plt.subplots(1, figsize=(12, 9))\n",
        "#     ax.imshow(img)\n",
        "\n",
        "#     for box in boxes:\n",
        "#         x, y, w, h, conf, cls = box\n",
        "#         rect = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
        "#         ax.add_patch(rect)\n",
        "#         plt.text(x, y, f'{cls} {conf:.2f}', color='red', fontsize=12)\n",
        "\n",
        "#     plt.show()"
      ],
      "metadata": {
        "id": "k_agKFIIIceJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "history_loss = []\n",
        "\n",
        "for epoch in tqdm(range(epochsno), desc=\"Epochs\"):\n",
        "  model.train()\n",
        "\n",
        "  losses = []\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  for batch_idx, (x,y) in enumerate(train_loader):\n",
        "    x = x.to(DEVICE)\n",
        "    y0, y1, y2 = (y[0].to(DEVICE),\n",
        "                  y[1].to(DEVICE),\n",
        "                  y[2].to(DEVICE))\n",
        "\n",
        "    with torch.cuda.amp.autocast():\n",
        "      out = model(x)\n",
        "      loss = (\n",
        "          loss_fn(out[0], y0, scaled_anchors[0])\n",
        "          + loss_fn(out[1], y1, scaled_anchors[1])\n",
        "          + loss_fn(out[2], y2, scaled_anchors[2])\n",
        "      )\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "  end_time = time.time()\n",
        "  epoch_duration = end_time - start_time\n",
        "\n",
        "  history_loss.append(sum(losses)/len(losses))\n",
        "\n",
        "  if (epoch+1) % 10 == 0:\n",
        "    tqdm.write(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds\")\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochsno}], \"\n",
        "          f\"Loss: {sum(losses)/len(losses):.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), f'/kaggle/working/Yolov3_epoch{epoch+1}.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7h1wi7LOtO_",
        "outputId": "5389deb4-abf2-4b60-a4b0-2bc11afc4367"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:   0%|          | 0/150 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "Epochs:   6%|▌         | 9/150 [09:28<2:11:03, 55.77s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 completed in 55.52 seconds\n",
            "Epoch [10/150], Loss: 26.0588\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  13%|█▎        | 19/150 [18:45<1:59:51, 54.89s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20 completed in 53.61 seconds\n",
            "Epoch [20/150], Loss: 16.6168\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  19%|█▉        | 29/150 [28:07<1:54:46, 56.91s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30 completed in 55.14 seconds\n",
            "Epoch [30/150], Loss: 13.0076\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  26%|██▌       | 39/150 [37:40<1:44:29, 56.48s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40 completed in 54.23 seconds\n",
            "Epoch [40/150], Loss: 10.7729\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  33%|███▎      | 49/150 [46:54<1:31:53, 54.59s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50 completed in 54.21 seconds\n",
            "Epoch [50/150], Loss: 9.4248\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  39%|███▉      | 59/150 [56:16<1:24:45, 55.88s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 60 completed in 54.72 seconds\n",
            "Epoch [60/150], Loss: 8.0632\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  46%|████▌     | 69/150 [1:08:00<1:48:26, 80.33s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 70 completed in 57.32 seconds\n",
            "Epoch [70/150], Loss: 7.1565\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  53%|█████▎    | 79/150 [1:18:04<1:07:11, 56.78s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 80 completed in 54.13 seconds\n",
            "Epoch [80/150], Loss: 6.0814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  59%|█████▉    | 89/150 [1:27:21<55:49, 54.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 90 completed in 54.73 seconds\n",
            "Epoch [90/150], Loss: 6.0831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  61%|██████    | 91/150 [1:28:18<54:30, 55.43s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = YOLOv3(NumClasses=NumClasses).to(DEVICE)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(), lr=RateOfLearning\n",
        ")\n",
        "loss_fn = YoloLoss()\n",
        "\n",
        "# Scaler\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Train-Test Loader\n",
        "train_loader, test_loader = get_loaders(\n",
        "    train_csv_path='/kaggle/input/pascalvoc-yolo/8examples.csv', test_csv_path='/kaggle/input/pascalvoc-yolo/8examples.csv'\n",
        ")\n",
        "\n",
        "# Anchors\n",
        "scaled_anchors = (\n",
        "    torch.tensor(ANCHORS) * torch.tensor([13,26,52]).unsqueeze(1).unsqueeze(1).repeat(1,3,2)\n",
        ").to(DEVICE)"
      ],
      "metadata": {
        "id": "Cv1Y_uDpjc9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd4w8IsGt17f"
      },
      "outputs": [],
      "source": [
        "# # Example usage\n",
        "# if __name__ == \"__main__\":\n",
        "\n",
        "#     for epoch in range(10):\n",
        "#         model.train()\n",
        "#         running_loss = 0.0\n",
        "\n",
        "#         for images, targets in tqdm(train_loader):\n",
        "#           if images is None or targets is None:\n",
        "#             continue  # Skip the iteration if the image or target is None\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             outputs = model(images)\n",
        "#             loss = criterion(outputs, targets)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             running_loss += loss.item()\n",
        "\n",
        "#         avg_loss = running_loss / len(train_loader)\n",
        "#         print(f'Epoch {epoch+1}, Loss: {avg_loss}')\n",
        "\n",
        "#         # Log metrics to wandb\n",
        "#         wandb.log({\n",
        "#             \"epoch\": epoch + 1,\n",
        "#             \"loss\": avg_loss\n",
        "#         })\n",
        "\n",
        "#         # Optionally log model\n",
        "#         wandb.watch(model, log=\"all\", log_freq=10)\n",
        "\n",
        "#     # Save the model\n",
        "#     torch.save(model.state_dict(), \"yolov3_model.pth\")\n",
        "#     wandb.save(\"yolov3_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "if not os.path.exists('checkpoints'):\n",
        "    os.makedirs('checkpoints')\n",
        "\n",
        "N_EPOCHS = wandb.config.epochs\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    wandb.log({\"train_loss\": train_loss, \"train_accuracy\": train_acc,\n",
        "               \"valid_loss\": valid_loss, \"valid_accuracy\": valid_acc, \"valid_bleu\": bleu,\n",
        "               \"epoch\": epoch + 1, \"epoch_time_mins\": epoch_mins, \"epoch_time_secs\": epoch_secs})\n",
        "\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "mW6BZRhBua7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Convert outputs to predictions\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate F1 score for the epoch\n",
        "    epoch_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    # Log metrics to wandb\n",
        "    wandb.log({\"epoch\": epoch, \"loss\": running_loss / len(data_loader), \"f1_score\": epoch_f1})\n"
      ],
      "metadata": {
        "id": "ztN_vUmpfoNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    for inputs, labels in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    # Log metrics to wandb\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch,\n",
        "        \"loss\": epoch_loss,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1\n",
        "    })\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Inference on a single image\n",
        "def infer_image(image_path):\n",
        "    model.eval()\n",
        "    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
        "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        _, pred = torch.max(output, 1)\n",
        "        print(f'Predicted class: {pred.item()}')\n",
        "\n",
        "# Example usage for inference\n",
        "infer_image('path_to_your_image.png')"
      ],
      "metadata": {
        "id": "cH5uSU_Zi3rS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}