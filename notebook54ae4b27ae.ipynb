{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["yjwnv6MYoEgc"],"gpuType":"A100","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1556326,"sourceType":"datasetVersion","datasetId":918769},{"sourceId":7240078,"sourceType":"datasetVersion","datasetId":4193279}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":12211.677353,"end_time":"2023-12-19T20:51:56.359253","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-12-19T17:28:24.6819","version":"2.4.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wandb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2GwwOx1o6qp7","outputId":"29369caf-9b82-4f9f-e7de-a473baae65e8","execution":{"iopub.status.busy":"2024-08-07T11:58:17.139140Z","iopub.execute_input":"2024-08-07T11:58:17.139609Z","iopub.status.idle":"2024-08-07T11:58:34.087523Z","shell.execute_reply.started":"2024-08-07T11:58:17.139574Z","shell.execute_reply":"2024-08-07T11:58:34.085981Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.17.4)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.8.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Imports\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport numpy as np\nimport os\nimport torch.optim as optim\nimport torch\nimport pandas as pd\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image, ImageFile\nfrom tqdm import tqdm\nimport time\nfrom collections import Counter\nimport wandb","metadata":{"id":"qwgTtAyp3C7K","papermill":{"duration":4.058162,"end_time":"2023-12-19T17:28:39.273964","exception":false,"start_time":"2023-12-19T17:28:35.215802","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-07T11:58:34.089950Z","iopub.execute_input":"2024-08-07T11:58:34.090357Z","iopub.status.idle":"2024-08-07T11:58:39.157643Z","shell.execute_reply.started":"2024-08-07T11:58:34.090317Z","shell.execute_reply":"2024-08-07T11:58:39.156443Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# YOLO v3 model architecture","metadata":{"id":"yjwnv6MYoEgc","papermill":{"duration":0.011698,"end_time":"2023-12-19T17:28:39.297965","exception":false,"start_time":"2023-12-19T17:28:39.286267","status":"completed"},"tags":[]}},{"cell_type":"code","source":"config = [\n    (32, 3, 1),\n    (128, 3, 1),  # New layer added here\n    (64, 3, 2),\n    [\"list\", 1],\n    (128, 3, 2),\n    [\"list\", 2],\n    (256, 3, 2),\n    [\"list\", 8],\n    (512, 3, 2),\n    [\"list\", 8],\n    (1024, 3, 2),\n    [\"list\", 4],  # To this point is Darknet-53\n\n    (512, 1, 1),\n    (1024, 3, 1),\n    \"sp\",\n    (256, 1, 1),\n    \"up\",\n    (256, 1, 1),\n    (512, 3, 1),\n    \"sp\",\n    (128, 1, 1),\n    \"up\",\n    (128, 1, 1),\n    (256, 3, 1),\n    \"sp\",\n]\n","metadata":{"id":"zs0uKuG55xC_","papermill":{"duration":0.022597,"end_time":"2023-12-19T17:28:39.332406","exception":false,"start_time":"2023-12-19T17:28:39.309809","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-07T11:58:39.159207Z","iopub.execute_input":"2024-08-07T11:58:39.159738Z","iopub.status.idle":"2024-08-07T11:58:39.170521Z","shell.execute_reply.started":"2024-08-07T11:58:39.159697Z","shell.execute_reply":"2024-08-07T11:58:39.168653Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class CNN_Block(nn.Module):\n  def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n    super(CNN_Block, self).__init__()\n    self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs) # If batchnorm layer(bn_act) is true, then bias is False\n    self.bn = nn.BatchNorm2d(out_channels)\n    self.leaky = nn.LeakyReLU(0.1)\n    self.use_bn_act = bn_act\n\n  def forward(self, x):\n    if self.use_bn_act:\n      return self.leaky(self.bn(self.conv(x)))\n    else:\n      return self.conv(x)\n\n\nclass Residual_Block(nn.Module):\n  def __init__(self, channels, use_residual=True, num_repeats=1):\n    super(Residual_Block, self).__init__()\n    self.layers = nn.ModuleList() # Like regular python list, but is container for pytorch nn modules\n\n    for repeat in range(num_repeats):\n      self.layers += [\n          nn.Sequential(\n            CNN_Block(channels, channels//2, kernel_size=1),\n            CNN_Block(channels//2, channels, kernel_size=3, padding=1)\n          )\n      ]\n\n    self.use_residual = use_residual\n    self.num_repeats = num_repeats\n\n  def forward(self, x):\n    for layer in self.layers:\n      if self.use_residual:\n        x = x + layer(x)\n      else:\n        x = layer(x)\n\n    return x","metadata":{"id":"Q_zaXn4YQCl7","papermill":{"duration":0.02402,"end_time":"2023-12-19T17:28:39.368315","exception":false,"start_time":"2023-12-19T17:28:39.344295","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-07T11:58:39.173372Z","iopub.execute_input":"2024-08-07T11:58:39.174485Z","iopub.status.idle":"2024-08-07T11:58:39.188569Z","shell.execute_reply.started":"2024-08-07T11:58:39.174437Z","shell.execute_reply":"2024-08-07T11:58:39.187398Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Prediction_Scale(nn.Module):\n  def __init__(self, in_channels, ClassesNo):\n    super(Prediction_Scale, self).__init__()\n    self.pred = nn.Sequential(\n        CNN_Block(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n        CNN_Block(2 * in_channels, (ClassesNo + 5) * 3, bn_act=False, kernel_size=1), # (ClassesNo + 5) * 3 --> (20+5) for each anchor box which in total is 3\n    )\n    self.ClassesNo = ClassesNo\n\n  def forward(self, x):\n    return (\n        self.pred(x)\n        .reshape(x.shape[0], 3, self.ClassesNo + 5, x.shape[2], x.shape[3]) # [batch_size, anchor_boxes, prediction(25), grid_h, grid_w]\n        .permute(0, 1, 3, 4, 2) # [batch_size, anchor_boxes, grid_h, grid_w, prediction(25)]\n      )","metadata":{"id":"IUrOUQNeXDcG","papermill":{"duration":0.021337,"end_time":"2023-12-19T17:28:39.401877","exception":false,"start_time":"2023-12-19T17:28:39.38054","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-07T11:58:39.189829Z","iopub.execute_input":"2024-08-07T11:58:39.190153Z","iopub.status.idle":"2024-08-07T11:58:39.215632Z","shell.execute_reply.started":"2024-08-07T11:58:39.190126Z","shell.execute_reply":"2024-08-07T11:58:39.214255Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class YOLOv3(nn.Module):\n  def __init__(self, in_channels=3, ClassesNo=20):\n    super(YOLOv3, self).__init__()\n    self.ClassesNo = ClassesNo\n    self.in_channels = in_channels\n    self.layers = self._create_conv_layers()\n\n  def forward(self, x):\n    outputs = []\n    route_connections = []\n\n    for layer in self.layers:\n      if isinstance(layer, Prediction_Scale):\n        outputs.append(layer(x))\n        continue\n\n      x = layer(x)\n\n      if isinstance(layer, Residual_Block) and layer.num_repeats == 8:\n        route_connections.append(x)\n\n      elif isinstance(layer, nn.Upsample):\n        x = torch.cat([x, route_connections[-1]], dim=1)\n        route_connections.pop()\n\n    return outputs\n\n  def _create_conv_layers(self):\n    layers = nn.ModuleList()\n    in_channels = self.in_channels\n\n    for module in config:\n      if isinstance(module, tuple):\n        out_channels, kernel_size, stride = module\n        layers.append(CNN_Block(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=1 if kernel_size == 3 else 0\n        ))\n        in_channels = out_channels\n\n      elif isinstance(module, list):\n        num_repeats = module[1]\n        layers.append(Residual_Block(in_channels, num_repeats=num_repeats))\n\n      elif isinstance(module, str):\n        if module == \"sp\":\n          layers += [\n              Residual_Block(in_channels, use_residual=False, num_repeats=1),\n              CNN_Block(in_channels, in_channels//2, kernel_size=1),\n              Prediction_Scale(in_channels//2, ClassesNo = self.ClassesNo)\n          ]\n          in_channels = in_channels // 2\n\n        elif module == \"up\":\n          layers.append(nn.Upsample(scale_factor=2))\n          in_channels = in_channels * 3\n\n    return layers\n","metadata":{"id":"iwYyoG0AQXtj","papermill":{"duration":0.026458,"end_time":"2023-12-19T17:28:39.440149","exception":false,"start_time":"2023-12-19T17:28:39.413691","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-07T11:58:39.217197Z","iopub.execute_input":"2024-08-07T11:58:39.217930Z","iopub.status.idle":"2024-08-07T11:58:39.234088Z","shell.execute_reply.started":"2024-08-07T11:58:39.217884Z","shell.execute_reply":"2024-08-07T11:58:39.232657Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def WeidthHeight(boxa, boxb):\n\n    intersection = torch.min(boxa[..., 0], boxb[..., 0]) * torch.min(\n        boxa[..., 1], boxb[..., 1]\n    )\n    union = (\n        boxa[..., 0] * boxa[..., 1] + boxb[..., 0] * boxb[..., 1] - intersection\n    )\n    return intersection / union","metadata":{"id":"4P448jivxbnu","papermill":{"duration":0.021393,"end_time":"2023-12-19T17:28:47.701182","exception":false,"start_time":"2023-12-19T17:28:47.679789","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-07T11:58:39.235995Z","iopub.execute_input":"2024-08-07T11:58:39.236437Z","iopub.status.idle":"2024-08-07T11:58:39.251690Z","shell.execute_reply.started":"2024-08-07T11:58:39.236402Z","shell.execute_reply":"2024-08-07T11:58:39.249811Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def non_max_suppression(boxx, iou_threshold, threshold, box_format=\"corners\"):\n\n    assert type(boxx) == list\n\n    boxx = [box for box in boxx if box[1] > threshold]\n    boxx = sorted(boxx, key=lambda x: x[1], reverse=True)\n    boxx_after_nms = []\n\n    while boxx:\n        chosen_box = boxx.pop(0)\n\n        boxx = [\n            box\n            for box in boxx\n            if box[0] != chosen_box[0]\n            or InterctionOverUnion(\n                torch.tensor(chosen_box[2:]),\n                torch.tensor(box[2:]),\n                box_format=box_format,\n            )\n            < iou_threshold\n        ]\n\n        boxx_after_nms.append(chosen_box)\n\n    return boxx_after_nms","metadata":{"id":"0oIq_NZpxzNi","papermill":{"duration":0.022558,"end_time":"2023-12-19T17:28:47.827509","exception":false,"start_time":"2023-12-19T17:28:47.804951","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-07T11:58:39.254656Z","iopub.execute_input":"2024-08-07T11:58:39.255123Z","iopub.status.idle":"2024-08-07T11:58:39.267905Z","shell.execute_reply.started":"2024-08-07T11:58:39.255077Z","shell.execute_reply":"2024-08-07T11:58:39.266613Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def InterctionOverUnion(PredsBox, lableBox, box_format=\"midpoint\"):\n\n    if box_format == \"midpoint\":\n        box1_a1 = PredsBox[..., 0:1] - PredsBox[..., 2:3] / 2\n        box1_b1 = PredsBox[..., 1:2] - PredsBox[..., 3:4] / 2\n        box1_a2 = PredsBox[..., 0:1] + PredsBox[..., 2:3] / 2\n        box1_b2 = PredsBox[..., 1:2] + PredsBox[..., 3:4] / 2\n        box2_a1 = lableBox[..., 0:1] - lableBox[..., 2:3] / 2\n        box2_y1 = lableBox[..., 1:2] - lableBox[..., 3:4] / 2\n        box2_a2 = lableBox[..., 0:1] + lableBox[..., 2:3] / 2\n        box2_y2 = lableBox[..., 1:2] + lableBox[..., 3:4] / 2\n\n    if box_format == \"corners\":\n        box1_a1 = PredsBox[..., 0:1]\n        box1_b1 = PredsBox[..., 1:2]\n        box1_a2 = PredsBox[..., 2:3]\n        box1_b2 = PredsBox[..., 3:4]\n        box2_a1 = lableBox[..., 0:1]\n        box2_y1 = lableBox[..., 1:2]\n        box2_a2 = lableBox[..., 2:3]\n        box2_y2 = lableBox[..., 3:4]\n\n    x1 = torch.max(box1_a1, box2_a1)\n    y1 = torch.max(box1_b1, box2_y1)\n    x2 = torch.min(box1_a2, box2_a2)\n    y2 = torch.min(box1_b2, box2_y2)\n\n    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n    box1_area = abs((box1_a2 - box1_a1) * (box1_b2 - box1_b1))\n    box2_area = abs((box2_a2 - box2_a1) * (box2_y2 - box2_y1))\n\n    return intersection / (box1_area + box2_area - intersection + 1e-6)","metadata":{"id":"RvwUplkYxqrw","papermill":{"duration":0.027159,"end_time":"2023-12-19T17:28:47.766138","exception":false,"start_time":"2023-12-19T17:28:47.738979","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-07T11:58:39.269815Z","iopub.execute_input":"2024-08-07T11:58:39.270274Z","iopub.status.idle":"2024-08-07T11:58:39.289916Z","shell.execute_reply.started":"2024-08-07T11:58:39.270239Z","shell.execute_reply":"2024-08-07T11:58:39.288338Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def calculate_metrics(all_preds, all_labels, num_classes):\n    # Flatten the lists\n    all_preds = torch.cat([torch.flatten(p) for p in all_preds])\n    all_labels = torch.cat([torch.flatten(l) for l in all_labels])\n\n    # Calculate precision, recall, and F1 score for each class\n    precision = []\n    recall = []\n    f1_score = []\n\n    for c in range(num_classes):\n        tp = ((all_preds == c) & (all_labels == c)).sum().item()\n        fp = ((all_preds == c) & (all_labels != c)).sum().item()\n        fn = ((all_preds != c) & (all_labels == c)).sum().item()\n\n        p = tp / (tp + fp + 1e-6)\n        r = tp / (tp + fn + 1e-6)\n        f1 = 2 * (p * r) / (p + r + 1e-6)\n\n        precision.append(p)\n        recall.append(r)\n        f1_score.append(f1)\n\n    return precision, recall, f1_score\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T11:58:39.296999Z","iopub.execute_input":"2024-08-07T11:58:39.297623Z","iopub.status.idle":"2024-08-07T11:58:39.310492Z","shell.execute_reply.started":"2024-08-07T11:58:39.297570Z","shell.execute_reply":"2024-08-07T11:58:39.308495Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def train(model, train_loader, optimizer, criterion, DEVICE):\n    model.train()\n    total_loss = 0\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        data = data.to(DEVICE)\n        targets = [target.to(DEVICE) for target in targets]\n        \n        optimizer.zero_grad()\n        outputs = model(data)\n        loss = criterion(outputs, targets)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        if batch_idx % 10 == 0:\n            print(f\"Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}\")\n    \n    return total_loss / len(train_loader)\n\ndef evaluate(model, test_loader, criterion, DEVICE):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch_idx, (data, targets) in enumerate(test_loader):\n            data = data.to(DEVICE)\n            targets = [target.to(DEVICE) for target in targets]\n            \n            outputs = model(data)\n            loss = criterion(outputs, targets)\n            \n            total_loss += loss.item()\n            \n            for i in range(len(outputs)):\n                preds = outputs[i]\n                labels = targets[i]\n                all_preds.append(preds.cpu())\n                all_labels.append(labels.cpu())\n    \n    return total_loss / len(test_loader), all_preds, all_labels","metadata":{"execution":{"iopub.status.busy":"2024-08-07T11:58:39.312403Z","iopub.execute_input":"2024-08-07T11:58:39.312811Z","iopub.status.idle":"2024-08-07T11:58:39.331846Z","shell.execute_reply.started":"2024-08-07T11:58:39.312774Z","shell.execute_reply":"2024-08-07T11:58:39.328869Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"id":"6qWub9xCoJQP","papermill":{"duration":0.012761,"end_time":"2023-12-19T17:28:47.853131","exception":false,"start_time":"2023-12-19T17:28:47.84037","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# allows PIL to load images even if they are truncated or incomplete\nImageFile.LOAD_TRUNCATED_IMAGES = True","metadata":{"id":"Lfov94HNpy4C","papermill":{"duration":0.018826,"end_time":"2023-12-19T17:28:47.917203","exception":false,"start_time":"2023-12-19T17:28:47.898377","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-07T11:58:39.333833Z","iopub.execute_input":"2024-08-07T11:58:39.334339Z","iopub.status.idle":"2024-08-07T11:58:39.346906Z","shell.execute_reply.started":"2024-08-07T11:58:39.334277Z","shell.execute_reply":"2024-08-07T11:58:39.345489Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class YOLODataset(Dataset):\n  def __init__(self, csv_file, ImgDir, LableDir, anchors,\n               ImageSize=416, sp=[13,26,52], cp=20, transform=None):\n    self.annotations = pd.read_csv(csv_file)\n    self.ImgDir = ImgDir\n    self.LableDir = LableDir\n    self.transform = transform\n    self.sp = sp\n\n    self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2]) # For all 3 scales\n    self.num_anchors = self.anchors.shape[0]\n    self.num_anchors_per_scale = self.num_anchors // 3\n\n    self.cp = cp\n\n    self.ignore_iou_thresh = 0.5\n\n  def __len__(self):\n    return len(self.annotations)\n\n  def __getitem__(self, index):\n    label_path = os.path.join(self.LableDir, self.annotations.iloc[index, 1])\n    boxx = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist() # np.roll with shift 4 on axis 1: [class, x, y, w, h] --> [x, y, w, h, class]\n\n    img_path = os.path.join(self.ImgDir, self.annotations.iloc[index, 0])\n    image = Image.open(img_path)\n\n    if self.transform:\n      image = self.transform(image)\n\n    targets = [torch.zeros((self.num_anchors // 3, sp, sp, 6)) for sp in self.sp] # 6 because objectness score, bounding box coordinates (x, y, w, h), class label\n\n    for box in boxx:\n      iou_anchors = WeidthHeight(torch.tensor(box[2:4]), self.anchors) # IOU from height and width\n      anchor_indices = iou_anchors.argsort(descending=True, dim=0) # Sorting sucht that the first is the best anchor\n\n      x, y, width, height, class_label = box\n      has_anchor = [False, False, False] # Make sure there is an anchor for each of three scales for each bounding box\n\n      for anchor_idx in anchor_indices:\n        scale_idx = anchor_idx // self.num_anchors_per_scale # scale_idx is either 0,1,2: 0-->13x13, 1:-->26x26, 2:-->52x52\n        anchor_on_scale = anchor_idx % self.num_anchors_per_scale # In each scale, choosing the anchor thats either 0,1,2\n\n        sp = self.sp[scale_idx]\n        i, j = int(sp*y), int(sp*x) # x=0.5, sp=13 --> int(6.5) = 6 | i=y cell, j=x cell\n        anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n\n        if not anchor_taken and not has_anchor[scale_idx]:\n          targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n          x_cell, y_cell = sp*x - j, sp*y - i # 6.5 - 6 = 0.5 such that they are between [0,1]\n          width_cell, height_cell = (\n              width*sp, # sp=13, width=0.5, 6.5\n              height*sp\n          )\n\n          box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n\n          targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n          targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n          has_anchor[scale_idx] = True\n\n        # Even if the same grid shares another anchor having iou>ignore_iou_thresh then,\n        elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n          targets[scale_idx][anchor_on_scale, i, j, 0] = -1 # ignore this prediction\n\n    return image, tuple(targets)\n","metadata":{"id":"UiM8IiJeqIV4","papermill":{"duration":0.031674,"end_time":"2023-12-19T17:28:47.961698","exception":false,"start_time":"2023-12-19T17:28:47.930024","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-07T11:58:39.349174Z","iopub.execute_input":"2024-08-07T11:58:39.349694Z","iopub.status.idle":"2024-08-07T11:58:39.370807Z","shell.execute_reply.started":"2024-08-07T11:58:39.349650Z","shell.execute_reply":"2024-08-07T11:58:39.369513Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms as transforms\ntransform = transforms.Compose([transforms.Resize((416, 416)), transforms.ToTensor()])","metadata":{"id":"BsljMXYuXndg","papermill":{"duration":0.325693,"end_time":"2023-12-19T17:28:48.326146","exception":false,"start_time":"2023-12-19T17:28:48.000453","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-07T11:58:39.372504Z","iopub.execute_input":"2024-08-07T11:58:39.373032Z","iopub.status.idle":"2024-08-07T11:58:41.121332Z","shell.execute_reply.started":"2024-08-07T11:58:39.372975Z","shell.execute_reply":"2024-08-07T11:58:41.120049Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def get_loaders(train_csv_path, test_csv_path):\n\n    train_dataset = YOLODataset(\n        train_csv_path,\n        transform=transform,\n        sp=[ImageSize // 32, ImageSize // 16, ImageSize // 8],\n        ImgDir=DirImage,\n        LableDir=DirLable,\n        anchors=ANCHORS,\n    )\n    test_dataset = YOLODataset(\n        test_csv_path,\n        transform=transform,\n        sp=[ImageSize // 32, ImageSize // 16, ImageSize // 8],\n        ImgDir=DirImage,\n        LableDir=DirLable,\n        anchors=ANCHORS,\n    )\n    train_loader = DataLoader(\n        dataset=train_dataset,\n        batch_size=SizeOfBatch,\n        shuffle=True,\n        drop_last=False,\n    )\n    test_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=SizeOfBatch,\n        shuffle=False,\n        drop_last=False,\n    )\n\n    return train_loader, test_loader","metadata":{"id":"10SzzqKdzExo","papermill":{"duration":0.028829,"end_time":"2023-12-19T17:28:48.374519","exception":false,"start_time":"2023-12-19T17:28:48.34569","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-07T11:58:41.123042Z","iopub.execute_input":"2024-08-07T11:58:41.123922Z","iopub.status.idle":"2024-08-07T11:58:41.133293Z","shell.execute_reply.started":"2024-08-07T11:58:41.123883Z","shell.execute_reply":"2024-08-07T11:58:41.132098Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def mean_average_precision(\n    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", ClassesNo=4\n):\n\n    # list storing all AP for respective classes\n    average_precisions = []\n\n    # used for numerical stability later on\n    epsilon = 1e-6\n\n    for c in range(ClassesNo):\n        detections = []\n        ground_truths = []\n\n        for detection in pred_boxes:\n            if detection[1] == c:\n                detections.append(detection)\n\n        for true_box in true_boxes:\n            if true_box[1] == c:\n                ground_truths.append(true_box)\n\n        amount_boxx = Counter([gt[0] for gt in ground_truths])\n\n        for key, val in amount_boxx.items():\n            amount_boxx[key] = torch.zeros(val)\n\n        # sort by box probabilities which is index 2\n        detections.sort(key=lambda x: x[2], reverse=True)\n        TP = torch.zeros((len(detections)))\n        FP = torch.zeros((len(detections)))\n        total_true_boxx = len(ground_truths)\n\n        # If none exists for this class then we can safely skip\n        if total_true_boxx == 0:\n            continue\n\n        for detection_idx, detection in enumerate(detections):\n            ground_truth_img = [\n                bbox for bbox in ground_truths if bbox[0] == detection[0]\n            ]\n\n            num_gts = len(ground_truth_img)\n            best_iou = 0\n\n            for idx, gt in enumerate(ground_truth_img):\n                iou = InterctionOverUnion(\n                    torch.tensor(detection[3:]),\n                    torch.tensor(gt[3:]),\n                    box_format=box_format,\n                )\n\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_idx = idx\n\n            if best_iou > iou_threshold:\n                # only detect ground truth detection once\n                if amount_boxx[detection[0]][best_gt_idx] == 0:\n                    # true positive and add this bounding box to seen\n                    TP[detection_idx] = 1\n                    amount_boxx[detection[0]][best_gt_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n\n            # if IOU is lower then the detection is a false positive\n            else:\n                FP[detection_idx] = 1\n\n        TP_cumsum = torch.cumsum(TP, dim=0)\n        FP_cumsum = torch.cumsum(FP, dim=0)\n        recalls = TP_cumsum / (total_true_boxx + epsilon)\n        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n        # torch.trapz for numerical integration\n        average_precisions.append(torch.trapz(precisions, recalls))\n\n    return sum(average_precisions) / len(average_precisions)","metadata":{"id":"VtyQqkoWepao","papermill":{"duration":0.032317,"end_time":"2023-12-19T17:28:48.448856","exception":false,"start_time":"2023-12-19T17:28:48.416539","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-07T11:58:41.134729Z","iopub.execute_input":"2024-08-07T11:58:41.135109Z","iopub.status.idle":"2024-08-07T11:58:41.153930Z","shell.execute_reply.started":"2024-08-07T11:58:41.135078Z","shell.execute_reply":"2024-08-07T11:58:41.152575Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def get_evaluation_boxx(\n    loader,\n    model,\n    iou_threshold,\n    anchors,\n    threshold,\n    box_format=\"midpoint\",\n    DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n):\n    # make sure model is in eval before get boxx\n    model.eval()\n    train_idx = 0\n    all_pred_boxes = []\n    all_true_boxes = []\n    for batch_idx, (x, labels) in enumerate(loader):\n        x = x.float().to(DEVICE)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size = x.shape[0]\n        boxx = [[] for _ in range(batch_size)]\n        for i in range(3):\n            sp = predictions[i].shape[2] # grid cell size for each predictions\n            anchor = torch.tensor([*anchors[i]]).to(DEVICE) * sp # anchor for each grid, prediction type\n            boxes_scale_i = cells_to_boxx( # get boxx for each image in the batch\n                predictions[i], anchor, sp=sp, is_preds=True\n            )\n            for idx, (box) in enumerate(boxes_scale_i): # for each image, append the bbox to corr. boxx[idx]\n                boxx[idx] += box\n\n        # we just want one bbox for each label, not one for each scale\n        true_boxx = cells_to_boxx(\n            labels[2], anchor, sp=sp, is_preds=False\n        )\n\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                boxx[idx],\n                iou_threshold=iou_threshold,\n                threshold=threshold,\n                box_format=box_format,\n            )\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_boxx[idx]:\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            train_idx += 1\n\n    model.train()\n    return all_pred_boxes, all_true_boxes","metadata":{"id":"hLlsZs1aepap","papermill":{"duration":0.034433,"end_time":"2023-12-19T17:28:48.529102","exception":false,"start_time":"2023-12-19T17:28:48.494669","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-07T11:58:41.155823Z","iopub.execute_input":"2024-08-07T11:58:41.156208Z","iopub.status.idle":"2024-08-07T11:58:41.170252Z","shell.execute_reply.started":"2024-08-07T11:58:41.156177Z","shell.execute_reply":"2024-08-07T11:58:41.168975Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nWorkersNo = 4\nSizeOfBatch = 32\nImageSize = 416\nClassesNo = 20\nRateOfLearning = 1e-5\nepochsno = 150\nThresholdConf = 0.8\nThreshMap = 0.5\nThreshNms = 0.45\nsp = [ImageSize // 32, ImageSize // 16, ImageSize // 8]\n\nmodel = YOLOv3(ClassesNo=ClassesNo)\n\nDirImage = \"/kaggle/input/pascalvoc-yolo/images\"\nDirLable = \"/kaggle/input/pascalvoc-yolo/labels\"\n\nANCHORS = [\n    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n]\n\n\nAllClacess = [\n    \"aeroplane\",\n    \"bicycle\",\n    \"bird\",\n    \"boat\",\n    \"bottle\",\n    \"bus\",\n    \"car\",\n    \"cat\",\n    \"chair\",\n    \"cow\",\n    \"diningtable\",\n    \"dog\",\n    \"horse\",\n    \"motorbike\",\n    \"person\",\n    \"pottedplant\",\n    \"sheep\",\n    \"sofa\",\n    \"train\",\n    \"tvmonitor\"\n]","metadata":{"id":"9Ai7PMaUteOY","papermill":{"duration":0.371109,"end_time":"2023-12-19T17:28:47.640461","exception":false,"start_time":"2023-12-19T17:28:47.269352","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-07T11:58:41.171818Z","iopub.execute_input":"2024-08-07T11:58:41.172287Z","iopub.status.idle":"2024-08-07T11:58:42.003730Z","shell.execute_reply.started":"2024-08-07T11:58:41.172249Z","shell.execute_reply":"2024-08-07T11:58:42.002418Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def cells_to_boxx(predictions, anchors, sp, is_preds=True):\n\n    SizeOfBatch = predictions.shape[0]\n    num_anchors = len(anchors)\n    box_predictions = predictions[..., 1:5]\n    if is_preds:\n        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n        scores = torch.sigmoid(predictions[..., 0:1])\n        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n    else:\n        scores = predictions[..., 0:1]\n        best_class = predictions[..., 5:6]\n\n    cell_indices = (\n        torch.arange(sp)\n        .repeat(predictions.shape[0], 3, sp, 1)\n        .unsqueeze(-1)\n        .to(predictions.DEVICE)\n    )\n    x = 1 / sp * (box_predictions[..., 0:1] + cell_indices)\n    y = 1 / sp * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n    w_h = 1 / sp * box_predictions[..., 2:4]\n    converted_boxx = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(SizeOfBatch, num_anchors * sp * sp, 6)\n    return converted_boxx.tolist()","metadata":{"id":"Vi_8Z4d5epap","papermill":{"duration":0.029879,"end_time":"2023-12-19T17:28:48.576674","exception":false,"start_time":"2023-12-19T17:28:48.546795","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-07T11:58:42.005393Z","iopub.execute_input":"2024-08-07T11:58:42.005822Z","iopub.status.idle":"2024-08-07T11:58:42.019615Z","shell.execute_reply.started":"2024-08-07T11:58:42.005778Z","shell.execute_reply":"2024-08-07T11:58:42.018003Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Loss","metadata":{"id":"BlYBUIFvh9yd","papermill":{"duration":0.012777,"end_time":"2023-12-19T17:28:48.6023","exception":false,"start_time":"2023-12-19T17:28:48.589523","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class YoloLoss(nn.Module):\n  def __init__(self):\n    super(YoloLoss, self).__init__()\n    self.mse = nn.MSELoss() # For bounding box loss\n    self.bce = nn.BCEWithLogitsLoss() # For multi-label prediction: Binary cross entropy\n    self.entropy = nn.CrossEntropyLoss() # For classification\n    self.sigmoid = nn.Sigmoid()\n\n    # Constants for significance of obj, or no obj.\n    self.lambda_class = 1\n    self.lambda_noobj = 10\n    self.lambda_obj = 1\n    self.lambda_box = 10\n\n  def forward(self, predictions, target, anchors):\n    obj = target[..., 0] == 1\n    noobj = target[..., 0] == 0\n\n    no_object_loss = self.bce(\n        (predictions[..., 0:1][noobj]), (target[..., 0:1][noobj])\n    )\n\n    anchors = anchors.reshape(1,3,1,1,2) # Anchors initial shape 3x2 --> 3 anchor boxes each of certain hxw (2)\n\n    # box_preds = [..., sigmoid(x), sigmoid(y), [p_w * exp(t_w)], [p_h * exp(t_h)], ...]\n    box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n\n    # iou between predicted box and target box\n    ious = InterctionOverUnion(box_preds[obj], target[..., 1:5][obj]).detach()\n\n    object_loss = self.bce(\n        (predictions[..., 0:1][obj]), (ious * target[..., 0:1][obj]) # target * iou because only intersected part object loss calc\n    )\n\n    predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3]) # x, y to be between [0,1]\n    target[..., 3:5] = torch.log(\n        (1e-6 + target[..., 3:5] / anchors)\n    ) # Exponential of hxw (taking log because opp. of exp)\n\n    box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n\n    class_loss = self.entropy(\n        (predictions[..., 5:][obj]), (target[..., 5][obj].long())\n    )\n\n    return(\n        self.lambda_box * box_loss\n        + self.lambda_obj * object_loss\n        + self.lambda_noobj * no_object_loss\n        + self.lambda_class * class_loss\n    )\n","metadata":{"id":"i5nJTWLRiAY1","papermill":{"duration":0.032184,"end_time":"2023-12-19T17:28:48.648025","exception":false,"start_time":"2023-12-19T17:28:48.615841","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-07T11:58:42.022241Z","iopub.execute_input":"2024-08-07T11:58:42.022701Z","iopub.status.idle":"2024-08-07T11:58:42.039442Z","shell.execute_reply.started":"2024-08-07T11:58:42.022664Z","shell.execute_reply":"2024-08-07T11:58:42.038054Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Plot Image","metadata":{"id":"M2mlDBxU2pjJ","papermill":{"duration":0.012422,"end_time":"2023-12-19T17:28:48.673454","exception":false,"start_time":"2023-12-19T17:28:48.661032","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def plot_image(image, boxes):\n    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n    cmap = plt.get_cmap(\"tab20b\")\n    class_labels = AllClacess\n    colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n    im = np.array(image)\n    height, width, _ = im.shape\n\n    # Create figure and axes\n    fig, ax = plt.subplots(1)\n    # Display the image\n    ax.imshow(im)\n\n    # Create a Rectangle patch\n    for box in boxes:\n        assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n        class_pred = box[0]\n        box = box[2:]\n        UpperLeft_x = box[0] - box[2] / 2\n        UpperLeft_y = box[1] - box[3] / 2\n        rect = patches.Rectangle(\n            (UpperLeft_x * width, UpperLeft_y * height),\n            box[2] * width,\n            box[3] * height,\n            linewidth=2,\n            edgecolor=colors[int(class_pred)],\n            facecolor=\"none\",\n        )\n        # Add the patch to the Axes\n        ax.add_patch(rect)\n        plt.text(\n            UpperLeft_x * width,\n            UpperLeft_y * height,\n            s=class_labels[int(class_pred)],\n            color=\"white\",\n            verticalalignment=\"top\",\n            bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n        )\n\n    plt.show()","metadata":{"id":"Xjl_AldR2rX9","papermill":{"duration":0.02615,"end_time":"2023-12-19T17:28:48.712437","exception":false,"start_time":"2023-12-19T17:28:48.686287","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-07T11:58:42.040896Z","iopub.execute_input":"2024-08-07T11:58:42.041258Z","iopub.status.idle":"2024-08-07T11:58:42.055047Z","shell.execute_reply.started":"2024-08-07T11:58:42.041228Z","shell.execute_reply":"2024-08-07T11:58:42.053962Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Compile the model\noptimizer = torch.optim.Adam(\n    model.parameters(), lr=RateOfLearning\n)\nloss_fn = YoloLoss()\n\n# Scaler\nscaler = torch.cuda.amp.GradScaler()\n\n# Train-Test Loader\ntrain_loader, test_loader = get_loaders(\n    train_csv_path='/kaggle/input/pascalvoc-yolo/8examples.csv', test_csv_path='/kaggle/input/pascalvoc-yolo/8examples.csv'\n)\n\n# Anchors\nscaled_anchors = (\n    torch.tensor(ANCHORS) * torch.tensor([13,26,52]).unsqueeze(1).unsqueeze(1).repeat(1,3,2)\n).to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T11:58:42.056449Z","iopub.execute_input":"2024-08-07T11:58:42.056820Z","iopub.status.idle":"2024-08-07T11:58:42.130446Z","shell.execute_reply.started":"2024-08-07T11:58:42.056779Z","shell.execute_reply":"2024-08-07T11:58:42.129239Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"history_loss = []\n\nfor epoch in tqdm(range(epochsno), desc=\"Epochs\"):\n  model.train()\n\n  losses = []\n\n  start_time = time.time()\n\n  for batch_idx, (x,y) in enumerate(train_loader):\n    x = x.to(DEVICE)\n    y0, y1, y2 = (y[0].to(DEVICE),\n                  y[1].to(DEVICE),\n                  y[2].to(DEVICE))\n\n    with torch.cuda.amp.autocast():\n      out = model(x)\n      loss = (\n          loss_fn(out[0], y0, scaled_anchors[0])\n          + loss_fn(out[1], y1, scaled_anchors[1])\n          + loss_fn(out[2], y2, scaled_anchors[2])\n      )\n\n    losses.append(loss.item())\n\n    optimizer.zero_grad()\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\n  end_time = time.time()\n  epoch_duration = end_time - start_time\n\n  history_loss.append(sum(losses)/len(losses))\n\n  if (epoch+1) % 10 == 0:\n    tqdm.write(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds\")\n\n    print(f\"Epoch [{epoch+1}/{epochsno}], \"\n          f\"Loss: {sum(losses)/len(losses):.4f}\")\n\n    torch.save(model.state_dict(), f'/kaggle/working/Yolov3_epoch{epoch+1}.pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T11:58:42.131722Z","iopub.execute_input":"2024-08-07T11:58:42.132051Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Epochs:   0%|          | 0/150 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\nEpochs:   4%|â–         | 6/150 [02:39<1:02:02, 25.85s/it]","output_type":"stream"}]},{"cell_type":"code","source":"# Login with the API KEY\nwandb.login(key=\"ab35ea8191eba471c2b58a844910531625b00550\")\n\n# Initialize wandb run\nwandb.init(project='Untitled10', name='mblogge785-work')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\nfor epoch in range(epochsno):\n        train_loss = train(model, train_loader, optimizer, criterion, DEVICE)\n        val_loss, all_preds, all_labels = evaluate(model, test_loader, criterion, DEVICE)\n        \n        precision, recall, f1_score = calculate_metrics(all_preds, all_labels, ClassesNo)\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss}, Validation Loss: {val_loss}\")\n        print(f\"Precision: {precision}\")\n        print(f\"Recall: {recall}\")\n        print(f\"F1 Score: {f1_score}\")\n        \n        # Log metrics to wandb\n        wandb.log({\"epoch\": epoch+1, \"train_loss\": train_loss, \"val_loss\": val_loss,\n                   \"precision\": precision, \"recall\": recall, \"f1_score\": f1_score})","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}