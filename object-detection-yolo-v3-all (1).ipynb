{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["yjwnv6MYoEgc"],"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1556326,"sourceType":"datasetVersion","datasetId":918769},{"sourceId":7240078,"sourceType":"datasetVersion","datasetId":4193279}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":12211.677353,"end_time":"2023-12-19T20:51:56.359253","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-12-19T17:28:24.681900","version":"2.4.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# YOLO v3 model architecture","metadata":{"id":"yjwnv6MYoEgc","papermill":{"duration":0.011698,"end_time":"2023-12-19T17:28:39.297965","exception":false,"start_time":"2023-12-19T17:28:39.286267","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Imports\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nfrom collections import Counter\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nconfig = [\n    (32, 3, 1),\n    (64, 3, 2),\n    [\"List\", 1],\n    (128, 3, 2),\n    [\"List\", 2],\n    (256, 3, 2),\n    [\"List\", 8],\n    (512, 3, 2),\n    [\"List\", 8],\n    (1024, 3, 2),\n    [\"List\", 4],  # To this point is Darknet-53\n\n    (512, 1, 1),\n    (1024, 3, 1),\n    \"sp\",\n    (256, 1, 1),\n    \"um\",\n    (256, 1, 1),\n    (512, 3, 1),\n    \"sp\",\n    (128, 1, 1),\n    \"um\",\n    (128, 1, 1),\n    (256, 3, 1),\n    \"sp\",\n]\nclass CNNBlock(nn.Module):\n  def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n    super(CNNBlock, self).__init__()\n    self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs) # If batchnorm layer(bn_act) is true, then bias is False\n    self.bn = nn.BatchNorm2d(out_channels)\n    self.leaky = nn.LeakyReLU(0.1)\n    self.use_bn_act = bn_act\n\n  def forward(self, x):\n    if self.use_bn_act:\n      return self.leaky(self.bn(self.conv(x)))\n    else:\n      return self.conv(x)\n\n\nclass ResidualBlock(nn.Module):\n  def __init__(self, channels, use_residual=True, num_repeats=1):\n    super(ResidualBlock, self).__init__()\n    self.layers = nn.ModuleList() # Like regular python list, but is container for pytorch nn modules\n\n    for repeat in range(num_repeats):\n      self.layers += [\n          nn.Sequential(\n            CNNBlock(channels, channels//2, kernel_size=1),\n            CNNBlock(channels//2, channels, kernel_size=3, padding=1)\n          )\n      ]\n\n    self.use_residual = use_residual\n    self.num_repeats = num_repeats\n\n  def forward(self, x):\n    for layer in self.layers:\n      if self.use_residual:\n        x = x + layer(x)\n      else:\n        x = layer(x)\n\n    return x\n\nclass ScalePrediction(nn.Module):\n  def __init__(self, in_channels, num_classes):\n    super(ScalePrediction, self).__init__()\n    self.pred = nn.Sequential(\n        CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n        CNNBlock(2 * in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1), # (num_classes + 5) * 3 --> (20+5) for each anchor box which in total is 3\n    )\n    self.num_classes = num_classes\n\n  def forward(self, x):\n    return (\n        self.pred(x)\n        .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3]) # [batch_size, anchor_boxes, prediction(25), grid_h, grid_w]\n        .permute(0, 1, 3, 4, 2) # [batch_size, anchor_boxes, grid_h, grid_w, prediction(25)]\n      )\n\nclass YOLOv3(nn.Module):\n  def __init__(self, in_channels=3, num_classes=20):\n    super(YOLOv3, self).__init__()\n    self.num_classes = num_classes\n    self.in_channels = in_channels\n    self.layers = self._create_conv_layers()\n\n  def forward(self, x):\n    outputs = []\n    route_connections = []\n\n    for layer in self.layers:\n      if isinstance(layer, ScalePrediction):\n        outputs.append(layer(x))\n        continue\n\n      x = layer(x)\n\n      if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n        route_connections.append(x)\n\n      elif isinstance(layer, nn.Upsample):\n        x = torch.cat([x, route_connections[-1]], dim=1)\n        route_connections.pop()\n\n    return outputs\n\n\n  def _create_conv_layers(self):\n    layers = nn.ModuleList()\n    in_channels = self.in_channels\n\n    for module in config:\n      if isinstance(module, tuple):\n        out_channels, kernel_size, stride = module\n        layers.append(CNNBlock(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=1 if kernel_size == 3 else 0\n        ))\n        in_channels = out_channels\n\n      elif isinstance(module, list):\n        num_repeats = module[1]\n        layers.append(ResidualBlock(in_channels, num_repeats=num_repeats))\n\n      elif isinstance(module, str):\n        if module == \"S\":\n          layers += [\n              ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n              CNNBlock(in_channels, in_channels//2, kernel_size=1),\n              ScalePrediction(in_channels//2, num_classes = self.num_classes)\n          ]\n          in_channels = in_channels // 2\n\n        elif module == \"U\":\n          layers.append(nn.Upsample(scale_factor=2))\n          in_channels = in_channels * 3\n\n    return layers\n\nnum_classes = 20\nIMAGE_SIZE = 416\nmodel = YOLOv3(num_classes=num_classes)\nx = torch.randn((2, 3, IMAGE_SIZE, IMAGE_SIZE))\nout = model(x)\nassert model(x)[0].shape == (2, 3, IMAGE_SIZE//32, IMAGE_SIZE//32, num_classes + 5)\nassert model(x)[1].shape == (2, 3, IMAGE_SIZE//16, IMAGE_SIZE//16, num_classes + 5)\nassert model(x)[2].shape == (2, 3, IMAGE_SIZE//8, IMAGE_SIZE//8, num_classes + 5)","metadata":{"id":"Q_zaXn4YQCl7","papermill":{"duration":0.02402,"end_time":"2023-12-19T17:28:39.368315","exception":false,"start_time":"2023-12-19T17:28:39.344295","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config File","metadata":{"id":"dj8JoGrytb6y","papermill":{"duration":0.012705,"end_time":"2023-12-19T17:28:47.256538","exception":false,"start_time":"2023-12-19T17:28:47.243833","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import cv2\nimport torch\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nNUM_WORKERS = 4\nBATCH_SIZE = 32\nIMAGE_SIZE = 416\nNUM_CLASSES = 20\nLEARNING_RATE = 1e-5\nNUM_EPOCHS = 80\nCONF_THRESHOLD = 0.8\nMAP_IOU_THRESH = 0.5\nNMS_IOU_THRESH = 0.45\nS = [IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8]\n\nIMG_DIR = \"/kaggle/input/pascalvoc-yolo/images\"\nLABEL_DIR = \"/kaggle/input/pascalvoc-yolo/labels\"\n\nANCHORS = [\n    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n] \n\n\nPASCAL_CLASSES = [\n    \"aeroplane\",\n    \"bicycle\",\n    \"bird\",\n    \"boat\",\n    \"bottle\",\n    \"bus\",\n    \"car\",\n    \"cat\",\n    \"chair\",\n    \"cow\",\n    \"diningtable\",\n    \"dog\",\n    \"horse\",\n    \"motorbike\",\n    \"person\",\n    \"pottedplant\",\n    \"sheep\",\n    \"sofa\",\n    \"train\",\n    \"tvmonitor\"\n]\n\ndef iou_width_height(boxes1, boxes2):\n    \"\"\"\n    Parameters:\n        boxes1 (tensor): width and height of the first bounding boxes\n        boxes2 (tensor): width and height of the second bounding boxes\n    Returns:\n        tensor: Intersection over union of the corresponding boxes\n    \"\"\"\n    intersection = torch.min(boxes1[..., 0], boxes2[..., 0]) * torch.min(\n        boxes1[..., 1], boxes2[..., 1]\n    )\n    union = (\n        boxes1[..., 0] * boxes1[..., 1] + boxes2[..., 0] * boxes2[..., 1] - intersection\n    )\n    return intersection / union\n\ndef intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n\n    if box_format == \"midpoint\":\n        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n\n    if box_format == \"corners\":\n        box1_x1 = boxes_preds[..., 0:1]\n        box1_y1 = boxes_preds[..., 1:2]\n        box1_x2 = boxes_preds[..., 2:3]\n        box1_y2 = boxes_preds[..., 3:4]\n        box2_x1 = boxes_labels[..., 0:1]\n        box2_y1 = boxes_labels[..., 1:2]\n        box2_x2 = boxes_labels[..., 2:3]\n        box2_y2 = boxes_labels[..., 3:4]\n\n    x1 = torch.max(box1_x1, box2_x1)\n    y1 = torch.max(box1_y1, box2_y1)\n    x2 = torch.min(box1_x2, box2_x2)\n    y2 = torch.min(box1_y2, box2_y2)\n\n    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n\n    return intersection / (box1_area + box2_area - intersection + 1e-6)\n\ndef non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n\n    assert type(bboxes) == list\n\n    bboxes = [box for box in bboxes if box[1] > threshold]\n    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n    bboxes_after_nms = []\n\n    while bboxes:\n        chosen_box = bboxes.pop(0)\n\n        bboxes = [\n            box\n            for box in bboxes\n            if box[0] != chosen_box[0]\n            or intersection_over_union(\n                torch.tensor(chosen_box[2:]),\n                torch.tensor(box[2:]),\n                box_format=box_format,\n            )\n            < iou_threshold\n        ]\n\n        bboxes_after_nms.append(chosen_box)\n\n    return bboxes_after_nms","metadata":{"id":"9Ai7PMaUteOY","papermill":{"duration":0.371109,"end_time":"2023-12-19T17:28:47.640461","exception":false,"start_time":"2023-12-19T17:28:47.269352","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-01T14:55:22.363061Z","iopub.execute_input":"2024-08-01T14:55:22.363403Z","iopub.status.idle":"2024-08-01T14:55:22.373676Z","shell.execute_reply.started":"2024-08-01T14:55:22.363366Z","shell.execute_reply":"2024-08-01T14:55:22.372527Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"id":"6qWub9xCoJQP","papermill":{"duration":0.012761,"end_time":"2023-12-19T17:28:47.853131","exception":false,"start_time":"2023-12-19T17:28:47.840370","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport pandas as pd\nimport torch\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image, ImageFile\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nclass YOLODataset(Dataset):\n  def __init__(self, csv_file, img_dir, label_dir, anchors,\n               image_size=416, S=[13,26,52], C=20, transform=None):\n    self.annotations = pd.read_csv(csv_file)\n    self.img_dir = img_dir\n    self.label_dir = label_dir\n    self.transform = transform\n    self.S = S\n\n    self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2]) # For all 3 scales\n    self.num_anchors = self.anchors.shape[0]\n    self.num_anchors_per_scale = self.num_anchors // 3\n\n    self.C = C\n\n    self.ignore_iou_thresh = 0.5\n\n  def __len__(self):\n    return len(self.annotations)\n\n  def __getitem__(self, index):\n    label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n    bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist() # np.roll with shift 4 on axis 1: [class, x, y, w, h] --> [x, y, w, h, class]\n\n    img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n    image = Image.open(img_path)\n\n    if self.transform:\n      image = self.transform(image)\n\n    targets = [torch.zeros((self.num_anchors // 3, S, S, 6)) for S in self.S] # 6 because objectness score, bounding box coordinates (x, y, w, h), class label\n\n    for box in bboxes:\n    \n    iou_anchors = iou_width_height(torch.tensor(box[2:4]), self.anchors) # IOU from height and width\n      anchor_indices = iou_anchors.argsort(descending=True, dim=0) # Sorting sucht that the first is the best anchor\n\n      x, y, width, height, class_label = box\n      has_anchor = [False, False, False] # Make sure there is an anchor for each of three scales for each bounding box\n\n      for anchor_idx in anchor_indices:\n        scale_idx = anchor_idx // self.num_anchors_per_scale # scale_idx is either 0,1,2: 0-->13x13, 1:-->26x26, 2:-->52x52\n        anchor_on_scale = anchor_idx % self.num_anchors_per_scale # In each scale, choosing the anchor thats either 0,1,2\n\n        S = self.S[scale_idx]\n        i, j = int(S*y), int(S*x) # x=0.5, S=13 --> int(6.5) = 6 | i=y cell, j=x cell\n        anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n\n        if not anchor_taken and not has_anchor[scale_idx]:\n          targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n          x_cell, y_cell = S*x - j, S*y - i # 6.5 - 6 = 0.5 such that they are between [0,1]\n          width_cell, height_cell = (\n              width*S, # S=13, width=0.5, 6.5\n              height*S\n          )\n\n          box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n\n          targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n          targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n          has_anchor[scale_idx] = True\n\n        # Even if the same grid shares another anchor having iou>ignore_iou_thresh then,\n        elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n          targets[scale_idx][anchor_on_scale, i, j, 0] = -1 # ignore this prediction\n\n    return image, tuple(targets)\n\nimport torchvision.transforms as transforms\ntransform = transforms.Compose([transforms.Resize((416, 416)), transforms.ToTensor()])\n\ndef get_loaders(train_csv_path, test_csv_path):\n\n    train_dataset = YOLODataset(\n        train_csv_path,\n        transform=transform,\n        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n        img_dir=IMG_DIR,\n        label_dir=LABEL_DIR,\n        anchors=ANCHORS,\n    )\n    test_dataset = YOLODataset(\n        test_csv_path,\n        transform=transform,\n        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n        img_dir=IMG_DIR,\n        label_dir=LABEL_DIR,\n        anchors=ANCHORS,\n    )\n    train_loader = DataLoader(\n        dataset=train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=False,\n    )\n    test_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        drop_last=False,\n    )\n\n    return train_loader, test_loader\n\ndef mean_average_precision(\n    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=4\n):\n    \n    # list storing all AP for respective classes\n    average_precisions = []\n\n    # used for numerical stability later on\n    epsilon = 1e-6\n\n    for c in range(num_classes):\n        detections = []\n        ground_truths = []\n\n        # Go through all predictions and targets,\n        # and only add the ones that belong to the\n        # current class c\n        for detection in pred_boxes:\n            if detection[1] == c:\n                detections.append(detection)\n\n        for true_box in true_boxes:\n            if true_box[1] == c:\n                ground_truths.append(true_box)\n\n        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n\n        for key, val in amount_bboxes.items():\n            amount_bboxes[key] = torch.zeros(val)\n\n        # sort by box probabilities which is index 2\n        detections.sort(key=lambda x: x[2], reverse=True)\n        TP = torch.zeros((len(detections)))\n        FP = torch.zeros((len(detections)))\n        total_true_bboxes = len(ground_truths)\n\n        # If none exists for this class then we can safely skip\n        if total_true_bboxes == 0:\n            continue\n\n        for detection_idx, detection in enumerate(detections):\n            ground_truth_img = [\n                bbox for bbox in ground_truths if bbox[0] == detection[0]\n            ]\n\n            num_gts = len(ground_truth_img)\n            best_iou = 0\n\n            for idx, gt in enumerate(ground_truth_img):\n                iou = intersection_over_union(\n                    torch.tensor(detection[3:]),\n                    torch.tensor(gt[3:]),\n                    box_format=box_format,\n                )\n\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_idx = idx\n\n            if best_iou > iou_threshold:\n                # only detect ground truth detection once\n                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n                    # true positive and add this bounding box to seen\n                    TP[detection_idx] = 1\n                    amount_bboxes[detection[0]][best_gt_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n\n            # if IOU is lower then the detection is a false positive\n            else:\n                FP[detection_idx] = 1\n\n        TP_cumsum = torch.cumsum(TP, dim=0)\n        FP_cumsum = torch.cumsum(FP, dim=0)\n        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n        # torch.trapz for numerical integration\n        average_precisions.append(torch.trapz(precisions, recalls))\n\n    return sum(average_precisions) / len(average_precisions)\n\ndef get_evaluation_bboxes(\n    loader,\n    model,\n    iou_threshold,\n    anchors,\n    threshold,\n    box_format=\"midpoint\",\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n):\n    # make sure model is in eval before get bboxes\n    model.eval()\n    train_idx = 0\n    all_pred_boxes = []\n    all_true_boxes = []\n    for batch_idx, (x, labels) in enumerate(loader):\n        x = x.float().to(device)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size = x.shape[0]\n        bboxes = [[] for _ in range(batch_size)]\n        for i in range(3):\n            S = predictions[i].shape[2] # grid cell size for each predictions\n            anchor = torch.tensor([*anchors[i]]).to(device) * S # anchor for each grid, prediction type\n            boxes_scale_i = cells_to_bboxes( # get bboxes for each image in the batch\n                predictions[i], anchor, S=S, is_preds=True\n            )\n            for idx, (box) in enumerate(boxes_scale_i): # for each image, append the bbox to corr. bboxes[idx]\n                bboxes[idx] += box\n\n        # we just want one bbox for each label, not one for each scale\n        true_bboxes = cells_to_bboxes(\n            labels[2], anchor, S=S, is_preds=False\n        )\n\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                bboxes[idx],\n                iou_threshold=iou_threshold,\n                threshold=threshold,\n                box_format=box_format,\n            )\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_bboxes[idx]:\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            train_idx += 1\n\n    model.train()\n    return all_pred_boxes, all_true_boxes\n\ndef cells_to_bboxes(predictions, anchors, S, is_preds=True):\n    \n    BATCH_SIZE = predictions.shape[0]\n    num_anchors = len(anchors)\n    box_predictions = predictions[..., 1:5]\n    if is_preds:\n        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n        scores = torch.sigmoid(predictions[..., 0:1])\n        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n    else:\n        scores = predictions[..., 0:1]\n        best_class = predictions[..., 5:6]\n\n    cell_indices = (\n        torch.arange(S)\n        .repeat(predictions.shape[0], 3, S, 1)\n        .unsqueeze(-1)\n        .to(predictions.device)\n    )\n    x = 1 / S * (box_predictions[..., 0:1] + cell_indices)\n    y = 1 / S * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n    w_h = 1 / S * box_predictions[..., 2:4]\n    converted_bboxes = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(BATCH_SIZE, num_anchors * S * S, 6)\n    return converted_bboxes.tolist()\n\ndef get_evaluation_bboxes(\n    loader,\n    model,\n    iou_threshold,\n    anchors,\n    threshold,\n    box_format=\"midpoint\",\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n):\n    # make sure model is in eval before get bboxes\n    model.eval()\n    train_idx = 0\n    all_pred_boxes = []\n    all_true_boxes = []\n    for batch_idx, (x, labels) in enumerate(loader):\n        x = x.float().to(device)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size = x.shape[0]\n        bboxes = [[] for _ in range(batch_size)]\n        for i in range(3):\n            S = predictions[i].shape[2] # grid cell size for each predictions\n            anchor = torch.tensor([*anchors[i]]).to(device) * S # anchor for each grid, prediction type\n            boxes_scale_i = cells_to_bboxes( # get bboxes for each image in the batch\n                predictions[i], anchor, S=S, is_preds=True\n            )\n            for idx, (box) in enumerate(boxes_scale_i): # for each image, append the bbox to corr. bboxes[idx]\n                bboxes[idx] += box\n\n        # we just want one bbox for each label, not one for each scale\n        true_bboxes = cells_to_bboxes(\n            labels[2], anchor, S=S, is_preds=False\n        )\n\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                bboxes[idx],\n                iou_threshold=iou_threshold,\n                threshold=threshold,\n                box_format=box_format,\n            )\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_bboxes[idx]:\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            train_idx += 1\n\n    model.train()\n    return all_pred_boxes, all_true_boxes\n\ndef cells_to_bboxes(predictions, anchors, S, is_preds=True):\n    \n    BATCH_SIZE = predictions.shape[0]\n    num_anchors = len(anchors)\n    box_predictions = predictions[..., 1:5]\n    if is_preds:\n        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n        scores = torch.sigmoid(predictions[..., 0:1])\n        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n    else:\n        scores = predictions[..., 0:1]\n        best_class = predictions[..., 5:6]\n\n    cell_indices = (\n        torch.arange(S)\n        .repeat(predictions.shape[0], 3, S, 1)\n        .unsqueeze(-1)\n        .to(predictions.device)\n    )\n    x = 1 / S * (box_predictions[..., 0:1] + cell_indices)\n    y = 1 / S * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n    w_h = 1 / S * box_predictions[..., 2:4]\n    converted_bboxes = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(BATCH_SIZE, num_anchors * S * S, 6)\n    return converted_bboxes.tolist()class YoloLoss(nn.Module):\n  def __init__(self):\n    super(YoloLoss, self).__init__()\n    self.mse = nn.MSELoss() # For bounding box loss\n    self.bce = nn.BCEWithLogitsLoss() # For multi-label prediction: Binary cross entropy\n    self.entropy = nn.CrossEntropyLoss() # For classification\n    self.sigmoid = nn.Sigmoid()\n\n    # Constants for significance of obj, or no obj.\n    self.lambda_class = 1\n    self.lambda_noobj = 10\n    self.lambda_obj = 1\n    self.lambda_box = 10\n\n  def forward(self, predictions, target, anchors):\n    obj = target[..., 0] == 1\n    noobj = target[..., 0] == 0\n\n    no_object_loss = self.bce(\n        (predictions[..., 0:1][noobj]), (target[..., 0:1][noobj])\n    )\n\n    anchors = anchors.reshape(1,3,1,1,2) # Anchors initial shape 3x2 --> 3 anchor boxes each of certain hxw (2)\n\n    # box_preds = [..., sigmoid(x), sigmoid(y), [p_w * exp(t_w)], [p_h * exp(t_h)], ...]\n    box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n\n    # iou between predicted box and target box\n    ious = intersection_over_union(box_preds[obj], target[..., 1:5][obj]).detach()\n\n    object_loss = self.bce(\n        (predictions[..., 0:1][obj]), (ious * target[..., 0:1][obj]) # target * iou because only intersected part object loss calc\n    )\n\n    predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3]) # x, y to be between [0,1]\n    target[..., 3:5] = torch.log(\n        (1e-6 + target[..., 3:5] / anchors)\n    ) # Exponential of hxw (taking log because opp. of exp)\n\n    box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n\n    class_loss = self.entropy(\n        (predictions[..., 5:][obj]), (target[..., 5][obj].long())\n    )\n\n    return(\n        self.lambda_box * box_loss\n        + self.lambda_obj * object_loss\n        + self.lambda_noobj * no_object_loss\n        + self.lambda_class * class_loss\n    )\n\ndef plot_image(image, boxes):\n    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n    cmap = plt.get_cmap(\"tab20b\")\n    class_labels = PASCAL_CLASSES\n    colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n    im = np.array(image)\n    height, width, _ = im.shape\n\n    # Create figure and axes\n    fig, ax = plt.subplots(1)\n    # Display the image\n    ax.imshow(im)\n\n    # Create a Rectangle patch\n    for box in boxes:\n        assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n        class_pred = box[0]\n        box = box[2:]\n        upper_left_x = box[0] - box[2] / 2\n        upper_left_y = box[1] - box[3] / 2\n        rect = patches.Rectangle(\n            (upper_left_x * width, upper_left_y * height),\n            box[2] * width,\n            box[3] * height,\n            linewidth=2,\n            edgecolor=colors[int(class_pred)],\n            facecolor=\"none\",\n        )\n        # Add the patch to the Axes\n        ax.add_patch(rect)\n        plt.text(\n            upper_left_x * width,\n            upper_left_y * height,\n            s=class_labels[int(class_pred)],\n            color=\"white\",\n            verticalalignment=\"top\",\n            bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n        )\n\n    plt.show()\n    \n    # Instantiate the model\nmodel = YOLOv3(num_classes=NUM_CLASSES).to(DEVICE)\n\n# Compile the model\noptimizer = torch.optim.Adam(\n    model.parameters(), lr=LEARNING_RATE\n)\nloss_fn = YoloLoss()\n\n# Scaler\nscaler = torch.cuda.amp.GradScaler()\n\n# Train-Test Loader\ntrain_loader, test_loader = get_loaders(\n    train_csv_path='/kaggle/input/pascalvoc-yolo/test.csv', test_csv_path='/kaggle/input/pascalvoc-yolo/test.csv'\n)\n\n# Anchors\nscaled_anchors = (\n    torch.tensor(ANCHORS) * torch.tensor([13,26,52]).unsqueeze(1).unsqueeze(1).repeat(1,3,2)\n).to(DEVICE)\n\n# Save test loader to a file\ntorch.save(test_loader, '/kaggle/working/test_loader.pth')","metadata":{"id":"17VUsG5An6ED","papermill":{"duration":0.019761,"end_time":"2023-12-19T17:28:47.885721","exception":false,"start_time":"2023-12-19T17:28:47.865960","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-01T14:55:22.416252Z","iopub.execute_input":"2024-08-01T14:55:22.416628Z","iopub.status.idle":"2024-08-01T14:55:22.428646Z","shell.execute_reply.started":"2024-08-01T14:55:22.416593Z","shell.execute_reply":"2024-08-01T14:55:22.427575Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"id":"--AS2dslvM-D","papermill":{"duration":0.012926,"end_time":"2023-12-19T17:28:48.743475","exception":false,"start_time":"2023-12-19T17:28:48.730549","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch.optim as optim\n\nfrom tqdm import tqdm\nimport time\n\nhistory_loss = [] # To plot the epoch vs. loss\n\nfor epoch in tqdm(range(NUM_EPOCHS), desc=\"Epochs\"):\n  model.train()\n\n  losses = []\n\n  start_time = time.time() # Start time of the epoch\n\n  for batch_idx, (x,y) in enumerate(train_loader):\n    x = x.to(DEVICE)\n    y0, y1, y2 = (y[0].to(DEVICE),\n                  y[1].to(DEVICE),\n                  y[2].to(DEVICE))\n\n    # context manager is used in PyTorch to automatically handle mixed-precision computations on CUDA-enabled GPUs\n    with torch.cuda.amp.autocast():\n      out = model(x)\n      loss = (\n          loss_fn(out[0], y0, scaled_anchors[0])\n          + loss_fn(out[1], y1, scaled_anchors[1])\n          + loss_fn(out[2], y2, scaled_anchors[2])\n      )\n\n    losses.append(loss.item())\n    \n    optimizer.zero_grad()\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\n  end_time = time.time()  # End time of the epoch\n  epoch_duration = end_time - start_time  # Duration of the epoch\n    \n  history_loss.append(sum(losses)/len(losses))\n\n  if (epoch+1) % 10 == 0:\n    # Print the epoch duration\n    tqdm.write(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds\")\n\n    # Print the loss and accuracy for training and validation data\n    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], \"\n          f\"Loss: {sum(losses)/len(losses):.4f}\")\n\n    # save the model after every 10 epoch\n    torch.save(model.state_dict(), f'/kaggle/working/Yolov3_epoch{epoch+1}.pth')\n\nmodel.eval()\nx, y = next(iter(test_loader))\nx = x.float().to(DEVICE)\n\nwith torch.no_grad():\n    out = model(x)\n    bboxes = [[] for _ in range(x.shape[0])]\n    batch_size, A, S, _, _ = out[0].shape\n    anchor = torch.tensor([*ANCHORS[0]]).to(DEVICE) * S\n    boxes_scale_i = cells_to_bboxes(\n        out[0], anchor, S=S, is_preds=True\n    )\n    for idx, (box) in enumerate(boxes_scale_i):\n        bboxes[idx] += box\n\n    for i in range(batch_size):\n        nms_boxes = non_max_suppression(\n            bboxes[i], iou_threshold=0.5, threshold=0.6, box_format=\"midpoint\",\n        )\n        plot_image(x[i].permute(1,2,0).detach().cpu(), nms_boxes)","metadata":{"id":"QxkC5a_lFOvA","outputId":"cb8c11e8-e6ba-42b5-ff27-d580e23ab481","papermill":{"duration":11747.786746,"end_time":"2023-12-19T20:44:40.200757","exception":false,"start_time":"2023-12-19T17:28:52.414011","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-01T14:55:23.392470Z","iopub.execute_input":"2024-08-01T14:55:23.392915Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Epochs:  11%|█▏        | 9/80 [24:36<2:54:05, 147.12s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 10 completed in 146.64 seconds\nEpoch [10/80], Loss: 18.0049\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  24%|██▍       | 19/80 [49:00<2:28:42, 146.27s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 20 completed in 147.15 seconds\nEpoch [20/80], Loss: 12.8638\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  31%|███▏      | 25/80 [1:01:14<2:14:12, 146.41s/it]","output_type":"stream"}]},{"cell_type":"markdown","source":"# Testing","metadata":{"papermill":{"duration":0.15928,"end_time":"2023-12-19T20:44:51.430295","exception":false,"start_time":"2023-12-19T20:44:51.271015","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load the model\nmodel = YOLOv3(num_classes=NUM_CLASSES)\nmodel_path = \"/kaggle/input/80-epoch-yolov3-model/Yolov3_epoch80.pth\"\nstate_dict = torch.load(model_path)\nmodel.load_state_dict(state_dict)\nmodel = model.to(DEVICE)\n\n# Testing\nlosses = []\n\nwith torch.no_grad():\n    model.eval()\n\n    for batch_idx, (x,y) in enumerate(test_loader):\n        x = x.to(DEVICE)\n        y0, y1, y2 = (y[0].to(DEVICE),\n                    y[1].to(DEVICE),\n                    y[2].to(DEVICE))\n\n        out = model(x)\n        loss = (\n            loss_fn(out[0], y0, scaled_anchors[0])\n            + loss_fn(out[1], y1, scaled_anchors[1])\n            + loss_fn(out[2], y2, scaled_anchors[2])\n        )\n\n        losses.append(loss.item())\n\nprint(f\"Loss: {sum(losses)/len(losses):.4f}\")\n\npred_boxes, true_boxes = get_evaluation_bboxes(\n                test_loader,\n                model,\n                iou_threshold=NMS_IOU_THRESH,\n                anchors=ANCHORS,\n                threshold=CONF_THRESHOLD,\n            )\n\nmapval = mean_average_precision(\n    pred_boxes,\n    true_boxes,\n    iou_threshold=MAP_IOU_THRESH,\n    box_format=\"midpoint\",\n    num_classes=NUM_CLASSES,\n)\nprint(f\"MAP: {mapval.item()}\")","metadata":{"papermill":{"duration":119.671402,"end_time":"2023-12-19T20:46:51.245946","exception":false,"start_time":"2023-12-19T20:44:51.574544","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}