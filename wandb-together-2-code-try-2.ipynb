{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1556326,"sourceType":"datasetVersion","datasetId":918769}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\r\nimport time\r\nimport numpy as np\r\nimport pandas as pd\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom torch.utils.data import Dataset, DataLoader\r\nfrom torchvision import transforms\r\nfrom PIL import Image, ImageFile\r\nfrom collections import Counter\r\nfrom tqdm import tqdm\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib.patches as patches\r\nimport wandb  # Import wandb\r\nfrom transformers import BertTokenizer\r\n\r\n# Login with the API KEY\r\nwandb.login(key=\"ab35ea8191eba471c2b58a844910531625b00550\")\r\n\r\n# Initialize wandb\r\nwandb.init(project=\"Untitled10\", entity=\"mblogge785-work\")\r\n\r\n# Initialize the tokenizer\r\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n\r\n# Define configuration for YOLOv3\r\nconfig = [\r\n    (32, 3, 1),\r\n    (128, 3, 1),\r\n    (64, 3, 2),\r\n    [\"list\", 1],\r\n    (128, 3, 2),\r\n    [\"list\", 2],\r\n    (256, 3, 2),\r\n    [\"list\", 8],\r\n    (512, 3, 2),\r\n    [\"list\", 8],\r\n    (1024, 3, 2),\r\n    [\"list\", 4],\r\n    (512, 1, 1),\r\n    (1024, 3, 1),\r\n    \"sp\",\r\n    (256, 1, 1),\r\n    \"up\",\r\n    (256, 1, 1),\r\n    (512, 3, 1),\r\n    \"sp\",\r\n    (128, 1, 1),\r\n    \"up\",\r\n    (128, 1, 1),\r\n    (256, 3, 1),\r\n    \"sp\",\r\n]\r\n\r\nclass CNN_Block(nn.Module):\r\n    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\r\n        super(CNN_Block, self).__init__()\r\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\r\n        self.bn = nn.BatchNorm2d(out_channels)\r\n        self.leaky = nn.LeakyReLU(0.1)\r\n        self.use_bn_act = bn_act\r\n\r\n    def forward(self, x):\r\n        if self.use_bn_act:\r\n            return self.leaky(self.bn(self.conv(x)))\r\n        else:\r\n            return self.conv(x)\r\n\r\nclass Residual_Block(nn.Module):\r\n    def __init__(self, channels, use_residual=True, num_repeats=1):\r\n        super(Residual_Block, self).__init__()\r\n        self.layers = nn.ModuleList()\r\n        for repeat in range(num_repeats):\r\n            self.layers += [\r\n                nn.Sequential(\r\n                    CNN_Block(channels, channels//2, kernel_size=1),\r\n                    CNN_Block(channels//2, channels, kernel_size=3, padding=1)\r\n                )\r\n            ]\r\n        self.use_residual = use_residual\r\n        self.num_repeats = num_repeats\r\n\r\n    def forward(self, x):\r\n        for layer in self.layers:\r\n            if self.use_residual:\r\n                x = x + layer(x)\r\n            else:\r\n                x = layer(x)\r\n        return x\r\n\r\nclass Prediction_Scale(nn.Module):\r\n    def __init__(self, in_channels, NumClasses):\r\n        super(Prediction_Scale, self).__init__()\r\n        self.pred = nn.Sequential(\r\n            CNN_Block(in_channels, 2 * in_channels, kernel_size=3, padding=1),\r\n            CNN_Block(2 * in_channels, (NumClasses + 5) * 3, bn_act=False, kernel_size=1),\r\n        )\r\n        self.NumClasses = NumClasses\r\n\r\n    def forward(self, x):\r\n        return (\r\n            self.pred(x)\r\n            .reshape(x.shape[0], 3, self.NumClasses + 5, x.shape[2], x.shape[3])\r\n            .permute(0, 1, 3, 4, 2)\r\n        )\r\n\r\nclass YOLOv3(nn.Module):\r\n    def __init__(self, in_channels=3, NumClasses=20):\r\n        super(YOLOv3, self).__init__()\r\n        self.NumClasses = NumClasses\r\n        self.in_channels = in_channels\r\n        self.layers = self._create_conv_layers()\r\n\r\n    def forward(self, x):\r\n        outputs = []\r\n        route_connections = []\r\n        for layer in self.layers:\r\n            if isinstance(layer, Prediction_Scale):\r\n                outputs.append(layer(x))\r\n                continue\r\n            x = layer(x)\r\n            if isinstance(layer, Residual_Block) and layer.num_repeats == 8:\r\n                route_connections.append(x)\r\n            elif isinstance(layer, nn.Upsample):\r\n                x = torch.cat([x, route_connections.pop()], dim=1)\r\n        return outputs\r\n\r\n    def _create_conv_layers(self):\r\n        layers = nn.ModuleList()\r\n        in_channels = self.in_channels\r\n        for module in config:\r\n            if isinstance(module, tuple):\r\n                out_channels, kernel_size, stride = module\r\n                layers.append(CNN_Block(\r\n                    in_channels, out_channels,\r\n                    kernel_size=kernel_size, stride=stride,\r\n                    padding=1 if kernel_size == 3 else 0\r\n                ))\r\n                in_channels = out_channels\r\n            elif isinstance(module, list):\r\n                layers.append(Residual_Block(in_channels, num_repeats=module[1]))\r\n            elif isinstance(module, str):\r\n                if module == \"sp\":\r\n                    layers += [\r\n                        Residual_Block(in_channels, use_residual=False, num_repeats=1),\r\n                        CNN_Block(in_channels, in_channels//2, kernel_size=1),\r\n                        Prediction_Scale(in_channels//2, NumClasses=self.NumClasses)\r\n                    ]\r\n                    in_channels = in_channels // 2\r\n                elif module == \"up\":\r\n                    layers.append(nn.Upsample(scale_factor=2))\r\n                    in_channels = in_channels * 3\r\n        return layers\r\n\r\nclass YOLODataset(Dataset):\r\n    def __init__(self, csv_file, ImgDir, LableDir, anchors,\r\n                 ImageSize=416, sp=[13,26,52], cp=20, transform=None):\r\n        self.annotations = pd.read_csv(csv_file)\r\n        self.ImgDir = ImgDir\r\n        self.LableDir = LableDir\r\n        self.transform = transform\r\n        self.sp = sp\r\n        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])\r\n        self.num_anchors = self.anchors.shape[0]\r\n        self.num_anchors_per_scale = self.num_anchors // 3\r\n        self.cp = cp\r\n        self.ignore_iou_thresh = 0.5\r\n\r\n    def __len__(self):\r\n        return len(self.annotations)\r\n\r\n    def __getitem__(self, index):\r\n        label_path = os.path.join(self.LableDir, self.annotations.iloc[index, 1])\r\n        boxx = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\r\n        img_path = os.path.join(self.ImgDir, self.annotations.iloc[index, 0])\r\n        image = Image.open(img_path)\r\n        if self.transform:\r\n            image = self.transform(image)\r\n        targets = [torch.zeros((self.num_anchors // 3, sp, sp, 6)) for sp in self.sp]\r\n        for box in boxx:\r\n            iou_anchors = WeidthHeight(torch.tensor(box[2:4]), self.anchors)\r\n            anchor_indices = iou_anchors.argsort(descending=True, dim=0)\r\n            x, y, width, height, class_label = box\r\n            has_anchor = [False, False, False]\r\n            for anchor_idx in anchor_indices:\r\n                scale_idx = anchor_idx // self.num_anchors_per_scale\r\n                anchor_on_scale = anchor_idx % self.num_anchors_per_scale\r\n                sp = self.sp[scale_idx]\r\n                i, j = int(sp*y), int(sp*x)\r\n                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\r\n                if not anchor_taken and not has_anchor[scale_idx]:\r\n                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\r\n                    x_cell, y_cell = sp*x - j, sp*y - i\r\n                    width_cell, height_cell = width*sp, height*sp\r\n                    box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\r\n                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\r\n                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\r\n                    has_anchor[scale_idx] = True\r\n                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\r\n                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1\r\n        return image, tuple(targets)\r\n\r\nclass YoloLoss(nn.Module):\r\n    def __init__(self):\r\n        super(YoloLoss, self).__init__()\r\n        self.mse = nn.MSELoss()\r\n        self.bce = nn.BCEWithLogitsLoss()\r\n        self.entropy = nn.CrossEntropyLoss()\r\n        self.sigmoid = nn.Sigmoid()\r\n        self.lambda_class = 1\r\n        self.lambda_noobj = 10\r\n        self.lambda_obj = 1\r\n        self.lambda_box = 10\r\n\r\n    def forward(self, predictions, target, anchors):\r\n        obj = target[..., 0] == 1\r\n        noobj = target[..., 0] == 0\r\n        no_object_loss = self.bce((predictions[..., 0:1][noobj]), (target[..., 0:1][noobj]))\r\n        anchors = anchors.reshape(1,3,1,1,2)\r\n        box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\r\n        ious = InterctionOverUnion(box_preds[obj], target[..., 1:5][obj]).detach()\r\n        object_loss = self.bce((predictions[..., 0:1][obj]), (ious * target[..., 0:1][obj]))\r\n        predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3])\r\n        target[..., 3:5] = torch.log((1e-6 + target[..., 3:5] / anchors))\r\n        box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\r\n        class_loss = self.entropy((predictions[..., 5:][obj]), (target[..., 5][obj].long()))\r\n        return (self.lambda_box * box_loss + self.lambda_obj * object_loss + self.lambda_noobj * no_object_loss + self.lambda_class * class_loss)\r\n\r\n# Utility functions\r\ndef WeidthHeight(boxa, boxb):\r\n    intersection = torch.min(boxa[..., 0], boxb[..., 0]) * torch.min(boxa[..., 1], boxb[..., 1])\r\n    union = (boxa[..., 0] * boxa[..., 1] + boxb[..., 0] * boxb[..., 1] - intersection)\r\n    return intersection / union\r\n\r\ndef non_max_suppression(boxx, iou_threshold, threshold, box_format=\"corners\"):\r\n    assert type(boxx) == list\r\n    boxx = [box for box in boxx if box[1] > threshold]\r\n    boxx = sorted(boxx, key=lambda x: x[1], reverse=True)\r\n    boxx_after_nms = []\r\n    while boxx:\r\n        chosen_box = boxx.pop(0)\r\n        boxx = [box for box in boxx if box[0] != chosen_box[0] or InterctionOverUnion(\r\n            torch.tensor(chosen_box[2:]), torch.tensor(box[2:]), box_format=box_format) < iou_threshold]\r\n        boxx_after_nms.append(chosen_box)\r\n    return boxx_after_nms\r\n\r\ndef InterctionOverUnion(PredsBox, lableBox, box_format=\"midpoint\"):\r\n    if box_format == \"midpoint\":\r\n        box1_a1 = PredsBox[..., 0:1] - PredsBox[..., 2:3] / 2\r\n        box1_b1 = PredsBox[..., 1:2] - PredsBox[..., 3:4] / 2\r\n        box1_a2 = PredsBox[..., 0:1] + PredsBox[..., 2:3] / 2\r\n        box1_b2 = PredsBox[..., 1:2] + PredsBox[..., 3:4] / 2\r\n        box2_a1 = lableBox[..., 0:1] - lableBox[..., 2:3] / 2\r\n        box2_y1 = lableBox[..., 1:2] - lableBox[..., 3:4] / 2\r\n        box2_a2 = lableBox[..., 0:1] + lableBox[..., 2:3] / 2\r\n        box2_y2 = lableBox[..., 1:2] + lableBox[..., 3:4] / 2\r\n    if box_format == \"corners\":\r\n        box1_a1 = PredsBox[..., 0:1]\r\n        box1_b1 = PredsBox[..., 1:2]\r\n        box1_a2 = PredsBox[..., 2:3]\r\n        box1_b2 = PredsBox[..., 3:4]\r\n        box2_a1 = lableBox[..., 0:1]\r\n        box2_y1 = lableBox[..., 1:2]\r\n        box2_a2 = lableBox[..., 2:3]\r\n        box2_y2 = lableBox[..., 3:4]\r\n    x1 = torch.max(box1_a1, box2_a1)\r\n    y1 = torch.max(box1_b1, box2_y1)\r\n    x2 = torch.min(box1_a2, box2_a2)\r\n    y2 = torch.min(box1_b2, box2_y2)\r\n    intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\r\n    box1_area = (box1_a2 - box1_a1) * (box1_b2 - box1_b1)\r\n    box2_area = (box2_a2 - box2_a1) * (box2_y2 - box2_y1)\r\n    return intersection / (box1_area + box2_area - intersection)\r\n\r\ndef mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", NumClasses=4):\r\n    average_precisions = []\r\n    epsilon = 1e-6\r\n    for c in range(NumClasses):\r\n        detections = []\r\n        ground_truths = []\r\n        for detection in pred_boxes:\r\n            if detection[1] == c:\r\n                detections.append(detection)\r\n        for true_box in true_boxes:\r\n            if true_box[1] == c:\r\n                ground_truths.append(true_box)\r\n        amount_boxx = Counter([gt[0] for gt in ground_truths])\r\n        for key, val in amount_boxx.items():\r\n            amount_boxx[key] = torch.zeros(val)\r\n        detections.sort(key=lambda x: x[2], reverse=True)\r\n        TP = torch.zeros((len(detections)))\r\n        FP = torch.zeros((len(detections)))\r\n        total_true_boxx = len(ground_truths)\r\n        if total_true_boxx == 0:\r\n            continue\r\n        for detection_idx, detection in enumerate(detections):\r\n            ground_truth_img = [bbox for bbox in ground_truths if bbox[0] == detection[0]]\r\n            num_gts = len(ground_truth_img)\r\n            best_iou = 0\r\n            for idx, gt in enumerate(ground_truth_img):\r\n                iou = InterctionOverUnion(torch.tensor(detection[3:]), torch.tensor(gt[3:]), box_format=box_format)\r\n                if iou > best_iou:\r\n                    best_iou = iou\r\n                    best_gt_idx = idx\r\n            if best_iou > iou_threshold:\r\n                if amount_boxx[detection[0]][best_gt_idx] == 0:\r\n                    TP[detection_idx] = 1\r\n                    amount_boxx[detection[0]][best_gt_idx] = 1\r\n                else:\r\n                    FP[detection_idx] = 1\r\n            else:\r\n                FP[detection_idx] = 1\r\n        TP_cumsum = torch.cumsum(TP, dim=0)\r\n        FP_cumsum = torch.cumsum(FP, dim=0)\r\n        recalls = TP_cumsum / (total_true_boxx + epsilon)\r\n        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\r\n        precisions = torch.cat((torch.tensor([1]), precisions))\r\n        recalls = torch.cat((torch.tensor([0]), recalls))\r\n        average_precisions.append(torch.trapz(precisions, recalls))\r\n    return sum(average_precisions) / len(average_precisions)\r\n\r\ndef get_evaluation_boxx(loader, model, iou_threshold, anchors, threshold, box_format=\"midpoint\"):\r\n    model.eval()\r\n    train_idx = 0\r\n    all_pred_boxes = []\r\n    all_true_boxes = []\r\n    for batch_idx, (x, labels) in enumerate(loader):\r\n        x = x.float().to(DEVICE)\r\n        with torch.no_grad():\r\n            predictions = model(x)\r\n        batch_size = x.shape[0]\r\n        boxx = [[] for _ in range(batch_size)]\r\n        for i in range(3):\r\n            sp = predictions[i].shape[2]\r\n            anchor = torch.tensor([*anchors[i]]).to(DEVICE) * sp\r\n            boxes_scale_i = cells_to_boxx(predictions[i], anchor, sp=sp, is_preds=True)\r\n            for idx, box in enumerate(boxes_scale_i):\r\n                boxx[idx] += box\r\n        true_boxx = cells_to_boxx(labels[2], anchor, sp=sp, is_preds=False)\r\n        for idx in range(batch_size):\r\n            nms_boxes = non_max_suppression(boxx[idx], iou_threshold=iou_threshold, threshold=threshold, box_format=box_format)\r\n            for nms_box in nms_boxes:\r\n                all_pred_boxes.append([train_idx] + nms_box)\r\n            for box in true_boxx[idx]:\r\n                if box[1] > threshold:\r\n                    all_true_boxes.append([train_idx] + box)\r\n            train_idx += 1\r\n    model.train()\r\n    return all_pred_boxes, all_true_boxes\r\n\r\ndef cells_to_boxx(predictions, anchors, sp, is_preds=True):\r\n    SizeOfBatch = predictions.shape[0]\r\n    num_anchors = len(anchors)\r\n    box_predictions = predictions[..., 1:5]\r\n    if is_preds:\r\n        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\r\n        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\r\n        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\r\n        scores = torch.sigmoid(predictions[..., 0:1])\r\n        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\r\n    else:\r\n        scores = predictions[..., 0:1]\r\n        best_class = predictions[..., 5:6]\r\n    cell_indices = (\r\n        torch.arange(sp)\r\n        .repeat(predictions.shape[0], 3, sp, 1)\r\n        .unsqueeze(-1)\r\n        .to(predictions.device)\r\n    )\r\n    x = 1 / sp * (box_predictions[..., 0:1] + cell_indices)\r\n    y = 1 / sp * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\r\n    w_h = 1 / sp * box_predictions[..., 2:4]\r\n    converted_boxx = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(SizeOfBatch, num_anchors * sp * sp, 6)\r\n    return converted_boxx.tolist()\r\n\r\ndef plot_image(image, boxes):\r\n    cmap = plt.get_cmap(\"tab20b\")\r\n    class_labels = AllClacess\r\n    colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\r\n    im = np.array(image)\r\n    height, width, _ = im.shape\r\n    fig, ax = plt.subplots(1)\r\n    ax.imshow(im)\r\n    for box in boxes:\r\n        assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\r\n        class_pred = box[0]\r\n        box = box[2:]\r\n        UpperLeft_x = box[0] - box[2] / 2\r\n        UpperLeft_y = box[1] - box[3] / 2\r\n        rect = patches.Rectangle(\r\n            (UpperLeft_x * width, UpperLeft_y * height),\r\n            box[2] * width,\r\n            box[3] * height,\r\n            linewidth=2,\r\n            edgecolor=colors[int(class_pred)],\r\n            facecolor=\"none\",\r\n        )\r\n        ax.add_patch(rect)\r\n        plt.text(\r\n            UpperLeft_x * width,\r\n            UpperLeft_y * height,\r\n            s=class_labels[int(class_pred)],\r\n            color=\"white\",\r\n            verticalalignment=\"top\",\r\n            bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\r\n        )\r\n    plt.show()\r\n\r\n# Instantiate the model\r\nmodel = YOLOv3(NumClasses=NumClasses).to(DEVICE)\r\n\r\n# Compile the model\r\noptimizer = torch.optim.Adam(model.parameters(), lr=RateOfLearning)\r\nloss_fn = YoloLoss()\r\n\r\n# Scaler\r\nscaler = torch.cuda.amp.GradScaler()\r\n\r\n# Transform\r\ntransform = transforms.Compose([transforms.Resize((416, 416)), transforms.ToTensor()])\r\n\r\n# Train-Test Loader\r\ndef get_loaders(train_csv_path, test_csv_path):\r\n    train_dataset = YOLODataset(train_csv_path, transform=transform, sp=[ImageSize // 32, ImageSize // 16, ImageSize // 8],\r\n                                ImgDir=DirImage, LableDir=DirLable, anchors=ANCHORS)\r\n    test_dataset = YOLODataset(test_csv_path, transform=transform, sp=[ImageSize // 32, ImageSize // 16, ImageSize // 8],\r\n                               ImgDir=DirImage, LableDir=DirLable, anchors=ANCHORS)\r\n    train_loader = DataLoader(dataset=train_dataset, batch_size=SizeOfBatch, shuffle=True, drop_last=False)\r\n    test_loader = DataLoader(dataset=test_dataset, batch_size=SizeOfBatch, shuffle=False, drop_last=False)\r\n    return train_loader, test_loader\r\n\r\ntrain_loader, test_loader = get_loaders(\r\n    train_csv_path='/kaggle/input/pascalvoc-yolo/8examples.csv', test_csv_path='/kaggle/input/pascalvoc-yolo/8examples.csv'\r\n)\r\n\r\n# Anchors\r\nscaled_anchors = (\r\n    torch.tensor(ANCHORS) * torch.tensor([13,26,52]).unsqueeze(1).unsqueeze(1).repeat(1,3,2)\r\n).to(DEVICE)\r\n\r\n# Training loop\r\nhistory_loss = []\r\n\r\nfor epoch in tqdm(range(epochsno), desc=\"Epochs\"):\r\n    model.train()\r\n    losses = []\r\n    start_time = time.time()\r\n    for batch_idx, (x,y) in enumerate(train_loader):\r\n        x = x.to(DEVICE)\r\n        y0, y1, y2 = (y[0].to(DEVICE), y[1].to(DEVICE), y[2].to(DEVICE))\r\n        with torch.cuda.amp.autocast():\r\n            out = model(x)\r\n            loss = (loss_fn(out[0], y0, scaled_anchors[0]) +\r\n                    loss_fn(out[1], y1, scaled_anchors[1]) +\r\n                    loss_fn(out[2], y2, scaled_anchors[2]))\r\n        losses.append(loss.item())\r\n        optimizer.zero_grad()\r\n        scaler.scale(loss).backward()\r\n        scaler.step(optimizer)\r\n        scaler.update()\r\n    end_time = time.time()\r\n    epoch_duration = end_time - start_time\r\n    history_loss.append(sum(losses)/len(losses))\r\n    precision, recall, f1 = compute_metrics(pred_boxes, true_boxes)\r\n    if (epoch+1) % 10 == 0:\r\n        tqdm.write(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds\")\r\n        print(f\"Epoch [{epoch+1}/{epochsno}], Loss: {sum(losses)/len(losses):.4f}\")\r\n        wandb.log({\r\n            \"epoch\": epoch + 1,\r\n            \"loss\": sum(losses)/len(losses),\r\n            \"epoch_duration\": epoch_duration,\r\n            \"precision\": precision,\r\n            \"recall\": recall,\r\n            \"f1_score\": f1\r\n        })\r\n        torch.save(model.state_dict(), f'/kaggle/working/Yolov3_epoch{epoch+1}.pth')\r\n\r\nwandb.finish()","metadata":{"_uuid":"0bd3ac44-a315-4577-b285-6ecf809d2e57","_cell_guid":"06d72187-0298-457a-bd7e-a1ebe3643561","collapsed":false,"jupyter":{"outputs_hidden":false},"editable":false,"trusted":true},"execution_count":null,"outputs":[]}]}