{"metadata":{"colab":{"provenance":[],"collapsed_sections":["yjwnv6MYoEgc"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1556326,"sourceType":"datasetVersion","datasetId":918769},{"sourceId":7240078,"sourceType":"datasetVersion","datasetId":4193279}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport numpy as np\nimport os\n\nimport torch\n\nfrom collections import Counter\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm","metadata":{"id":"qwgTtAyp3C7K","execution":{"iopub.status.busy":"2024-08-08T09:36:46.310938Z","iopub.execute_input":"2024-08-08T09:36:46.311194Z","iopub.status.idle":"2024-08-08T09:36:49.527023Z","shell.execute_reply.started":"2024-08-08T09:36:46.311170Z","shell.execute_reply":"2024-08-08T09:36:49.526067Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.login()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:37:05.862443Z","iopub.execute_input":"2024-08-08T09:37:05.863253Z","iopub.status.idle":"2024-08-08T09:37:18.340769Z","shell.execute_reply.started":"2024-08-08T09:37:05.863217Z","shell.execute_reply":"2024-08-08T09:37:18.339815Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"wandb.init(project='Untitled10', name='mblogge785-work')","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:37:25.728958Z","iopub.execute_input":"2024-08-08T09:37:25.729970Z","iopub.status.idle":"2024-08-08T09:37:56.942051Z","shell.execute_reply.started":"2024-08-08T09:37:25.729932Z","shell.execute_reply":"2024-08-08T09:37:56.941205Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmblogge785\u001b[0m (\u001b[33mmblogge785-work\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240808_093725-ygwh1blz</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mblogge785-work/Untitled10/runs/ygwh1blz' target=\"_blank\">mblogge785-work</a></strong> to <a href='https://wandb.ai/mblogge785-work/Untitled10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mblogge785-work/Untitled10' target=\"_blank\">https://wandb.ai/mblogge785-work/Untitled10</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mblogge785-work/Untitled10/runs/ygwh1blz' target=\"_blank\">https://wandb.ai/mblogge785-work/Untitled10/runs/ygwh1blz</a>"},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mblogge785-work/Untitled10/runs/ygwh1blz?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7ae79ad6c8b0>"},"metadata":{}}]},{"cell_type":"markdown","source":"# YOLO v3 model architecture","metadata":{"id":"yjwnv6MYoEgc"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\"\"\"\nArchitecture config:\n- Tuple --> (filters, kernel_size, stride)\n- List --> ['B', num_repeats] where 'B' is residual block\n- 'S' --> scale prediction block. Also for computing yolo loss\n- 'U' --> upsampling the feature map and concatenating with a previous layer\n\"\"\"\nconfig = [\n    (32, 3, 1),\n    (64, 3, 2),\n    [\"B\", 1],\n    (128, 3, 2),\n    [\"B\", 2],\n    (256, 3, 2),\n    [\"B\", 8],\n    (512, 3, 2),\n    [\"B\", 8],\n    (1024, 3, 2),\n    [\"B\", 4],  # To this point is Darknet-53\n\n    (512, 1, 1),\n    (1024, 3, 1),\n    \"S\",\n    (256, 1, 1),\n    \"U\",\n    (256, 1, 1),\n    (512, 3, 1),\n    \"S\",\n    (128, 1, 1),\n    \"U\",\n    (128, 1, 1),\n    (256, 3, 1),\n    \"S\",\n]\n","metadata":{"id":"zs0uKuG55xC_","execution":{"iopub.status.busy":"2024-08-08T09:38:28.848485Z","iopub.execute_input":"2024-08-08T09:38:28.849326Z","iopub.status.idle":"2024-08-08T09:38:28.858515Z","shell.execute_reply.started":"2024-08-08T09:38:28.849290Z","shell.execute_reply":"2024-08-08T09:38:28.857316Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class CNNBlock(nn.Module):\n  def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n    super(CNNBlock, self).__init__()\n    self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs) # If batchnorm layer(bn_act) is true, then bias is False\n    self.bn = nn.BatchNorm2d(out_channels)\n    self.leaky = nn.LeakyReLU(0.1)\n    self.use_bn_act = bn_act\n\n  def forward(self, x):\n    if self.use_bn_act:\n      return self.leaky(self.bn(self.conv(x)))\n    else:\n      return self.conv(x)\n\n\nclass ResidualBlock(nn.Module):\n  def __init__(self, channels, use_residual=True, num_repeats=1):\n    super(ResidualBlock, self).__init__()\n    self.layers = nn.ModuleList() # Like regular python list, but is container for pytorch nn modules\n\n    for repeat in range(num_repeats):\n      self.layers += [\n          nn.Sequential(\n            CNNBlock(channels, channels//2, kernel_size=1),\n            CNNBlock(channels//2, channels, kernel_size=3, padding=1)\n          )\n      ]\n\n    self.use_residual = use_residual\n    self.num_repeats = num_repeats\n\n  def forward(self, x):\n    for layer in self.layers:\n      if self.use_residual:\n        x = x + layer(x)\n      else:\n        x = layer(x)\n\n    return x","metadata":{"id":"Q_zaXn4YQCl7","execution":{"iopub.status.busy":"2024-08-08T09:38:33.119135Z","iopub.execute_input":"2024-08-08T09:38:33.119494Z","iopub.status.idle":"2024-08-08T09:38:33.131540Z","shell.execute_reply.started":"2024-08-08T09:38:33.119462Z","shell.execute_reply":"2024-08-08T09:38:33.130550Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class ScalePrediction(nn.Module):\n  def __init__(self, in_channels, num_classes):\n    super(ScalePrediction, self).__init__()\n    self.pred = nn.Sequential(\n        CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n        CNNBlock(2 * in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1), # (num_classes + 5) * 3 --> (20+5) for each anchor box which in total is 3\n    )\n    self.num_classes = num_classes\n\n  def forward(self, x):\n    return (\n        self.pred(x)\n        .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3]) # [batch_size, anchor_boxes, prediction(25), grid_h, grid_w]\n        .permute(0, 1, 3, 4, 2) # [batch_size, anchor_boxes, grid_h, grid_w, prediction(25)]\n      )","metadata":{"id":"IUrOUQNeXDcG","execution":{"iopub.status.busy":"2024-08-08T09:38:38.854408Z","iopub.execute_input":"2024-08-08T09:38:38.855571Z","iopub.status.idle":"2024-08-08T09:38:38.864138Z","shell.execute_reply.started":"2024-08-08T09:38:38.855528Z","shell.execute_reply":"2024-08-08T09:38:38.863094Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class YOLOv3(nn.Module):\n  def __init__(self, in_channels=3, num_classes=20):\n    super(YOLOv3, self).__init__()\n    self.num_classes = num_classes\n    self.in_channels = in_channels\n    self.layers = self._create_conv_layers()\n\n  def forward(self, x):\n    outputs = []\n    route_connections = []\n\n    for layer in self.layers:\n      if isinstance(layer, ScalePrediction):\n        outputs.append(layer(x))\n        continue\n\n      x = layer(x)\n\n      if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n        route_connections.append(x)\n\n      elif isinstance(layer, nn.Upsample):\n        x = torch.cat([x, route_connections[-1]], dim=1)\n        route_connections.pop()\n\n    return outputs\n\n\n  def _create_conv_layers(self):\n    layers = nn.ModuleList()\n    in_channels = self.in_channels\n\n    for module in config:\n      if isinstance(module, tuple):\n        out_channels, kernel_size, stride = module\n        layers.append(CNNBlock(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=1 if kernel_size == 3 else 0\n        ))\n        in_channels = out_channels\n\n      elif isinstance(module, list):\n        num_repeats = module[1]\n        layers.append(ResidualBlock(in_channels, num_repeats=num_repeats))\n\n      elif isinstance(module, str):\n        if module == \"S\":\n          layers += [\n              ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n              CNNBlock(in_channels, in_channels//2, kernel_size=1),\n              ScalePrediction(in_channels//2, num_classes = self.num_classes)\n          ]\n          in_channels = in_channels // 2\n\n        elif module == \"U\":\n          layers.append(nn.Upsample(scale_factor=2))\n          in_channels = in_channels * 3\n\n    return layers\n","metadata":{"id":"iwYyoG0AQXtj","execution":{"iopub.status.busy":"2024-08-08T09:38:40.523614Z","iopub.execute_input":"2024-08-08T09:38:40.523994Z","iopub.status.idle":"2024-08-08T09:38:40.539280Z","shell.execute_reply.started":"2024-08-08T09:38:40.523963Z","shell.execute_reply":"2024-08-08T09:38:40.538328Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"num_classes = 20\nIMAGE_SIZE = 416\nmodel = YOLOv3(num_classes=num_classes)\nx = torch.randn((2, 3, IMAGE_SIZE, IMAGE_SIZE))\nout = model(x)\nassert model(x)[0].shape == (2, 3, IMAGE_SIZE//32, IMAGE_SIZE//32, num_classes + 5)\nassert model(x)[1].shape == (2, 3, IMAGE_SIZE//16, IMAGE_SIZE//16, num_classes + 5)\nassert model(x)[2].shape == (2, 3, IMAGE_SIZE//8, IMAGE_SIZE//8, num_classes + 5)\nprint(\"Success!\")","metadata":{"id":"-S2pyv5SmViF","outputId":"a24522a5-0211-475d-8154-518fbcf6d0e5","execution":{"iopub.status.busy":"2024-08-08T09:38:44.132203Z","iopub.execute_input":"2024-08-08T09:38:44.133068Z","iopub.status.idle":"2024-08-08T09:38:49.048188Z","shell.execute_reply.started":"2024-08-08T09:38:44.133032Z","shell.execute_reply":"2024-08-08T09:38:49.047047Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Success!\n","output_type":"stream"}]},{"cell_type":"code","source":"model(x)[0].shape","metadata":{"id":"jc_-au2vmuSG","outputId":"3a3b154b-6320-4388-f716-3315f7c97f05","execution":{"iopub.status.busy":"2024-08-08T09:38:51.522329Z","iopub.execute_input":"2024-08-08T09:38:51.522707Z","iopub.status.idle":"2024-08-08T09:38:52.358337Z","shell.execute_reply.started":"2024-08-08T09:38:51.522654Z","shell.execute_reply":"2024-08-08T09:38:52.357189Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"torch.Size([2, 3, 13, 13, 25])"},"metadata":{}}]},{"cell_type":"code","source":"model(x)[1].shape","metadata":{"id":"cfXEjuaGn0Ey","outputId":"5ad71d82-0bad-4644-f778-b7b401f3f1ba","execution":{"iopub.status.busy":"2024-08-08T09:38:55.272609Z","iopub.execute_input":"2024-08-08T09:38:55.273040Z","iopub.status.idle":"2024-08-08T09:38:56.110824Z","shell.execute_reply.started":"2024-08-08T09:38:55.273004Z","shell.execute_reply":"2024-08-08T09:38:56.109555Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"torch.Size([2, 3, 26, 26, 25])"},"metadata":{}}]},{"cell_type":"code","source":"model(x)[2].shape","metadata":{"id":"efmVBBZ2n3Ea","outputId":"0134bee6-2d71-4176-cbc8-64b7a92a3b4a","execution":{"iopub.status.busy":"2024-08-08T09:38:57.816234Z","iopub.execute_input":"2024-08-08T09:38:57.816603Z","iopub.status.idle":"2024-08-08T09:38:58.605972Z","shell.execute_reply.started":"2024-08-08T09:38:57.816568Z","shell.execute_reply":"2024-08-08T09:38:58.604946Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"torch.Size([2, 3, 52, 52, 25])"},"metadata":{}}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:39:05.571928Z","iopub.execute_input":"2024-08-08T09:39:05.572552Z","iopub.status.idle":"2024-08-08T09:39:05.586580Z","shell.execute_reply.started":"2024-08-08T09:39:05.572518Z","shell.execute_reply":"2024-08-08T09:39:05.585747Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"YOLOv3(\n  (layers): ModuleList(\n    (0): CNNBlock(\n      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leaky): LeakyReLU(negative_slope=0.1)\n    )\n    (1): CNNBlock(\n      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leaky): LeakyReLU(negative_slope=0.1)\n    )\n    (2): ResidualBlock(\n      (layers): ModuleList(\n        (0): Sequential(\n          (0): CNNBlock(\n            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (leaky): LeakyReLU(negative_slope=0.1)\n          )\n          (1): CNNBlock(\n            (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (leaky): LeakyReLU(negative_slope=0.1)\n          )\n        )\n      )\n    )\n    (3): CNNBlock(\n      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leaky): LeakyReLU(negative_slope=0.1)\n    )\n    (4): ResidualBlock(\n      (layers): ModuleList(\n        (0-1): 2 x Sequential(\n          (0): CNNBlock(\n            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (leaky): LeakyReLU(negative_slope=0.1)\n          )\n          (1): CNNBlock(\n            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (leaky): LeakyReLU(negative_slope=0.1)\n          )\n        )\n      )\n    )\n    (5): CNNBlock(\n      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leaky): LeakyReLU(negative_slope=0.1)\n    )\n    (6): ResidualBlock(\n      (layers): ModuleList(\n        (0-7): 8 x Sequential(\n          (0): CNNBlock(\n            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (leaky): LeakyReLU(negative_slope=0.1)\n          )\n          (1): CNNBlock(\n            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (leaky): LeakyReLU(negative_slope=0.1)\n          )\n        )\n      )\n    )\n    (7): CNNBlock(\n      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leaky): LeakyReLU(negative_slope=0.1)\n    )\n    (8): ResidualBlock(\n      (layers): ModuleList(\n        (0-7): 8 x Sequential(\n          (0): CNNBlock(\n            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (leaky): LeakyReLU(negative_slope=0.1)\n          )\n          (1): CNNBlock(\n            (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (leaky): LeakyReLU(negative_slope=0.1)\n          )\n        )\n      )\n    )\n    (9): CNNBlock(\n      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leaky): LeakyReLU(negative_slope=0.1)\n    )\n    (10): ResidualBlock(\n      (layers): ModuleList(\n        (0-3): 4 x Sequential(\n          (0): CNNBlock(\n            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (leaky): LeakyReLU(negative_slope=0.1)\n          )\n          (1): CNNBlock(\n            (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (leaky): LeakyReLU(negative_slope=0.1)\n          )\n        )\n      )\n    )\n    (11): CNNBlock(\n      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leaky): LeakyReLU(negative_slope=0.1)\n    )\n    (12): CNNBlock(\n      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leaky): LeakyReLU(negative_slope=0.1)\n    )\n    (13): ResidualBlock(\n      (layers): ModuleList(\n        (0): Sequential(\n          (0): CNNBlock(\n            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (leaky): LeakyReLU(negative_slope=0.1)\n          )\n          (1): CNNBlock(\n            (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (leaky): LeakyReLU(negative_slope=0.1)\n          )\n        )\n      )\n    )\n    (14): CNNBlock(\n      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leaky): LeakyReLU(negative_slope=0.1)\n    )\n    (15): ScalePrediction(\n      (pred): Sequential(\n        (0): CNNBlock(\n          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (leaky): LeakyReLU(negative_slope=0.1)\n        )\n        (1): CNNBlock(\n          (conv): Conv2d(1024, 75, kernel_size=(1, 1), stride=(1, 1))\n          (bn): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (leaky): LeakyReLU(negative_slope=0.1)\n        )\n      )\n    )\n    (16): CNNBlock(\n      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leaky): LeakyReLU(negative_slope=0.1)\n    )\n    (17): Upsample(scale_factor=2.0, mode='nearest')\n    (18): CNNBlock(\n      (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leaky): LeakyReLU(negative_slope=0.1)\n    )\n    (19): CNNBlock(\n      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leaky): LeakyReLU(negative_slope=0.1)\n    )\n    (20): ResidualBlock(\n      (layers): ModuleList(\n        (0): Sequential(\n          (0): CNNBlock(\n            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (leaky): LeakyReLU(negative_slope=0.1)\n          )\n          (1): CNNBlock(\n            (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (leaky): LeakyReLU(negative_slope=0.1)\n          )\n        )\n      )\n    )\n    (21): CNNBlock(\n      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leaky): LeakyReLU(negative_slope=0.1)\n    )\n    (22): ScalePrediction(\n      (pred): Sequential(\n        (0): CNNBlock(\n          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (leaky): LeakyReLU(negative_slope=0.1)\n        )\n        (1): CNNBlock(\n          (conv): Conv2d(512, 75, kernel_size=(1, 1), stride=(1, 1))\n          (bn): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (leaky): LeakyReLU(negative_slope=0.1)\n        )\n      )\n    )\n    (23): CNNBlock(\n      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leaky): LeakyReLU(negative_slope=0.1)\n    )\n    (24): Upsample(scale_factor=2.0, mode='nearest')\n    (25): CNNBlock(\n      (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leaky): LeakyReLU(negative_slope=0.1)\n    )\n    (26): CNNBlock(\n      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leaky): LeakyReLU(negative_slope=0.1)\n    )\n    (27): ResidualBlock(\n      (layers): ModuleList(\n        (0): Sequential(\n          (0): CNNBlock(\n            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (leaky): LeakyReLU(negative_slope=0.1)\n          )\n          (1): CNNBlock(\n            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (leaky): LeakyReLU(negative_slope=0.1)\n          )\n        )\n      )\n    )\n    (28): CNNBlock(\n      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leaky): LeakyReLU(negative_slope=0.1)\n    )\n    (29): ScalePrediction(\n      (pred): Sequential(\n        (0): CNNBlock(\n          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (leaky): LeakyReLU(negative_slope=0.1)\n        )\n        (1): CNNBlock(\n          (conv): Conv2d(256, 75, kernel_size=(1, 1), stride=(1, 1))\n          (bn): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (leaky): LeakyReLU(negative_slope=0.1)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Count the total trainable parameters\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total trainable parameters: {total_params}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:39:11.804693Z","iopub.execute_input":"2024-08-08T09:39:11.805415Z","iopub.status.idle":"2024-08-08T09:39:11.814090Z","shell.execute_reply.started":"2024-08-08T09:39:11.805383Z","shell.execute_reply":"2024-08-08T09:39:11.812994Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Total trainable parameters: 61626499\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Config File","metadata":{"id":"dj8JoGrytb6y"}},{"cell_type":"code","source":"import cv2\nimport torch\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nNUM_WORKERS = 4\nBATCH_SIZE = 32\nIMAGE_SIZE = 416\nNUM_CLASSES = 20\nLEARNING_RATE = 1e-5\nNUM_EPOCHS = 80\nCONF_THRESHOLD = 0.8\nMAP_IOU_THRESH = 0.5\nNMS_IOU_THRESH = 0.45\nS = [IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8]\n\nIMG_DIR = \"/kaggle/input/pascalvoc-yolo/images\"\nLABEL_DIR = \"/kaggle/input/pascalvoc-yolo/labels\"\n\nANCHORS = [\n    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n] \n\n\nPASCAL_CLASSES = [\n    \"aeroplane\",\n    \"bicycle\",\n    \"bird\",\n    \"boat\",\n    \"bottle\",\n    \"bus\",\n    \"car\",\n    \"cat\",\n    \"chair\",\n    \"cow\",\n    \"diningtable\",\n    \"dog\",\n    \"horse\",\n    \"motorbike\",\n    \"person\",\n    \"pottedplant\",\n    \"sheep\",\n    \"sofa\",\n    \"train\",\n    \"tvmonitor\"\n]","metadata":{"id":"9Ai7PMaUteOY","execution":{"iopub.status.busy":"2024-08-08T09:39:15.566132Z","iopub.execute_input":"2024-08-08T09:39:15.566472Z","iopub.status.idle":"2024-08-08T09:39:15.770221Z","shell.execute_reply.started":"2024-08-08T09:39:15.566438Z","shell.execute_reply":"2024-08-08T09:39:15.769327Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# IOU width height\n- Take in hxw of anchor boxe and bounding box to calc. IOU","metadata":{"id":"aGho3C7Zw5jk"}},{"cell_type":"code","source":"def iou_width_height(boxes1, boxes2):\n    \"\"\"\n    Parameters:\n        boxes1 (tensor): width and height of the first bounding boxes\n        boxes2 (tensor): width and height of the second bounding boxes\n    Returns:\n        tensor: Intersection over union of the corresponding boxes\n    \"\"\"\n    intersection = torch.min(boxes1[..., 0], boxes2[..., 0]) * torch.min(\n        boxes1[..., 1], boxes2[..., 1]\n    )\n    union = (\n        boxes1[..., 0] * boxes1[..., 1] + boxes2[..., 0] * boxes2[..., 1] - intersection\n    )\n    return intersection / union","metadata":{"id":"4P448jivxbnu","execution":{"iopub.status.busy":"2024-08-08T09:39:21.223307Z","iopub.execute_input":"2024-08-08T09:39:21.223683Z","iopub.status.idle":"2024-08-08T09:39:21.231262Z","shell.execute_reply.started":"2024-08-08T09:39:21.223640Z","shell.execute_reply":"2024-08-08T09:39:21.230214Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Intersection over union","metadata":{"id":"uXiVIrYjxfnO"}},{"cell_type":"code","source":"def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n    \"\"\"\n    This function calculates intersection over union (iou) given pred boxes\n    and target boxes.\n\n    Parameters:\n        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n        box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n\n    Returns:\n        tensor: Intersection over union for all examples\n    \"\"\"\n\n    if box_format == \"midpoint\":\n        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n\n    if box_format == \"corners\":\n        box1_x1 = boxes_preds[..., 0:1]\n        box1_y1 = boxes_preds[..., 1:2]\n        box1_x2 = boxes_preds[..., 2:3]\n        box1_y2 = boxes_preds[..., 3:4]\n        box2_x1 = boxes_labels[..., 0:1]\n        box2_y1 = boxes_labels[..., 1:2]\n        box2_x2 = boxes_labels[..., 2:3]\n        box2_y2 = boxes_labels[..., 3:4]\n\n    x1 = torch.max(box1_x1, box2_x1)\n    y1 = torch.max(box1_y1, box2_y1)\n    x2 = torch.min(box1_x2, box2_x2)\n    y2 = torch.min(box1_y2, box2_y2)\n\n    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n\n    return intersection / (box1_area + box2_area - intersection + 1e-6)","metadata":{"id":"RvwUplkYxqrw","execution":{"iopub.status.busy":"2024-08-08T09:39:23.874436Z","iopub.execute_input":"2024-08-08T09:39:23.874808Z","iopub.status.idle":"2024-08-08T09:39:23.889284Z","shell.execute_reply.started":"2024-08-08T09:39:23.874779Z","shell.execute_reply":"2024-08-08T09:39:23.888146Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Non-max Supression","metadata":{"id":"IkcXigCkxv9_"}},{"cell_type":"code","source":"def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n    \"\"\"\n     Does Non Max Suppression given bboxes\n\n    Parameters:\n        bboxes (list): list of lists containing all bboxes with each bboxes\n        specified as [class_pred, prob_score, x1, y1, x2, y2]\n        iou_threshold (float): threshold where predicted bboxes is correct\n        threshold (float): threshold to remove predicted bboxes (independent of IoU)\n        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n\n    Returns:\n        list: bboxes after performing NMS given a specific IoU threshold\n    \"\"\"\n\n    assert type(bboxes) == list\n\n    bboxes = [box for box in bboxes if box[1] > threshold]\n    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n    bboxes_after_nms = []\n\n    while bboxes:\n        chosen_box = bboxes.pop(0)\n\n        bboxes = [\n            box\n            for box in bboxes\n            if box[0] != chosen_box[0]\n            or intersection_over_union(\n                torch.tensor(chosen_box[2:]),\n                torch.tensor(box[2:]),\n                box_format=box_format,\n            )\n            < iou_threshold\n        ]\n\n        bboxes_after_nms.append(chosen_box)\n\n    return bboxes_after_nms","metadata":{"id":"0oIq_NZpxzNi","execution":{"iopub.status.busy":"2024-08-08T09:39:26.126148Z","iopub.execute_input":"2024-08-08T09:39:26.126509Z","iopub.status.idle":"2024-08-08T09:39:26.136598Z","shell.execute_reply.started":"2024-08-08T09:39:26.126477Z","shell.execute_reply":"2024-08-08T09:39:26.135521Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"id":"6qWub9xCoJQP"}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport pandas as pd\nimport torch\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image, ImageFile","metadata":{"id":"17VUsG5An6ED","execution":{"iopub.status.busy":"2024-08-08T09:39:28.331035Z","iopub.execute_input":"2024-08-08T09:39:28.331376Z","iopub.status.idle":"2024-08-08T09:39:28.338283Z","shell.execute_reply.started":"2024-08-08T09:39:28.331349Z","shell.execute_reply":"2024-08-08T09:39:28.336986Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# allows PIL to load images even if they are truncated or incomplete\nImageFile.LOAD_TRUNCATED_IMAGES = True","metadata":{"id":"Lfov94HNpy4C","execution":{"iopub.status.busy":"2024-08-08T09:39:29.762162Z","iopub.execute_input":"2024-08-08T09:39:29.762847Z","iopub.status.idle":"2024-08-08T09:39:29.768226Z","shell.execute_reply.started":"2024-08-08T09:39:29.762814Z","shell.execute_reply":"2024-08-08T09:39:29.767263Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class YOLODataset(Dataset):\n  def __init__(self, csv_file, img_dir, label_dir, anchors,\n               image_size=416, S=[13,26,52], C=20, transform=None):\n    self.annotations = pd.read_csv(csv_file)\n    self.img_dir = img_dir\n    self.label_dir = label_dir\n    self.transform = transform\n    self.S = S\n\n    # Suppose, anchors[0] = [a,b,c], anchors[1] = [d,e,f], anchors[2] = [g,h,i] : Each set of anchors for each scale\n    # List addition gives shape 3x3\n    # Anchors per scale suggests that there are three different aspect ratios for each anchor position.\n    self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2]) # For all 3 scales\n    self.num_anchors = self.anchors.shape[0]\n    self.num_anchors_per_scale = self.num_anchors // 3\n\n    self.C = C\n\n    # If a cell has obj. then one anchor is responsible for outputting it,\n    # one that's responsible is the one that has highest iou with ground truth box\n    # but, there might be cases where there are several boxes in the same cell\n    self.ignore_iou_thresh = 0.5\n\n  def __len__(self):\n    return len(self.annotations)\n\n  def __getitem__(self, index):\n    label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n    bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist() # np.roll with shift 4 on axis 1: [class, x, y, w, h] --> [x, y, w, h, class]\n\n    img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n    image = Image.open(img_path)\n\n    if self.transform:\n      image = self.transform(image)\n\n    targets = [torch.zeros((self.num_anchors // 3, S, S, 6)) for S in self.S] # 6 because objectness score, bounding box coordinates (x, y, w, h), class label\n\n    for box in bboxes:\n      \"\"\"For each box in bboxes,\n      we want to assign which anchor should be responsible and\n      which cell should be responsible for all the three different scales prediction\"\"\"\n      iou_anchors = iou_width_height(torch.tensor(box[2:4]), self.anchors) # IOU from height and width\n      anchor_indices = iou_anchors.argsort(descending=True, dim=0) # Sorting sucht that the first is the best anchor\n\n      x, y, width, height, class_label = box\n      has_anchor = [False, False, False] # Make sure there is an anchor for each of three scales for each bounding box\n\n      for anchor_idx in anchor_indices:\n        scale_idx = anchor_idx // self.num_anchors_per_scale # scale_idx is either 0,1,2: 0-->13x13, 1:-->26x26, 2:-->52x52\n        anchor_on_scale = anchor_idx % self.num_anchors_per_scale # In each scale, choosing the anchor thats either 0,1,2\n\n        S = self.S[scale_idx]\n        i, j = int(S*y), int(S*x) # x=0.5, S=13 --> int(6.5) = 6 | i=y cell, j=x cell\n        anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n\n        if not anchor_taken and not has_anchor[scale_idx]:\n          targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n          x_cell, y_cell = S*x - j, S*y - i # 6.5 - 6 = 0.5 such that they are between [0,1]\n          width_cell, height_cell = (\n              width*S, # S=13, width=0.5, 6.5\n              height*S\n          )\n\n          box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n\n          targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n          targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n          has_anchor[scale_idx] = True\n\n        # Even if the same grid shares another anchor having iou>ignore_iou_thresh then,\n        elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n          targets[scale_idx][anchor_on_scale, i, j, 0] = -1 # ignore this prediction\n\n    return image, tuple(targets)\n","metadata":{"id":"UiM8IiJeqIV4","execution":{"iopub.status.busy":"2024-08-08T09:39:31.496637Z","iopub.execute_input":"2024-08-08T09:39:31.497482Z","iopub.status.idle":"2024-08-08T09:39:31.516620Z","shell.execute_reply.started":"2024-08-08T09:39:31.497441Z","shell.execute_reply":"2024-08-08T09:39:31.514940Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# DataLoader","metadata":{"id":"lJRincdJzCyU"}},{"cell_type":"code","source":"import torchvision.transforms as transforms\ntransform = transforms.Compose([transforms.Resize((416, 416)), transforms.ToTensor()])","metadata":{"id":"BsljMXYuXndg","execution":{"iopub.status.busy":"2024-08-08T09:39:35.681387Z","iopub.execute_input":"2024-08-08T09:39:35.681989Z","iopub.status.idle":"2024-08-08T09:39:36.067222Z","shell.execute_reply.started":"2024-08-08T09:39:35.681952Z","shell.execute_reply":"2024-08-08T09:39:36.066185Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def get_loaders(train_csv_path, test_csv_path):\n\n    train_dataset = YOLODataset(\n        train_csv_path,\n        transform=transform,\n        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n        img_dir=IMG_DIR,\n        label_dir=LABEL_DIR,\n        anchors=ANCHORS,\n    )\n    test_dataset = YOLODataset(\n        test_csv_path,\n        transform=transform,\n        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n        img_dir=IMG_DIR,\n        label_dir=LABEL_DIR,\n        anchors=ANCHORS,\n    )\n    train_loader = DataLoader(\n        dataset=train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=False,\n    )\n    test_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        drop_last=False,\n    )\n\n    return train_loader, test_loader","metadata":{"id":"10SzzqKdzExo","execution":{"iopub.status.busy":"2024-08-08T09:39:56.751286Z","iopub.execute_input":"2024-08-08T09:39:56.751655Z","iopub.status.idle":"2024-08-08T09:39:56.763418Z","shell.execute_reply.started":"2024-08-08T09:39:56.751624Z","shell.execute_reply":"2024-08-08T09:39:56.762261Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Mean Average Precision","metadata":{}},{"cell_type":"code","source":"def mean_average_precision(\n    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=4\n):\n    \"\"\"\n    Video explanation of this function:\n    https://youtu.be/FppOzcDvaDI\n\n    This function calculates mean average precision (mAP)\n\n    Parameters:\n        pred_boxes (list): list of lists containing all bboxes with each bboxes\n        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n        true_boxes (list): Similar as pred_boxes except all the correct ones\n        iou_threshold (float): threshold where predicted bboxes is correct\n        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n        num_classes (int): number of classes\n\n    Returns:\n        float: mAP value across all classes given a specific IoU threshold\n    \"\"\"\n\n    # list storing all AP for respective classes\n    average_precisions = []\n\n    # used for numerical stability later on\n    epsilon = 1e-6\n\n    for c in range(num_classes):\n        detections = []\n        ground_truths = []\n\n        # Go through all predictions and targets,\n        # and only add the ones that belong to the\n        # current class c\n        for detection in pred_boxes:\n            if detection[1] == c:\n                detections.append(detection)\n\n        for true_box in true_boxes:\n            if true_box[1] == c:\n                ground_truths.append(true_box)\n\n        # find the amount of bboxes for each training example\n        # Counter here finds how many ground truth bboxes we get\n        # for each training example, so let's say img 0 has 3,\n        # img 1 has 5 then we will obtain a dictionary with:\n        # amount_bboxes = {0:3, 1:5}\n        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n\n        # We then go through each key, val in this dictionary\n        # and convert to the following (w.r.t same example):\n        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n        for key, val in amount_bboxes.items():\n            amount_bboxes[key] = torch.zeros(val)\n\n        # sort by box probabilities which is index 2\n        detections.sort(key=lambda x: x[2], reverse=True)\n        TP = torch.zeros((len(detections)))\n        FP = torch.zeros((len(detections)))\n        total_true_bboxes = len(ground_truths)\n\n        # If none exists for this class then we can safely skip\n        if total_true_bboxes == 0:\n            continue\n\n        for detection_idx, detection in enumerate(detections):\n            # Only take out the ground_truths that have the same\n            # training idx as detection\n            ground_truth_img = [\n                bbox for bbox in ground_truths if bbox[0] == detection[0]\n            ]\n\n            num_gts = len(ground_truth_img)\n            best_iou = 0\n\n            for idx, gt in enumerate(ground_truth_img):\n                iou = intersection_over_union(\n                    torch.tensor(detection[3:]),\n                    torch.tensor(gt[3:]),\n                    box_format=box_format,\n                )\n\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_idx = idx\n\n            if best_iou > iou_threshold:\n                # only detect ground truth detection once\n                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n                    # true positive and add this bounding box to seen\n                    TP[detection_idx] = 1\n                    amount_bboxes[detection[0]][best_gt_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n\n            # if IOU is lower then the detection is a false positive\n            else:\n                FP[detection_idx] = 1\n\n        TP_cumsum = torch.cumsum(TP, dim=0)\n        FP_cumsum = torch.cumsum(FP, dim=0)\n        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n        # torch.trapz for numerical integration\n        average_precisions.append(torch.trapz(precisions, recalls))\n\n    return sum(average_precisions) / len(average_precisions)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:40:03.677073Z","iopub.execute_input":"2024-08-08T09:40:03.677805Z","iopub.status.idle":"2024-08-08T09:40:03.695082Z","shell.execute_reply.started":"2024-08-08T09:40:03.677771Z","shell.execute_reply":"2024-08-08T09:40:03.694074Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Get bboxes and convert cells to bboxes","metadata":{}},{"cell_type":"code","source":"def get_evaluation_bboxes(\n    loader,\n    model,\n    iou_threshold,\n    anchors,\n    threshold,\n    box_format=\"midpoint\",\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n):\n    # make sure model is in eval before get bboxes\n    model.eval()\n    train_idx = 0\n    all_pred_boxes = []\n    all_true_boxes = []\n    for batch_idx, (x, labels) in enumerate(loader):\n        x = x.float().to(device)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size = x.shape[0]\n        bboxes = [[] for _ in range(batch_size)]\n        for i in range(3):\n            S = predictions[i].shape[2] # grid cell size for each predictions\n            anchor = torch.tensor([*anchors[i]]).to(device) * S # anchor for each grid, prediction type\n            boxes_scale_i = cells_to_bboxes( # get bboxes for each image in the batch\n                predictions[i], anchor, S=S, is_preds=True\n            )\n            for idx, (box) in enumerate(boxes_scale_i): # for each image, append the bbox to corr. bboxes[idx]\n                bboxes[idx] += box\n\n        # we just want one bbox for each label, not one for each scale\n        true_bboxes = cells_to_bboxes(\n            labels[2], anchor, S=S, is_preds=False\n        )\n\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                bboxes[idx],\n                iou_threshold=iou_threshold,\n                threshold=threshold,\n                box_format=box_format,\n            )\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_bboxes[idx]:\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            train_idx += 1\n\n    model.train()\n    return all_pred_boxes, all_true_boxes","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:40:09.218528Z","iopub.execute_input":"2024-08-08T09:40:09.218917Z","iopub.status.idle":"2024-08-08T09:40:09.231862Z","shell.execute_reply.started":"2024-08-08T09:40:09.218882Z","shell.execute_reply":"2024-08-08T09:40:09.230498Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def cells_to_bboxes(predictions, anchors, S, is_preds=True):\n    \"\"\"\n    Scales the predictions coming from the model to\n    be relative to the entire image such that they for example later\n    can be plotted or.\n    INPUT:\n    predictions: tensor of size (N, 3, S, S, num_classes+5)\n    anchors: the anchors used for the predictions\n    S: the number of cells the image is divided in on the width (and height)\n    is_preds: whether the input is predictions or the true bounding boxes\n    OUTPUT:\n    converted_bboxes: the converted boxes of sizes (N, num_anchors, S, S, 1+5) with class index,\n                      object score, bounding box coordinates\n    \"\"\"\n    BATCH_SIZE = predictions.shape[0]\n    num_anchors = len(anchors)\n    box_predictions = predictions[..., 1:5]\n    if is_preds:\n        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n        scores = torch.sigmoid(predictions[..., 0:1])\n        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n    else:\n        scores = predictions[..., 0:1]\n        best_class = predictions[..., 5:6]\n\n    cell_indices = (\n        torch.arange(S)\n        .repeat(predictions.shape[0], 3, S, 1)\n        .unsqueeze(-1)\n        .to(predictions.device)\n    )\n    x = 1 / S * (box_predictions[..., 0:1] + cell_indices)\n    y = 1 / S * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n    w_h = 1 / S * box_predictions[..., 2:4]\n    converted_bboxes = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(BATCH_SIZE, num_anchors * S * S, 6)\n    return converted_bboxes.tolist()","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:40:15.560544Z","iopub.execute_input":"2024-08-08T09:40:15.561411Z","iopub.status.idle":"2024-08-08T09:40:15.573890Z","shell.execute_reply.started":"2024-08-08T09:40:15.561378Z","shell.execute_reply":"2024-08-08T09:40:15.572740Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Loss","metadata":{"id":"BlYBUIFvh9yd"}},{"cell_type":"code","source":"class YoloLoss(nn.Module):\n  def __init__(self):\n    super(YoloLoss, self).__init__()\n    self.mse = nn.MSELoss() # For bounding box loss\n    self.bce = nn.BCEWithLogitsLoss() # For multi-label prediction: Binary cross entropy\n    self.entropy = nn.CrossEntropyLoss() # For classification\n    self.sigmoid = nn.Sigmoid()\n\n    # Constants for significance of obj, or no obj.\n    self.lambda_class = 1\n    self.lambda_noobj = 10\n    self.lambda_obj = 1\n    self.lambda_box = 10\n\n  def forward(self, predictions, target, anchors):\n    obj = target[..., 0] == 1\n    noobj = target[..., 0] == 0\n\n    # No object Loss\n    ################\n    no_object_loss = self.bce(\n        (predictions[..., 0:1][noobj]), (target[..., 0:1][noobj])\n    )\n\n    # Object Loss\n    #############\n    anchors = anchors.reshape(1,3,1,1,2) # Anchors initial shape 3x2 --> 3 anchor boxes each of certain hxw (2)\n\n    # box_preds = [..., sigmoid(x), sigmoid(y), [p_w * exp(t_w)], [p_h * exp(t_h)], ...]\n    box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n\n    # iou between predicted box and target box\n    ious = intersection_over_union(box_preds[obj], target[..., 1:5][obj]).detach()\n\n    object_loss = self.bce(\n        (predictions[..., 0:1][obj]), (ious * target[..., 0:1][obj]) # target * iou because only intersected part object loss calc\n    )\n\n    # Box Coordinate Loss\n    #####################\n    predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3]) # x, y to be between [0,1]\n    target[..., 3:5] = torch.log(\n        (1e-6 + target[..., 3:5] / anchors)\n    ) # Exponential of hxw (taking log because opp. of exp)\n\n    box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n\n    # Class Loss\n    ############\n    class_loss = self.entropy(\n        (predictions[..., 5:][obj]), (target[..., 5][obj].long())\n    )\n\n    return(\n        self.lambda_box * box_loss\n        + self.lambda_obj * object_loss\n        + self.lambda_noobj * no_object_loss\n        + self.lambda_class * class_loss\n    )\n","metadata":{"id":"i5nJTWLRiAY1","execution":{"iopub.status.busy":"2024-08-08T09:40:22.436921Z","iopub.execute_input":"2024-08-08T09:40:22.437287Z","iopub.status.idle":"2024-08-08T09:40:22.452586Z","shell.execute_reply.started":"2024-08-08T09:40:22.437254Z","shell.execute_reply":"2024-08-08T09:40:22.451588Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Plot Image","metadata":{"id":"M2mlDBxU2pjJ"}},{"cell_type":"code","source":"def plot_image(image, boxes):\n    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n    cmap = plt.get_cmap(\"tab20b\")\n    class_labels = PASCAL_CLASSES\n    colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n    im = np.array(image)\n    height, width, _ = im.shape\n\n    # Create figure and axes\n    fig, ax = plt.subplots(1)\n    # Display the image\n    ax.imshow(im)\n\n    # box[0] is x midpoint, box[2] is width\n    # box[1] is y midpoint, box[3] is height\n\n    # Create a Rectangle patch\n    for box in boxes:\n        assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n        class_pred = box[0]\n        box = box[2:]\n        upper_left_x = box[0] - box[2] / 2\n        upper_left_y = box[1] - box[3] / 2\n        rect = patches.Rectangle(\n            (upper_left_x * width, upper_left_y * height),\n            box[2] * width,\n            box[3] * height,\n            linewidth=2,\n            edgecolor=colors[int(class_pred)],\n            facecolor=\"none\",\n        )\n        # Add the patch to the Axes\n        ax.add_patch(rect)\n        plt.text(\n            upper_left_x * width,\n            upper_left_y * height,\n            s=class_labels[int(class_pred)],\n            color=\"white\",\n            verticalalignment=\"top\",\n            bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n        )\n\n    plt.show()","metadata":{"id":"Xjl_AldR2rX9","execution":{"iopub.status.busy":"2024-08-08T09:40:27.263013Z","iopub.execute_input":"2024-08-08T09:40:27.263853Z","iopub.status.idle":"2024-08-08T09:40:27.274035Z","shell.execute_reply.started":"2024-08-08T09:40:27.263816Z","shell.execute_reply":"2024-08-08T09:40:27.273033Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"id":"--AS2dslvM-D"}},{"cell_type":"code","source":"# Instantiate the model\nmodel = YOLOv3(num_classes=NUM_CLASSES).to(DEVICE)\n\n# Compile the model\noptimizer = torch.optim.Adam(\n    model.parameters(), lr=LEARNING_RATE\n)\nloss_fn = YoloLoss()\n\n# Scaler\nscaler = torch.cuda.amp.GradScaler()\n\n# Train-Test Loader\ntrain_loader, test_loader = get_loaders(\n    train_csv_path='/kaggle/input/pascalvoc-yolo/test.csv', test_csv_path='/kaggle/input/pascalvoc-yolo/test.csv'\n)\n\n# Anchors\nscaled_anchors = (\n    torch.tensor(ANCHORS) * torch.tensor([13,26,52]).unsqueeze(1).unsqueeze(1).repeat(1,3,2)\n).to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:40:32.412709Z","iopub.execute_input":"2024-08-08T09:40:32.413081Z","iopub.status.idle":"2024-08-08T09:40:33.088061Z","shell.execute_reply.started":"2024-08-08T09:40:32.413051Z","shell.execute_reply":"2024-08-08T09:40:33.086826Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Save test loader to a file\ntorch.save(test_loader, '/kaggle/working/test_loader.pth')","metadata":{"execution":{"iopub.status.busy":"2024-08-08T09:40:37.503011Z","iopub.execute_input":"2024-08-08T09:40:37.503924Z","iopub.status.idle":"2024-08-08T09:40:37.521170Z","shell.execute_reply.started":"2024-08-08T09:40:37.503887Z","shell.execute_reply":"2024-08-08T09:40:37.520033Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\nfrom tqdm import tqdm\nimport time\n\nhistory_loss = [] # To plot the epoch vs. loss\n\nfor epoch in tqdm(range(NUM_EPOCHS), desc=\"Epochs\"):\n  model.train()\n\n  losses = []\n\n  start_time = time.time() # Start time of the epoch\n\n  for batch_idx, (x,y) in enumerate(train_loader):\n    x = x.to(DEVICE)\n    y0, y1, y2 = (y[0].to(DEVICE),\n                  y[1].to(DEVICE),\n                  y[2].to(DEVICE))\n\n    # context manager is used in PyTorch to automatically handle mixed-precision computations on CUDA-enabled GPUs\n    with torch.cuda.amp.autocast():\n      out = model(x)\n      loss = (\n          loss_fn(out[0], y0, scaled_anchors[0])\n          + loss_fn(out[1], y1, scaled_anchors[1])\n          + loss_fn(out[2], y2, scaled_anchors[2])\n      )\n\n    losses.append(loss.item())\n    \n    optimizer.zero_grad()\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\n  end_time = time.time()  # End time of the epoch\n  epoch_duration = end_time - start_time  # Duration of the epoch\n    \n  history_loss.append(sum(losses)/len(losses))\n\n  if (epoch+1) % 10 == 0:\n    # Print the epoch duration\n    tqdm.write(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds\")\n\n    # Print the loss and accuracy for training and validation data\n    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], \"\n          f\"Loss: {sum(losses)/len(losses):.4f}\")\n\n    # save the model after every 10 epoch\n    torch.save(model.state_dict(), f'/kaggle/working/Yolov3_epoch{epoch+1}.pth')\n\n","metadata":{"id":"QxkC5a_lFOvA","outputId":"cb8c11e8-e6ba-42b5-ff27-d580e23ab481","execution":{"iopub.status.busy":"2024-08-08T09:40:39.932718Z","iopub.execute_input":"2024-08-08T09:40:39.933096Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Epochs:  10%|█         | 8/80 [22:21<3:10:49, 159.02s/it]","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nepochs = range(1, len(history_loss)+1)\n\n# Plot losses\nplt.plot(epochs, history_loss)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Losses\")\nplt.title(\"Training Loss\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T16:31:26.376108Z","iopub.status.idle":"2023-12-19T16:31:26.376612Z","shell.execute_reply.started":"2023-12-19T16:31:26.376364Z","shell.execute_reply":"2023-12-19T16:31:26.376389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"model.eval()\nx, y = next(iter(test_loader))\nx = x.float().to(DEVICE)\n\nwith torch.no_grad():\n    out = model(x)\n    bboxes = [[] for _ in range(x.shape[0])]\n    batch_size, A, S, _, _ = out[0].shape\n    anchor = torch.tensor([*ANCHORS[0]]).to(DEVICE) * S\n    boxes_scale_i = cells_to_bboxes(\n        out[0], anchor, S=S, is_preds=True\n    )\n    for idx, (box) in enumerate(boxes_scale_i):\n        bboxes[idx] += box\n\n    for i in range(batch_size):\n        nms_boxes = non_max_suppression(\n            bboxes[i], iou_threshold=0.5, threshold=0.6, box_format=\"midpoint\",\n        )\n        plot_image(x[i].permute(1,2,0).detach().cpu(), nms_boxes)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T16:31:26.377669Z","iopub.status.idle":"2023-12-19T16:31:26.378022Z","shell.execute_reply.started":"2023-12-19T16:31:26.377857Z","shell.execute_reply":"2023-12-19T16:31:26.377873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"# Load the model\nmodel = YOLOv3(num_classes=NUM_CLASSES)\nmodel_path = \"/kaggle/input/80-epoch-yolov3-model/Yolov3_epoch80.pth\"\nstate_dict = torch.load(model_path)\nmodel.load_state_dict(state_dict)\nmodel = model.to(DEVICE)\n\n# Testing\nlosses = []\n\nwith torch.no_grad():\n    model.eval()\n\n    for batch_idx, (x,y) in enumerate(test_loader):\n        x = x.to(DEVICE)\n        y0, y1, y2 = (y[0].to(DEVICE),\n                    y[1].to(DEVICE),\n                    y[2].to(DEVICE))\n\n        out = model(x)\n        loss = (\n            loss_fn(out[0], y0, scaled_anchors[0])\n            + loss_fn(out[1], y1, scaled_anchors[1])\n            + loss_fn(out[2], y2, scaled_anchors[2])\n        )\n\n        losses.append(loss.item())\n\nprint(f\"Loss: {sum(losses)/len(losses):.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-19T16:35:37.731336Z","iopub.execute_input":"2023-12-19T16:35:37.732229Z","iopub.status.idle":"2023-12-19T16:37:53.28884Z","shell.execute_reply.started":"2023-12-19T16:35:37.732191Z","shell.execute_reply":"2023-12-19T16:37:53.287821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_boxes, true_boxes = get_evaluation_bboxes(\n                test_loader,\n                model,\n                iou_threshold=NMS_IOU_THRESH,\n                anchors=ANCHORS,\n                threshold=CONF_THRESHOLD,\n            )\n\nmapval = mean_average_precision(\n    pred_boxes,\n    true_boxes,\n    iou_threshold=MAP_IOU_THRESH,\n    box_format=\"midpoint\",\n    num_classes=NUM_CLASSES,\n)\nprint(f\"MAP: {mapval.item()}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-19T17:18:55.059103Z","iopub.execute_input":"2023-12-19T17:18:55.059769Z","iopub.status.idle":"2023-12-19T17:24:17.324672Z","shell.execute_reply.started":"2023-12-19T17:18:55.059735Z","shell.execute_reply":"2023-12-19T17:24:17.323695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}