{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1556326,"sourceType":"datasetVersion","datasetId":918769},{"sourceId":7240078,"sourceType":"datasetVersion","datasetId":4193279}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install wandb","metadata":{"id":"jrfeB5wtBC0B","execution":{"iopub.status.busy":"2024-08-05T19:57:25.462426Z","iopub.execute_input":"2024-08-05T19:57:25.462807Z","iopub.status.idle":"2024-08-05T19:57:25.467399Z","shell.execute_reply.started":"2024-08-05T19:57:25.462774Z","shell.execute_reply":"2024-08-05T19:57:25.466354Z"},"editable":false,"trusted":true},"execution_count":387,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image, ImageFile\nfrom collections import Counter\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport wandb  # Import wandb\nfrom transformers import BertTokenizer","metadata":{"id":"hSixTi4uBIfN","execution":{"iopub.status.busy":"2024-08-05T19:57:25.477026Z","iopub.execute_input":"2024-08-05T19:57:25.477709Z","iopub.status.idle":"2024-08-05T19:57:25.483287Z","shell.execute_reply.started":"2024-08-05T19:57:25.477680Z","shell.execute_reply":"2024-08-05T19:57:25.482280Z"},"editable":false,"trusted":true},"execution_count":388,"outputs":[]},{"cell_type":"code","source":"# Initialize the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"id":"j-EQyFuGpdiW","execution":{"iopub.status.busy":"2024-08-05T19:57:25.484825Z","iopub.execute_input":"2024-08-05T19:57:25.485117Z","iopub.status.idle":"2024-08-05T19:57:25.942726Z","shell.execute_reply.started":"2024-08-05T19:57:25.485091Z","shell.execute_reply":"2024-08-05T19:57:25.941825Z"},"editable":false,"trusted":true},"execution_count":389,"outputs":[]},{"cell_type":"code","source":"# Define configuration for YOLOv3\nconfig = [\n    (32, 3, 1),\n    (128, 3, 1),\n    (64, 3, 2),\n    [\"list\", 1],\n    (128, 3, 2),\n    [\"list\", 2],\n    (256, 3, 2),\n    [\"list\", 8],\n    (512, 3, 2),\n    [\"list\", 8],\n    (1024, 3, 2),\n    [\"list\", 4],\n    (512, 1, 1),\n    (1024, 3, 1),\n    \"sp\",\n    (256, 1, 1),\n    \"up\",\n    (256, 1, 1),\n    (512, 3, 1),\n    \"sp\",\n    (128, 1, 1),\n    \"up\",\n    (128, 1, 1),\n    (256, 3, 1),\n    \"sp\",\n]","metadata":{"id":"PFiBWZrpDg1w","execution":{"iopub.status.busy":"2024-08-05T19:57:25.944494Z","iopub.execute_input":"2024-08-05T19:57:25.944829Z","iopub.status.idle":"2024-08-05T19:57:25.952763Z","shell.execute_reply.started":"2024-08-05T19:57:25.944799Z","shell.execute_reply":"2024-08-05T19:57:25.951748Z"},"editable":false,"trusted":true},"execution_count":390,"outputs":[]},{"cell_type":"code","source":"# Define CNN block\nclass CNN_Block(nn.Module):\n    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n        super(CNN_Block, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.leaky = nn.LeakyReLU(0.1)\n        self.use_bn_act = bn_act\n\n    def forward(self, x):\n        if self.use_bn_act:\n            return self.leaky(self.bn(self.conv(x)))\n        else:\n            return self.conv(x)","metadata":{"id":"xgUPQ25bD73i","execution":{"iopub.status.busy":"2024-08-05T19:57:25.953942Z","iopub.execute_input":"2024-08-05T19:57:25.954267Z","iopub.status.idle":"2024-08-05T19:57:25.966524Z","shell.execute_reply.started":"2024-08-05T19:57:25.954236Z","shell.execute_reply":"2024-08-05T19:57:25.965702Z"},"editable":false,"trusted":true},"execution_count":391,"outputs":[]},{"cell_type":"code","source":"# Define Residual block\nclass Residual_Block(nn.Module):\n    def __init__(self, channels, use_residual=True, num_repeats=1):\n        super(Residual_Block, self).__init__()\n        self.layers = nn.ModuleList()\n\n        for repeat in range(num_repeats):\n            self.layers += [\n                nn.Sequential(\n                    CNN_Block(channels, channels//2, kernel_size=1),\n                    CNN_Block(channels//2, channels, kernel_size=3, padding=1)\n                )\n            ]\n\n        self.use_residual = use_residual\n        self.num_repeats = num_repeats\n\n    def forward(self, x):\n        for layer in self.layers:\n            if self.use_residual:\n                x = x + layer(x)\n            else:\n                x = layer(x)\n\n        return x","metadata":{"id":"30BGaEvUEAG9","execution":{"iopub.status.busy":"2024-08-05T19:57:25.968991Z","iopub.execute_input":"2024-08-05T19:57:25.969372Z","iopub.status.idle":"2024-08-05T19:57:25.977949Z","shell.execute_reply.started":"2024-08-05T19:57:25.969347Z","shell.execute_reply":"2024-08-05T19:57:25.976986Z"},"editable":false,"trusted":true},"execution_count":392,"outputs":[]},{"cell_type":"code","source":"# Define Prediction Scale\nclass Prediction_Scale(nn.Module):\n    def __init__(self, in_channels, NumClasses):\n        super(Prediction_Scale, self).__init__()\n        self.pred = nn.Sequential(\n            CNN_Block(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n            CNN_Block(2 * in_channels, (NumClasses + 5) * 3, bn_act=False, kernel_size=1),\n        )\n        self.NumClasses = NumClasses\n\n    def forward(self, x):\n        return (\n            self.pred(x)\n            .reshape(x.shape[0], 3, self.NumClasses + 5, x.shape[2], x.shape[3])\n            .permute(0, 1, 3, 4, 2)\n        )","metadata":{"id":"WmWG3n0TEKR-","execution":{"iopub.status.busy":"2024-08-05T19:57:25.979029Z","iopub.execute_input":"2024-08-05T19:57:25.979695Z","iopub.status.idle":"2024-08-05T19:57:25.987967Z","shell.execute_reply.started":"2024-08-05T19:57:25.979669Z","shell.execute_reply":"2024-08-05T19:57:25.987095Z"},"editable":false,"trusted":true},"execution_count":393,"outputs":[]},{"cell_type":"code","source":"class YOLOv3(nn.Module):\n  def __init__(self, in_channels=3, NumClasses=20):\n    super(YOLOv3, self).__init__()\n    self.NumClasses = NumClasses\n    self.in_channels = in_channels\n    self.layers = self._create_conv_layers()\n\n  def forward(self, x):\n    outputs = []\n    route_connections = []\n\n    for layer in self.layers:\n      if isinstance(layer, Prediction_Scale):\n        outputs.append(layer(x))\n        continue\n\n      x = layer(x)\n\n      if isinstance(layer, Residual_Block) and layer.num_repeats == 8:\n        route_connections.append(x)\n\n      elif isinstance(layer, nn.Upsample):\n        x = torch.cat([x, route_connections[-1]], dim=1)\n        route_connections.pop()\n\n    return outputs\n\n  def _create_conv_layers(self):\n    layers = nn.ModuleList()\n    in_channels = self.in_channels\n\n    for module in config:\n      if isinstance(module, tuple):\n        out_channels, kernel_size, stride = module\n        layers.append(CNN_Block(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=1 if kernel_size == 3 else 0\n        ))\n        in_channels = out_channels\n\n      elif isinstance(module, list):\n        num_repeats = module[1]\n        layers.append(Residual_Block(in_channels, num_repeats=num_repeats))\n\n      elif isinstance(module, str):\n        if module == \"sp\":\n          layers += [\n              Residual_Block(in_channels, use_residual=False, num_repeats=1),\n              CNN_Block(in_channels, in_channels//2, kernel_size=1),\n              Prediction_Scale(in_channels//2, NumClasses = self.NumClasses)\n          ]\n          in_channels = in_channels // 2\n\n        elif module == \"up\":\n          layers.append(nn.Upsample(scale_factor=2))\n          in_channels = in_channels * 3\n\n    return layers","metadata":{"id":"xkCgT8d4EaWa","execution":{"iopub.status.busy":"2024-08-05T19:57:25.989179Z","iopub.execute_input":"2024-08-05T19:57:25.989448Z","iopub.status.idle":"2024-08-05T19:57:26.003946Z","shell.execute_reply.started":"2024-08-05T19:57:25.989424Z","shell.execute_reply":"2024-08-05T19:57:26.003046Z"},"editable":false,"trusted":true},"execution_count":394,"outputs":[]},{"cell_type":"code","source":"NumClasses = 20\nImageSize = 416\nmodel = YOLOv3(NumClasses=NumClasses)\nx = torch.randn((2, 3, ImageSize, ImageSize))\nout = model(x)\nassert model(x)[0].shape == (2, 3, ImageSize//32, ImageSize//32, NumClasses + 5)\nassert model(x)[1].shape == (2, 3, ImageSize//16, ImageSize//16, NumClasses + 5)\nassert model(x)[2].shape == (2, 3, ImageSize//8, ImageSize//8, NumClasses + 5)\nprint(\"Success!\")","metadata":{"execution":{"iopub.status.busy":"2024-08-05T19:57:26.005990Z","iopub.execute_input":"2024-08-05T19:57:26.006276Z","iopub.status.idle":"2024-08-05T19:57:31.890672Z","shell.execute_reply.started":"2024-08-05T19:57:26.006252Z","shell.execute_reply":"2024-08-05T19:57:31.889711Z"},"editable":false,"trusted":true},"execution_count":395,"outputs":[{"name":"stdout","text":"Success!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define utility functions\ndef WeidthHeight(boxa, boxb):\n    intersection = torch.min(boxa[..., 0], boxb[..., 0]) * torch.min(\n        boxa[..., 1], boxb[..., 1]\n    )\n    union = (\n        boxa[..., 0] * boxa[..., 1] + boxb[..., 0] * boxb[..., 1] - intersection\n    )\n    return intersection / union\n\ndef non_max_suppression(boxx, iou_threshold, threshold, box_format=\"corners\"):\n    assert type(boxx) == list\n\n    boxx = [box for box in boxx if box[1] > threshold]\n    boxx = sorted(boxx, key=lambda x: x[1], reverse=True)\n    boxx_after_nms = []\n\n    while boxx:\n        chosen_box = boxx.pop(0)\n\n        boxx = [\n            box\n            for box in boxx\n            if box[0] != chosen_box[0]\n            or InterctionOverUnion(\n                torch.tensor(chosen_box[2:]),\n                torch.tensor(box[2:]),\n                box_format=box_format,\n            )\n            < iou_threshold\n        ]\n\n        boxx_after_nms.append(chosen_box)\n\n    return boxx_after_nms\n\ndef InterctionOverUnion(PredsBox, lableBox, box_format=\"midpoint\"):\n    if box_format == \"midpoint\":\n        box1_a1 = PredsBox[..., 0:1] - PredsBox[..., 2:3] / 2\n        box1_b1 = PredsBox[..., 1:2] - PredsBox[..., 3:4] / 2\n        box1_a2 = PredsBox[..., 0:1] + PredsBox[..., 2:3] / 2\n        box1_b2 = PredsBox[..., 1:2] + PredsBox[..., 3:4] / 2\n        box2_a1 = lableBox[..., 0:1] - lableBox[..., 2:3] / 2\n        box2_y1 = lableBox[..., 1:2] - lableBox[..., 3:4] / 2\n        box2_a2 = lableBox[..., 0:1] + lableBox[..., 2:3] / 2\n        box2_y2 = lableBox[..., 1:2] + lableBox[..., 3:4] / 2\n\n    if box_format == \"corners\":\n        box1_a1 = PredsBox[..., 0:1]\n        box1_b1 = PredsBox[..., 1:2]\n        box1_a2 = PredsBox[..., 2:3]\n        box1_b2 = PredsBox[..., 3:4]\n        box2_a1 = lableBox[..., 0:1]\n        box2_y1 = lableBox[..., 1:2]\n        box2_a2 = lableBox[..., 2:3]\n        box2_y2 = lableBox[..., 3:4]\n\n    x1 = torch.max(box1_a1, box2_a1)\n    y1 = torch.max(box1_b1, box2_y1)\n    x2 = torch.min(box1_a2, box2_a2)\n    y2 = torch.min(box1_b2, box2_y2)\n    intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n    box1_area = (box1_a2 - box1_a1) * (box1_b2 - box1_b1)\n    box2_area = (box2_a2 - box2_a1) * (box2_y2 - box2_y1)\n    iou = intersection / (box1_area + box2_area - intersection)\n\n    return iou","metadata":{"id":"uuvFaoFbEwfR","execution":{"iopub.status.busy":"2024-08-05T19:57:31.892073Z","iopub.execute_input":"2024-08-05T19:57:31.893183Z","iopub.status.idle":"2024-08-05T19:57:31.910285Z","shell.execute_reply.started":"2024-08-05T19:57:31.893144Z","shell.execute_reply":"2024-08-05T19:57:31.909344Z"},"editable":false,"trusted":true},"execution_count":396,"outputs":[]},{"cell_type":"code","source":"#added own\n# allows PIL to load images even if they are truncated or incomplete\nImageFile.LOAD_TRUNCATED_IMAGES = True","metadata":{"id":"nOAqW-MGFJXE","execution":{"iopub.status.busy":"2024-08-05T19:57:31.911564Z","iopub.execute_input":"2024-08-05T19:57:31.912094Z","iopub.status.idle":"2024-08-05T19:57:31.928959Z","shell.execute_reply.started":"2024-08-05T19:57:31.912067Z","shell.execute_reply":"2024-08-05T19:57:31.928058Z"},"editable":false,"trusted":true},"execution_count":397,"outputs":[]},{"cell_type":"code","source":"#added own imp\nclass YOLODataset(Dataset):\n  def __init__(self, csv_file, ImgDir, LableDir, anchors,\n               ImageSize=416, sp=[13,26,52], cp=20, transform=None):\n    self.annotations = pd.read_csv(csv_file)\n    self.ImgDir = ImgDir\n    self.LableDir = LableDir\n    self.transform = transform\n    self.sp = sp\n\n    self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2]) # For all 3 scales\n    self.num_anchors = self.anchors.shape[0]\n    self.num_anchors_per_scale = self.num_anchors // 3\n\n    self.cp = cp\n\n    self.ignore_iou_thresh = 0.5\n\n  def __len__(self):\n    return len(self.annotations)\n\n  def __getitem__(self, index):\n    label_path = os.path.join(self.LableDir, self.annotations.iloc[index, 1])\n    boxx = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist() # np.roll with shift 4 on axis 1: [class, x, y, w, h] --> [x, y, w, h, class]\n\n    img_path = os.path.join(self.ImgDir, self.annotations.iloc[index, 0])\n    image = Image.open(img_path)\n\n    if self.transform:\n      image = self.transform(image)\n\n    targets = [torch.zeros((self.num_anchors // 3, sp, sp, 6)) for sp in self.sp] # 6 because objectness score, bounding box coordinates (x, y, w, h), class label\n\n    for box in boxx:\n      iou_anchors = WeidthHeight(torch.tensor(box[2:4]), self.anchors) # IOU from height and width\n      anchor_indices = iou_anchors.argsort(descending=True, dim=0) # Sorting sucht that the first is the best anchor\n\n      x, y, width, height, class_label = box\n      has_anchor = [False, False, False] # Make sure there is an anchor for each of three scales for each bounding box\n\n      for anchor_idx in anchor_indices:\n        scale_idx = anchor_idx // self.num_anchors_per_scale # scale_idx is either 0,1,2: 0-->13x13, 1:-->26x26, 2:-->52x52\n        anchor_on_scale = anchor_idx % self.num_anchors_per_scale # In each scale, choosing the anchor thats either 0,1,2\n\n        sp = self.sp[scale_idx]\n        i, j = int(sp*y), int(sp*x) # x=0.5, sp=13 --> int(6.5) = 6 | i=y cell, j=x cell\n        anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n\n        if not anchor_taken and not has_anchor[scale_idx]:\n          targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n          x_cell, y_cell = sp*x - j, sp*y - i # 6.5 - 6 = 0.5 such that they are between [0,1]\n          width_cell, height_cell = (\n              width*sp, # sp=13, width=0.5, 6.5\n              height*sp\n          )\n\n          box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n\n          targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n          targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n          has_anchor[scale_idx] = True\n\n        # Even if the same grid shares another anchor having iou>ignore_iou_thresh then,\n        elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n          targets[scale_idx][anchor_on_scale, i, j, 0] = -1 # ignore this prediction\n\n    return image, tuple(targets)","metadata":{"id":"mmYZNNBFGQuI","execution":{"iopub.status.busy":"2024-08-05T19:57:31.933193Z","iopub.execute_input":"2024-08-05T19:57:31.933502Z","iopub.status.idle":"2024-08-05T19:57:31.950687Z","shell.execute_reply.started":"2024-08-05T19:57:31.933476Z","shell.execute_reply":"2024-08-05T19:57:31.949864Z"},"editable":false,"trusted":true},"execution_count":398,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms as transforms\ntransform = transforms.Compose([transforms.Resize((416, 416)), transforms.ToTensor()])","metadata":{"id":"Zj_rMGdgkSMV","execution":{"iopub.status.busy":"2024-08-05T19:57:31.951738Z","iopub.execute_input":"2024-08-05T19:57:31.952073Z","iopub.status.idle":"2024-08-05T19:57:31.963694Z","shell.execute_reply.started":"2024-08-05T19:57:31.952039Z","shell.execute_reply":"2024-08-05T19:57:31.962840Z"},"editable":false,"trusted":true},"execution_count":399,"outputs":[]},{"cell_type":"code","source":"#added own imp\ndef get_loaders(train_csv_path, test_csv_path):\n\n    train_dataset = YOLODataset(\n        train_csv_path,\n        transform=transform,\n        sp=[ImageSize // 32, ImageSize // 16, ImageSize // 8],\n        ImgDir=DirImage,\n        LableDir=DirLable,\n        anchors=ANCHORS,\n    )\n    test_dataset = YOLODataset(\n        test_csv_path,\n        transform=transform,\n        sp=[ImageSize // 32, ImageSize // 16, ImageSize // 8],\n        ImgDir=DirImage,\n        LableDir=DirLable,\n        anchors=ANCHORS,\n    )\n    train_loader = DataLoader(\n        dataset=train_dataset,\n        batch_size=SizeOfBatch,\n        shuffle=True,\n        drop_last=False,\n    )\n    test_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=SizeOfBatch,\n        shuffle=False,\n        drop_last=False,\n    )\n\n    return train_loader, test_loader","metadata":{"id":"KrFm4bATG-Sm","execution":{"iopub.status.busy":"2024-08-05T19:57:31.964849Z","iopub.execute_input":"2024-08-05T19:57:31.965562Z","iopub.status.idle":"2024-08-05T19:57:31.974889Z","shell.execute_reply.started":"2024-08-05T19:57:31.965529Z","shell.execute_reply":"2024-08-05T19:57:31.973819Z"},"editable":false,"trusted":true},"execution_count":400,"outputs":[]},{"cell_type":"code","source":"def mean_average_precision(\n    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", NumClasses=4\n):\n\n    # list storing all AP for respective classes\n    average_precisions = []\n\n    # used for numerical stability later on\n    epsilon = 1e-6\n\n    for c in range(NumClasses):\n        detections = []\n        ground_truths = []\n\n        for detection in pred_boxes:\n            if detection[1] == c:\n                detections.append(detection)\n\n        for true_box in true_boxes:\n            if true_box[1] == c:\n                ground_truths.append(true_box)\n\n        amount_boxx = Counter([gt[0] for gt in ground_truths])\n\n        for key, val in amount_boxx.items():\n            amount_boxx[key] = torch.zeros(val)\n\n        # sort by box probabilities which is index 2\n        detections.sort(key=lambda x: x[2], reverse=True)\n        TP = torch.zeros((len(detections)))\n        FP = torch.zeros((len(detections)))\n        total_true_boxx = len(ground_truths)\n\n        # If none exists for this class then we can safely skip\n        if total_true_boxx == 0:\n            continue\n\n        for detection_idx, detection in enumerate(detections):\n            ground_truth_img = [\n                bbox for bbox in ground_truths if bbox[0] == detection[0]\n            ]\n\n            num_gts = len(ground_truth_img)\n            best_iou = 0\n\n            for idx, gt in enumerate(ground_truth_img):\n                iou = InterctionOverUnion(\n                    torch.tensor(detection[3:]),\n                    torch.tensor(gt[3:]),\n                    box_format=box_format,\n                )\n\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_idx = idx\n\n            if best_iou > iou_threshold:\n                # only detect ground truth detection once\n                if amount_boxx[detection[0]][best_gt_idx] == 0:\n                    # true positive and add this bounding box to seen\n                    TP[detection_idx] = 1\n                    amount_boxx[detection[0]][best_gt_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n\n            # if IOU is lower then the detection is a false positive\n            else:\n                FP[detection_idx] = 1\n\n        TP_cumsum = torch.cumsum(TP, dim=0)\n        FP_cumsum = torch.cumsum(FP, dim=0)\n        recalls = TP_cumsum / (total_true_boxx + epsilon)\n        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n        # torch.trapz for numerical integration\n        average_precisions.append(torch.trapz(precisions, recalls))\n\n    return sum(average_precisions) / len(average_precisions)","metadata":{"id":"XqvAW5UoizM3","execution":{"iopub.status.busy":"2024-08-05T19:57:31.976361Z","iopub.execute_input":"2024-08-05T19:57:31.976688Z","iopub.status.idle":"2024-08-05T19:57:31.991006Z","shell.execute_reply.started":"2024-08-05T19:57:31.976664Z","shell.execute_reply":"2024-08-05T19:57:31.990118Z"},"editable":false,"trusted":true},"execution_count":401,"outputs":[]},{"cell_type":"code","source":"#added own\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nWorkersNo = 4\nSizeOfBatch = 32\nImageSize = 416\nClassesNo = 20\nRateOfLearning = 1e-5\nepochsno = 150\nThresholdConf = 0.8\nThreshMap = 0.5\nThreshNms = 0.45\nsp = [ImageSize // 32, ImageSize // 16, ImageSize // 8]\n\nDirImage = \"/kaggle/input/pascalvoc-yolo/images\"\nDirLable = \"/kaggle/input/pascalvoc-yolo/labels\"\n\nANCHORS = [\n    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n]\n\n\nAllClacess = [\n    \"aeroplane\",\n    \"bicycle\",\n    \"bird\",\n    \"boat\",\n    \"bottle\",\n    \"bus\",\n    \"car\",\n    \"cat\",\n    \"chair\",\n    \"cow\",\n    \"diningtable\",\n    \"dog\",\n    \"horse\",\n    \"motorbike\",\n    \"person\",\n    \"pottedplant\",\n    \"sheep\",\n    \"sofa\",\n    \"train\",\n    \"tvmonitor\"\n]","metadata":{"id":"lRTkY-6e6Cb7","execution":{"iopub.status.busy":"2024-08-05T19:57:31.992228Z","iopub.execute_input":"2024-08-05T19:57:31.992496Z","iopub.status.idle":"2024-08-05T19:57:32.004086Z","shell.execute_reply.started":"2024-08-05T19:57:31.992472Z","shell.execute_reply":"2024-08-05T19:57:32.003186Z"},"editable":false,"trusted":true},"execution_count":402,"outputs":[]},{"cell_type":"code","source":"# Evaluation function\ndef get_evaluation_boxx(\n    loader,\n    model,\n    iou_threshold,\n    anchors,\n    num_classes,\n    threshold=0.4,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    verbose=True,\n):\n    model.eval()\n    boxes = []\n    targets = []\n    with torch.no_grad():\n        for images, targets_batch in loader:\n            images = images.to(device)\n            targets_batch = [target.to(device) for target in targets_batch]\n\n            preds = model(images)\n            preds = [\n                pred.view(pred.shape[0], -1, num_classes + 5)\n                for pred in preds\n            ]\n\n            # Apply non-max suppression\n            for i in range(len(preds)):\n                pred = preds[i]\n                preds[i] = non_max_suppression(\n                    pred.cpu().numpy(), iou_threshold, threshold\n                )\n\n            for i, pred in enumerate(preds):\n                for box in pred:\n                    boxes.append(box)\n\n            targets.extend(targets_batch)\n\n    return boxes, targets\n","metadata":{"id":"drSirQw4jJNQ","execution":{"iopub.status.busy":"2024-08-05T19:57:32.005236Z","iopub.execute_input":"2024-08-05T19:57:32.005494Z","iopub.status.idle":"2024-08-05T19:57:32.017896Z","shell.execute_reply.started":"2024-08-05T19:57:32.005471Z","shell.execute_reply":"2024-08-05T19:57:32.016961Z"},"editable":false,"trusted":true},"execution_count":403,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(pred_boxes, true_boxes, iou_threshold=0.5):\n    # Your implementation for computing precision, recall, and f1\n    pred_classes = [box[0] for box in pred_boxes]\n    pred_iou = [box[1] for box in pred_boxes]\n    \n    tp, fp, fn = 0, 0, 0\n    \n    for pred, iou in zip(pred_classes, pred_iou):\n        if iou > iou_threshold:\n            tp += 1\n        else:\n            fp += 1\n    \n    true_classes = [box[0] for box in true_boxes]\n    for true in true_classes:\n        if not any(pred == true for pred in pred_classes):\n            fn += 1\n    \n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    return precision, recall, f1\n\ndef get_evaluation_boxx(\n    loader,\n    model,\n    iou_threshold,\n    anchors,\n    num_classes,\n    threshold=0.4,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    verbose=True,\n):\n    model.eval()\n    pred_boxes = []\n    true_boxes = []\n    \n    with torch.no_grad():\n        for images, targets_batch in loader:\n            images = images.to(device)\n            targets_batch = [target.to(device) for target in targets_batch]\n\n            preds = model(images)\n            preds = [\n                pred.view(pred.shape[0], -1, num_classes + 5)\n                for pred in preds\n            ]\n\n            # Apply non-max suppression\n            for i in range(len(preds)):\n                pred = preds[i]\n                preds[i] = non_max_suppression(\n                    pred.cpu().numpy(), iou_threshold, threshold\n                )\n\n            for i, pred in enumerate(preds):\n                pred_boxes.extend(pred)\n\n            for target in targets_batch:\n                target = target.cpu().numpy()\n                true_boxes.extend(target)\n    \n    precision, recall, f1 = compute_metrics(pred_boxes, true_boxes, iou_threshold)\n    \n    # Collect results for logging\n    results = {\n        \"pred_boxes\": pred_boxes,\n        \"true_boxes\": true_boxes,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    }\n    \n    return results","metadata":{"execution":{"iopub.status.busy":"2024-08-05T20:09:11.193832Z","iopub.execute_input":"2024-08-05T20:09:11.194691Z","iopub.status.idle":"2024-08-05T20:09:11.208370Z","shell.execute_reply.started":"2024-08-05T20:09:11.194657Z","shell.execute_reply":"2024-08-05T20:09:11.207321Z"},"editable":false,"trusted":true},"execution_count":412,"outputs":[]},{"cell_type":"code","source":"#added own imp\ndef cells_to_boxx(predictions, anchors, sp, is_preds=True):\n\n    SizeOfBatch = predictions.shape[0]\n    num_anchors = len(anchors)\n    box_predictions = predictions[..., 1:5]\n    if is_preds:\n        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n        scores = torch.sigmoid(predictions[..., 0:1])\n        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n    else:\n        scores = predictions[..., 0:1]\n        best_class = predictions[..., 5:6]\n\n    cell_indices = (\n        torch.arange(sp)\n        .repeat(predictions.shape[0], 3, sp, 1)\n        .unsqueeze(-1)\n        .to(predictions.DEVICE)\n    )\n    x = 1 / sp * (box_predictions[..., 0:1] + cell_indices)\n    y = 1 / sp * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n    w_h = 1 / sp * box_predictions[..., 2:4]\n    converted_boxx = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(SizeOfBatch, num_anchors * sp * sp, 6)\n    return converted_boxx.tolist()","metadata":{"id":"9sWupTfzIDnn","execution":{"iopub.status.busy":"2024-08-05T19:57:32.028754Z","iopub.execute_input":"2024-08-05T19:57:32.029043Z","iopub.status.idle":"2024-08-05T19:57:32.041868Z","shell.execute_reply.started":"2024-08-05T19:57:32.029018Z","shell.execute_reply":"2024-08-05T19:57:32.041006Z"},"editable":false,"trusted":true},"execution_count":405,"outputs":[]},{"cell_type":"code","source":"#added own imp\nclass YoloLoss(nn.Module):\n  def __init__(self):\n    super(YoloLoss, self).__init__()\n    self.mse = nn.MSELoss() # For bounding box loss\n    self.bce = nn.BCEWithLogitsLoss() # For multi-label prediction: Binary cross entropy\n    self.entropy = nn.CrossEntropyLoss() # For classification\n    self.sigmoid = nn.Sigmoid()\n\n    # Constants for significance of obj, or no obj.\n    self.lambda_class = 1\n    self.lambda_noobj = 10\n    self.lambda_obj = 1\n    self.lambda_box = 10\n\n  def forward(self, predictions, target, anchors):\n    obj = target[..., 0] == 1\n    noobj = target[..., 0] == 0\n\n    no_object_loss = self.bce(\n        (predictions[..., 0:1][noobj]), (target[..., 0:1][noobj])\n    )\n\n    anchors = anchors.reshape(1,3,1,1,2) # Anchors initial shape 3x2 --> 3 anchor boxes each of certain hxw (2)\n\n    # box_preds = [..., sigmoid(x), sigmoid(y), [p_w * exp(t_w)], [p_h * exp(t_h)], ...]\n    box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n\n    # iou between predicted box and target box\n    ious = InterctionOverUnion(box_preds[obj], target[..., 1:5][obj]).detach()\n\n    object_loss = self.bce(\n        (predictions[..., 0:1][obj]), (ious * target[..., 0:1][obj]) # target * iou because only intersected part object loss calc\n    )\n\n    predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3]) # x, y to be between [0,1]\n    target[..., 3:5] = torch.log(\n        (1e-6 + target[..., 3:5] / anchors)\n    ) # Exponential of hxw (taking log because opp. of exp)\n\n    box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n\n    class_loss = self.entropy(\n        (predictions[..., 5:][obj]), (target[..., 5][obj].long())\n    )\n\n    return(\n        self.lambda_box * box_loss\n        + self.lambda_obj * object_loss\n        + self.lambda_noobj * no_object_loss\n        + self.lambda_class * class_loss\n    )","metadata":{"id":"Ewuf1zjJINfE","execution":{"iopub.status.busy":"2024-08-05T19:57:32.042777Z","iopub.execute_input":"2024-08-05T19:57:32.043021Z","iopub.status.idle":"2024-08-05T19:57:32.055136Z","shell.execute_reply.started":"2024-08-05T19:57:32.042994Z","shell.execute_reply":"2024-08-05T19:57:32.054165Z"},"editable":false,"trusted":true},"execution_count":406,"outputs":[]},{"cell_type":"code","source":"#added own imp\ndef plot_image(image, boxes):\n    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n    cmap = plt.get_cmap(\"tab20b\")\n    class_labels = AllClacess\n    colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n    im = np.array(image)\n    height, width, _ = im.shape\n\n    # Create figure and axes\n    fig, ax = plt.subplots(1)\n    # Display the image\n    ax.imshow(im)\n\n    # Create a Rectangle patch\n    for box in boxes:\n        assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n        class_pred = box[0]\n        box = box[2:]\n        UpperLeft_x = box[0] - box[2] / 2\n        UpperLeft_y = box[1] - box[3] / 2\n        rect = patches.Rectangle(\n            (UpperLeft_x * width, UpperLeft_y * height),\n            box[2] * width,\n            box[3] * height,\n            linewidth=2,\n            edgecolor=colors[int(class_pred)],\n            facecolor=\"none\",\n        )\n        # Add the patch to the Axes\n        ax.add_patch(rect)\n        plt.text(\n            UpperLeft_x * width,\n            UpperLeft_y * height,\n            s=class_labels[int(class_pred)],\n            color=\"white\",\n            verticalalignment=\"top\",\n            bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n        )\n\n    plt.show()\n","metadata":{"id":"k_agKFIIIceJ","execution":{"iopub.status.busy":"2024-08-05T19:57:32.056248Z","iopub.execute_input":"2024-08-05T19:57:32.056506Z","iopub.status.idle":"2024-08-05T19:57:32.068738Z","shell.execute_reply.started":"2024-08-05T19:57:32.056483Z","shell.execute_reply":"2024-08-05T19:57:32.067847Z"},"editable":false,"trusted":true},"execution_count":407,"outputs":[]},{"cell_type":"code","source":"# Instantiate the model\nmodel = YOLOv3(NumClasses=NumClasses).to(DEVICE)\n\n# Compile the model\noptimizer = torch.optim.Adam(\n    model.parameters(), lr=RateOfLearning\n)\nloss_fn = YoloLoss()\n\n# Scaler\nscaler = torch.cuda.amp.GradScaler()\n\n# Train-Test Loader\ntrain_loader, test_loader = get_loaders(\n    train_csv_path='/kaggle/input/pascalvoc-yolo/8examples.csv', test_csv_path='/kaggle/input/pascalvoc-yolo/8examples.csv'\n)\n\n# Anchors\nscaled_anchors = (\n    torch.tensor(ANCHORS) * torch.tensor([13,26,52]).unsqueeze(1).unsqueeze(1).repeat(1,3,2)\n).to(DEVICE)","metadata":{"id":"Cv1Y_uDpjc9j","execution":{"iopub.status.busy":"2024-08-05T19:57:32.069674Z","iopub.execute_input":"2024-08-05T19:57:32.069920Z","iopub.status.idle":"2024-08-05T19:57:32.572988Z","shell.execute_reply.started":"2024-08-05T19:57:32.069896Z","shell.execute_reply":"2024-08-05T19:57:32.572011Z"},"editable":false,"trusted":true},"execution_count":408,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\nfrom tqdm import tqdm\nimport time\n\nhistory_loss = []\n\nfor epoch in tqdm(range(epochsno), desc=\"Epochs\"):\n  model.train()\n\n  losses = []\n\n  start_time = time.time()\n\n  for batch_idx, (x,y) in enumerate(train_loader):\n    x = x.to(DEVICE)\n    y0, y1, y2 = (y[0].to(DEVICE),\n                  y[1].to(DEVICE),\n                  y[2].to(DEVICE))\n\n    with torch.cuda.amp.autocast():\n      out = model(x)\n      loss = (\n          loss_fn(out[0], y0, scaled_anchors[0])\n          + loss_fn(out[1], y1, scaled_anchors[1])\n          + loss_fn(out[2], y2, scaled_anchors[2])\n      )\n\n    losses.append(loss.item())\n\n    optimizer.zero_grad()\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\n  end_time = time.time()\n  epoch_duration = end_time - start_time\n\n  history_loss.append(sum(losses)/len(losses))\n\n  if (epoch+1) % 10 == 0:\n    tqdm.write(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds\")\n\n    print(f\"Epoch [{epoch+1}/{epochsno}], \"\n          f\"Loss: {sum(losses)/len(losses):.4f}\")\n\n    torch.save(model.state_dict(), f'/kaggle/working/Yolov3_epoch{epoch+1}.pth')\n    \n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H7h1wi7LOtO_","outputId":"5389deb4-abf2-4b60-a4b0-2bc11afc4367","execution":{"iopub.status.busy":"2024-08-05T20:00:35.135562Z","iopub.execute_input":"2024-08-05T20:00:35.136031Z","iopub.status.idle":"2024-08-05T20:00:35.148890Z","shell.execute_reply.started":"2024-08-05T20:00:35.135996Z","shell.execute_reply":"2024-08-05T20:00:35.147637Z"},"editable":false,"trusted":true},"execution_count":410,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[410], line 49\u001b[0;36m\u001b[0m\n\u001b[0;31m    wandb.log(\"Epoch\": epoch+1)\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (2118733647.py, line 49)","output_type":"error"}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n\ndef compute_metrics(pred_boxes, true_boxes, iou_threshold=0.5):\n    # Flatten lists\n    pred_boxes = [box for sublist in pred_boxes for box in sublist]\n    true_boxes = [box for sublist in true_boxes for box in sublist]\n    \n    # Initialize counters\n    tp, fp, fn = 0, 0, 0\n    \n    for pred in pred_boxes:\n        pred_class = int(pred[0])  # Assuming class is the first element in the prediction\n        pred_iou = pred[1]  # Assuming IoU is the second element in the prediction\n        if pred_iou > iou_threshold:\n            tp += 1\n        else:\n            fp += 1\n    \n    for true in true_boxes:\n        true_class = int(true[0])\n        if not any(pred[0] == true_class for pred in pred_boxes):\n            fn += 1\n    \n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    return precision, recall, f1\n\ndef get_evaluation_boxx(\n    loader,\n    model,\n    iou_threshold,\n    anchors,\n    num_classes,\n    threshold=0.4,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    verbose=True,\n):\n    model.eval()\n    pred_boxes = []\n    true_boxes = []\n    \n    with torch.no_grad():\n        for images, targets_batch in loader:\n            images = images.to(device)\n            targets_batch = [target.to(device) for target in targets_batch]\n\n            preds = model(images)\n            preds = [\n                pred.view(pred.shape[0], -1, num_classes + 5)\n                for pred in preds\n            ]\n\n            # Apply non-max suppression\n            for i in range(len(preds)):\n                pred = preds[i]\n                preds[i] = non_max_suppression(\n                    pred.cpu().numpy(), iou_threshold, threshold\n                )\n\n            for i, pred in enumerate(preds):\n                pred_boxes.extend(pred)\n\n            for target in targets_batch:\n                target = target.cpu().numpy()\n                true_boxes.extend(target)\n    \n    precision, recall, f1 = compute_metrics(pred_boxes, true_boxes, iou_threshold)\n    \n    if verbose:\n        print(f'Precision: {precision:.4f}')\n        print(f'Recall: {recall:.4f}')\n        print(f'F1 Score: {f1:.4f}')\n    \n    wandb.log({\"precision\": precision, \"recall\": recall, \"f1_score\": f1})\n    \n    return pred_boxes, true_boxes\n\n# Example usage\ntrain_loader, test_loader = get_loaders(train_csv_path, test_csv_path)\nevaluate_and_log(model, test_loader, device)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T19:57:32.587733Z","iopub.status.idle":"2024-08-05T19:57:32.588119Z","shell.execute_reply.started":"2024-08-05T19:57:32.587912Z","shell.execute_reply":"2024-08-05T19:57:32.587928Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inference on a single image\ndef infer_image(image_path):\n    model.eval()\n    image = Image.open(image_path).convert('L')  # Convert to grayscale\n    image = transform(image).unsqueeze(0)  # Add batch dimension\n    with torch.no_grad():\n        output = model(image)\n        _, pred = torch.max(output, 1)\n        print(f'Predicted class: {pred.item()}')\n\n# Example usage for inference\ninfer_image('/kaggle/input/pascalvoc-yolo/images/000007.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-08-05T19:57:32.589601Z","iopub.status.idle":"2024-08-05T19:57:32.589935Z","shell.execute_reply.started":"2024-08-05T19:57:32.589770Z","shell.execute_reply":"2024-08-05T19:57:32.589786Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, recall_score\n\n# Function to perform inference\ndef perform_inference(model, dataloader, device):\n    model.eval()\n    predictions = []\n    ground_truths = []\n    \n    with torch.no_grad():\n        for images, targets in dataloader:\n            images = [image.to(device) for image in images]\n            outputs = model(images)\n            \n            # Process outputs to get predictions\n            for output in outputs:\n                # Extract bounding boxes, confidences, etc.\n                # You need to adjust this part according to your model's output format\n                pred_boxes = output['boxes'].cpu().numpy()\n                pred_labels = output['labels'].cpu().numpy()\n                pred_scores = output['scores'].cpu().numpy()\n                \n                # Collect predictions\n                for box, label, score in zip(pred_boxes, pred_labels, pred_scores):\n                    if score > 0.5:  # Confidence threshold\n                        predictions.append((box, label, score))\n            \n            # Collect ground truths\n            for target in targets:\n                gt_boxes = target['boxes'].cpu().numpy()\n                gt_labels = target['labels'].cpu().numpy()\n                for box, label in zip(gt_boxes, gt_labels):\n                    ground_truths.append((box, label))\n\n    return predictions, ground_truths\n\n# Function to compute metrics\ndef calculate_metrics(predictions, ground_truths):\n    # Flatten predictions and ground_truths\n    pred_boxes = np.array([box for box, _, _ in predictions])\n    pred_labels = np.array([label for _, label, _ in predictions])\n    gt_boxes = np.array([box for box, _ in ground_truths])\n    gt_labels = np.array([label for _, label in ground_truths])\n    \n    # Calculate true positives, false positives, false negatives\n    # Implement matching logic between predictions and ground truths\n    # This example assumes binary classification for simplicity\n    y_true = np.zeros(len(gt_labels))\n    y_pred = np.zeros(len(pred_labels))\n    \n    # Example matching logic (this should be more sophisticated in practice)\n    for i, label in enumerate(pred_labels):\n        if label in gt_labels:\n            y_pred[i] = 1\n    for i, label in enumerate(gt_labels):\n        if label in pred_labels:\n            y_true[i] = 1\n    \n    recall = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    \n    return f1, recall\n\n# Log metrics to wandb\nwandb.log({'f1_score': f1, 'recall': recall})","metadata":{"execution":{"iopub.status.busy":"2024-08-05T19:57:32.591207Z","iopub.status.idle":"2024-08-05T19:57:32.591548Z","shell.execute_reply.started":"2024-08-05T19:57:32.591380Z","shell.execute_reply":"2024-08-05T19:57:32.591396Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image, ImageFile\nfrom collections import Counter\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport wandb  # Import wandb\n\n# Define configuration for YOLOv3\nconfig = [\n    (32, 3, 1),\n    (128, 3, 1),\n    (64, 3, 2),\n    [\"list\", 1],\n    (128, 3, 2),\n    [\"list\", 2],\n    (256, 3, 2),\n    [\"list\", 8],\n    (512, 3, 2),\n    [\"list\", 8],\n    (1024, 3, 2),\n    [\"list\", 4],\n    (512, 1, 1),\n    (1024, 3, 1),\n    \"sp\",\n    (256, 1, 1),\n    \"up\",\n    (256, 1, 1),\n    (512, 3, 1),\n    \"sp\",\n    (128, 1, 1),\n    \"up\",\n    (128, 1, 1),\n    (256, 3, 1),\n    \"sp\",\n]\n\n# Define CNN block\nclass CNN_Block(nn.Module):\n    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n        super(CNN_Block, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.leaky = nn.LeakyReLU(0.1)\n        self.use_bn_act = bn_act\n\n    def forward(self, x):\n        if self.use_bn_act:\n            return self.leaky(self.bn(self.conv(x)))\n        else:\n            return self.conv(x)\n\n# Define Residual block\nclass Residual_Block(nn.Module):\n    def __init__(self, channels, use_residual=True, num_repeats=1):\n        super(Residual_Block, self).__init__()\n        self.layers = nn.ModuleList()\n\n        for repeat in range(num_repeats):\n            self.layers += [\n                nn.Sequential(\n                    CNN_Block(channels, channels//2, kernel_size=1),\n                    CNN_Block(channels//2, channels, kernel_size=3, padding=1)\n                )\n            ]\n\n        self.use_residual = use_residual\n        self.num_repeats = num_repeats\n\n    def forward(self, x):\n        for layer in self.layers:\n            if self.use_residual:\n                x = x + layer(x)\n            else:\n                x = layer(x)\n\n        return x\n\n# Define Prediction Scale\nclass Prediction_Scale(nn.Module):\n    def __init__(self, in_channels, NumClasses):\n        super(Prediction_Scale, self).__init__()\n        self.pred = nn.Sequential(\n            CNN_Block(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n            CNN_Block(2 * in_channels, (NumClasses + 5) * 3, bn_act=False, kernel_size=1),\n        )\n        self.NumClasses = NumClasses\n\n    def forward(self, x):\n        return (\n            self.pred(x)\n            .reshape(x.shape[0], 3, self.NumClasses + 5, x.shape[2], x.shape[3])\n            .permute(0, 1, 3, 4, 2)\n        )\n\nclass YOLOv3(nn.Module):\n  def __init__(self, in_channels=3, NumClasses=20):\n    super(YOLOv3, self).__init__()\n    self.NumClasses = NumClasses\n    self.in_channels = in_channels\n    self.layers = self._create_conv_layers()\n\n  def forward(self, x):\n    outputs = []\n    route_connections = []\n\n    for layer in self.layers:\n      if isinstance(layer, Prediction_Scale):\n        outputs.append(layer(x))\n        continue\n\n      x = layer(x)\n\n      if isinstance(layer, Residual_Block) and layer.num_repeats == 8:\n        route_connections.append(x)\n\n      elif isinstance(layer, nn.Upsample):\n        x = torch.cat([x, route_connections[-1]], dim=1)\n        route_connections.pop()\n\n    return outputs\n\n  def _create_conv_layers(self):\n    layers = nn.ModuleList()\n    in_channels = self.in_channels\n\n    for module in config:\n      if isinstance(module, tuple):\n        out_channels, kernel_size, stride = module\n        layers.append(CNN_Block(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=1 if kernel_size == 3 else 0\n        ))\n        in_channels = out_channels\n\n      elif isinstance(module, list):\n        num_repeats = module[1]\n        layers.append(Residual_Block(in_channels, num_repeats=num_repeats))\n\n      elif isinstance(module, str):\n        if module == \"sp\":\n          layers += [\n              Residual_Block(in_channels, use_residual=False, num_repeats=1),\n              CNN_Block(in_channels, in_channels//2, kernel_size=1),\n              Prediction_Scale(in_channels//2, NumClasses = self.NumClasses)\n          ]\n          in_channels = in_channels // 2\n\n        elif module == \"up\":\n          layers.append(nn.Upsample(scale_factor=2))\n          in_channels = in_channels * 3\n\n    return layers\n\nNumClasses = 20\nImageSize = 416\nmodel = YOLOv3(NumClasses=NumClasses)\nx = torch.randn((2, 3, ImageSize, ImageSize))\nout = model(x)\nassert model(x)[0].shape == (2, 3, ImageSize//32, ImageSize//32, NumClasses + 5)\nassert model(x)[1].shape == (2, 3, ImageSize//16, ImageSize//16, NumClasses + 5)\nassert model(x)[2].shape == (2, 3, ImageSize//8, ImageSize//8, NumClasses + 5)\nprint(\"Success!\")\n\n# Define utility functions\ndef WeidthHeight(boxa, boxb):\n    intersection = torch.min(boxa[..., 0], boxb[..., 0]) * torch.min(\n        boxa[..., 1], boxb[..., 1]\n    )\n    union = (\n        boxa[..., 0] * boxa[..., 1] + boxb[..., 0] * boxb[..., 1] - intersection\n    )\n    return intersection / union\n\ndef non_max_suppression(boxx, iou_threshold, threshold, box_format=\"corners\"):\n    assert type(boxx) == list\n\n    boxx = [box for box in boxx if box[1] > threshold]\n    boxx = sorted(boxx, key=lambda x: x[1], reverse=True)\n    boxx_after_nms = []\n\n    while boxx:\n        chosen_box = boxx.pop(0)\n\n        boxx = [\n            box\n            for box in boxx\n            if box[0] != chosen_box[0]\n            or InterctionOverUnion(\n                torch.tensor(chosen_box[2:]),\n                torch.tensor(box[2:]),\n                box_format=box_format,\n            )\n            < iou_threshold\n        ]\n\n        boxx_after_nms.append(chosen_box)\n\n    return boxx_after_nms\n\ndef InterctionOverUnion(PredsBox, lableBox, box_format=\"midpoint\"):\n    if box_format == \"midpoint\":\n        box1_a1 = PredsBox[..., 0:1] - PredsBox[..., 2:3] / 2\n        box1_b1 = PredsBox[..., 1:2] - PredsBox[..., 3:4] / 2\n        box1_a2 = PredsBox[..., 0:1] + PredsBox[..., 2:3] / 2\n        box1_b2 = PredsBox[..., 1:2] + PredsBox[..., 3:4] / 2\n        box2_a1 = lableBox[..., 0:1] - lableBox[..., 2:3] / 2\n        box2_y1 = lableBox[..., 1:2] - lableBox[..., 3:4] / 2\n        box2_a2 = lableBox[..., 0:1] + lableBox[..., 2:3] / 2\n        box2_y2 = lableBox[..., 1:2] + lableBox[..., 3:4] / 2\n\n    if box_format == \"corners\":\n        box1_a1 = PredsBox[..., 0:1]\n        box1_b1 = PredsBox[..., 1:2]\n        box1_a2 = PredsBox[..., 2:3]\n        box1_b2 = PredsBox[..., 3:4]\n        box2_a1 = lableBox[..., 0:1]\n        box2_y1 = lableBox[..., 1:2]\n        box2_a2 = lableBox[..., 2:3]\n        box2_y2 = lableBox[..., 3:4]\n\n    x1 = torch.max(box1_a1, box2_a1)\n    y1 = torch.max(box1_b1, box2_y1)\n    x2 = torch.min(box1_a2, box2_a2)\n    y2 = torch.min(box1_b2, box2_y2)\n    intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n    box1_area = (box1_a2 - box1_a1) * (box1_b2 - box1_b1)\n    box2_area = (box2_a2 - box2_a1) * (box2_y2 - box2_y1)\n    iou = intersection / (box1_area + box2_area - intersection)\n\n    return iou\n\n# allows PIL to load images even if they are truncated or incomplete\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n#added own imp\nclass YOLODataset(Dataset):\n  def __init__(self, csv_file, ImgDir, LableDir, anchors,\n               ImageSize=416, sp=[13,26,52], cp=20, transform=None):\n    self.annotations = pd.read_csv(csv_file)\n    self.ImgDir = ImgDir\n    self.LableDir = LableDir\n    self.transform = transform\n    self.sp = sp\n\n    self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2]) # For all 3 scales\n    self.num_anchors = self.anchors.shape[0]\n    self.num_anchors_per_scale = self.num_anchors // 3\n\n    self.cp = cp\n\n    self.ignore_iou_thresh = 0.5\n\n  def __len__(self):\n    return len(self.annotations)\n\n  def __getitem__(self, index):\n    label_path = os.path.join(self.LableDir, self.annotations.iloc[index, 1])\n    boxx = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist() # np.roll with shift 4 on axis 1: [class, x, y, w, h] --> [x, y, w, h, class]\n\n    img_path = os.path.join(self.ImgDir, self.annotations.iloc[index, 0])\n    image = Image.open(img_path)\n\n    if self.transform:\n      image = self.transform(image)\n\n    targets = [torch.zeros((self.num_anchors // 3, sp, sp, 6)) for sp in self.sp] # 6 because objectness score, bounding box coordinates (x, y, w, h), class label\n\n    for box in boxx:\n      iou_anchors = WeidthHeight(torch.tensor(box[2:4]), self.anchors) # IOU from height and width\n      anchor_indices = iou_anchors.argsort(descending=True, dim=0) # Sorting sucht that the first is the best anchor\n\n      x, y, width, height, class_label = box\n      has_anchor = [False, False, False] # Make sure there is an anchor for each of three scales for each bounding box\n\n      for anchor_idx in anchor_indices:\n        scale_idx = anchor_idx // self.num_anchors_per_scale # scale_idx is either 0,1,2: 0-->13x13, 1:-->26x26, 2:-->52x52\n        anchor_on_scale = anchor_idx % self.num_anchors_per_scale # In each scale, choosing the anchor thats either 0,1,2\n\n        sp = self.sp[scale_idx]\n        i, j = int(sp*y), int(sp*x) # x=0.5, sp=13 --> int(6.5) = 6 | i=y cell, j=x cell\n        anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n\n        if not anchor_taken and not has_anchor[scale_idx]:\n          targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n          x_cell, y_cell = sp*x - j, sp*y - i # 6.5 - 6 = 0.5 such that they are between [0,1]\n          width_cell, height_cell = (\n              width*sp, # sp=13, width=0.5, 6.5\n              height*sp\n          )\n\n          box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n\n          targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n          targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n          has_anchor[scale_idx] = True\n\n        # Even if the same grid shares another anchor having iou>ignore_iou_thresh then,\n        elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n          targets[scale_idx][anchor_on_scale, i, j, 0] = -1 # ignore this prediction\n\n    return image, tuple(targets)\n  \n  import torchvision.transforms as transforms\ntransform = transforms.Compose([transforms.Resize((416, 416)), transforms.ToTensor()])\n\n#added own imp\ndef get_loaders(train_csv_path, test_csv_path):\n\n    train_dataset = YOLODataset(\n        train_csv_path,\n        transform=transform,\n        sp=[ImageSize // 32, ImageSize // 16, ImageSize // 8],\n        ImgDir=DirImage,\n        LableDir=DirLable,\n        anchors=ANCHORS,\n    )\n    test_dataset = YOLODataset(\n        test_csv_path,\n        transform=transform,\n        sp=[ImageSize // 32, ImageSize // 16, ImageSize // 8],\n        ImgDir=DirImage,\n        LableDir=DirLable,\n        anchors=ANCHORS,\n    )\n    train_loader = DataLoader(\n        dataset=train_dataset,\n        batch_size=SizeOfBatch,\n        shuffle=True,\n        drop_last=False,\n    )\n    test_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=SizeOfBatch,\n        shuffle=False,\n        drop_last=False,\n    )\n\n    return train_loader, test_loader\n\ndef mean_average_precision(\n    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", NumClasses=4\n):\n\n    # list storing all AP for respective classes\n    average_precisions = []\n\n    # used for numerical stability later on\n    epsilon = 1e-6\n\n    for c in range(NumClasses):\n        detections = []\n        ground_truths = []\n\n        for detection in pred_boxes:\n            if detection[1] == c:\n                detections.append(detection)\n\n        for true_box in true_boxes:\n            if true_box[1] == c:\n                ground_truths.append(true_box)\n\n        amount_boxx = Counter([gt[0] for gt in ground_truths])\n\n        for key, val in amount_boxx.items():\n            amount_boxx[key] = torch.zeros(val)\n\n        # sort by box probabilities which is index 2\n        detections.sort(key=lambda x: x[2], reverse=True)\n        TP = torch.zeros((len(detections)))\n        FP = torch.zeros((len(detections)))\n        total_true_boxx = len(ground_truths)\n\n        # If none exists for this class then we can safely skip\n        if total_true_boxx == 0:\n            continue\n\n        for detection_idx, detection in enumerate(detections):\n            ground_truth_img = [\n                bbox for bbox in ground_truths if bbox[0] == detection[0]\n            ]\n\n            num_gts = len(ground_truth_img)\n            best_iou = 0\n\n            for idx, gt in enumerate(ground_truth_img):\n                iou = InterctionOverUnion(\n                    torch.tensor(detection[3:]),\n                    torch.tensor(gt[3:]),\n                    box_format=box_format,\n                )\n\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_idx = idx\n\n            if best_iou > iou_threshold:\n                # only detect ground truth detection once\n                if amount_boxx[detection[0]][best_gt_idx] == 0:\n                    # true positive and add this bounding box to seen\n                    TP[detection_idx] = 1\n                    amount_boxx[detection[0]][best_gt_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n\n            # if IOU is lower then the detection is a false positive\n            else:\n                FP[detection_idx] = 1\n\n        TP_cumsum = torch.cumsum(TP, dim=0)\n        FP_cumsum = torch.cumsum(FP, dim=0)\n        recalls = TP_cumsum / (total_true_boxx + epsilon)\n        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n        # torch.trapz for numerical integration\n        average_precisions.append(torch.trapz(precisions, recalls))\n\n    return sum(average_precisions) / len(average_precisions)\n\n#added own\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nWorkersNo = 4\nSizeOfBatch = 32\nImageSize = 416\nClassesNo = 20\nRateOfLearning = 1e-5\nepochsno = 150\nThresholdConf = 0.8\nThreshMap = 0.5\nThreshNms = 0.45\nsp = [ImageSize // 32, ImageSize // 16, ImageSize // 8]\n\nDirImage = \"/kaggle/input/pascalvoc-yolo/images\"\nDirLable = \"/kaggle/input/pascalvoc-yolo/labels\"\n\nANCHORS = [\n    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n]\n\n\nAllClacess = [\n    \"aeroplane\",\n    \"bicycle\",\n    \"bird\",\n    \"boat\",\n    \"bottle\",\n    \"bus\",\n    \"car\",\n    \"cat\",\n    \"chair\",\n    \"cow\",\n    \"diningtable\",\n    \"dog\",\n    \"horse\",\n    \"motorbike\",\n    \"person\",\n    \"pottedplant\",\n    \"sheep\",\n    \"sofa\",\n    \"train\",\n    \"tvmonitor\"\n]\n\ndef compute_metrics(pred_boxes, true_boxes, iou_threshold=0.5):\n    # Your implementation for computing precision, recall, and f1\n    pred_classes = [box[0] for box in pred_boxes]\n    pred_iou = [box[1] for box in pred_boxes]\n    \n    tp, fp, fn = 0, 0, 0\n    \n    for pred, iou in zip(pred_classes, pred_iou):\n        if iou > iou_threshold:\n            tp += 1\n        else:\n            fp += 1\n    \n    true_classes = [box[0] for box in true_boxes]\n    for true in true_classes:\n        if not any(pred == true for pred in pred_classes):\n            fn += 1\n    \n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    return precision, recall, f1\n\ndef get_evaluation_boxx(\n    loader,\n    model,\n    iou_threshold,\n    anchors,\n    threshold,\n    box_format=\"midpoint\"\n):\n\n    # make sure model is in eval before get boxx\n    model.eval()\n    train_idx = 0\n    all_pred_boxes = []\n    all_true_boxes = []\n    for batch_idx, (x, labels) in enumerate(loader):\n        x = x.float().to(DEVICE)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size = x.shape[0]\n        boxx = [[] for _ in range(batch_size)]\n        for i in range(3):\n            sp = predictions[i].shape[2] # grid cell size for each predictions\n            anchor = torch.tensor([*anchors[i]]).to(DEVICE) * sp # anchor for each grid, prediction type\n            boxes_scale_i = cells_to_boxx( # get boxx for each image in the batch\n                predictions[i], anchor, sp=sp, is_preds=True\n            )\n            for idx, (box) in enumerate(boxes_scale_i): # for each image, append the bbox to corr. boxx[idx]\n                boxx[idx] += box\n\n        # we just want one bbox for each label, not one for each scale\n        true_boxx = cells_to_boxx(\n            labels[2], anchor, sp=sp, is_preds=False\n        )\n\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                boxx[idx],\n                iou_threshold=iou_threshold,\n                threshold=threshold,\n                box_format=box_format,\n            )\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_boxx[idx]:\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            # Apply non-max suppression\n            for i in range(len(preds)):\n                pred = preds[i]\n                preds[i] = non_max_suppression(\n                    pred.cpu().numpy(), iou_threshold, threshold\n                )\n\n            for i, pred in enumerate(preds):\n                all_pred_boxes.extend(pred)\n\n            for target in targets_batch:\n                target = target.cpu().numpy()\n                all_true_boxes.extend(target)\n                \n            precision, recall, f1 = compute_metrics(all_pred_boxes, all_true_boxes, iou_threshold)\n    \n    # Collect results for logging\n    results = {\n        \"all_pred_boxes\": all_pred_boxes,\n        \"all_true_boxes\": all_true_boxes,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    }\n    \n    return results\n\n    model.train()\n    return all_pred_boxes, all_true_boxes\n\ndef compute_metrics(pred_boxes, true_boxes, iou_threshold=0.5):\n    # Your implementation for computing precision, recall, and f1\n    pred_classes = [box[0] for box in pred_boxes]\n    pred_iou = [box[1] for box in pred_boxes]\n    \n    tp, fp, fn = 0, 0, 0\n    \n    for pred, iou in zip(pred_classes, pred_iou):\n        if iou > iou_threshold:\n            tp += 1\n        else:\n            fp += 1\n    \n    true_classes = [box[0] for box in true_boxes]\n    for true in true_classes:\n        if not any(pred == true for pred in pred_classes):\n            fn += 1\n    \n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    return precision, recall, f1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    #added own imp\ndef cells_to_boxx(predictions, anchors, sp, is_preds=True):\n\n    SizeOfBatch = predictions.shape[0]\n    num_anchors = len(anchors)\n    box_predictions = predictions[..., 1:5]\n    if is_preds:\n        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n        scores = torch.sigmoid(predictions[..., 0:1])\n        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n    else:\n        scores = predictions[..., 0:1]\n        best_class = predictions[..., 5:6]\n\n    cell_indices = (\n        torch.arange(sp)\n        .repeat(predictions.shape[0], 3, sp, 1)\n        .unsqueeze(-1)\n        .to(predictions.DEVICE)\n    )\n    x = 1 / sp * (box_predictions[..., 0:1] + cell_indices)\n    y = 1 / sp * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n    w_h = 1 / sp * box_predictions[..., 2:4]\n    converted_boxx = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(SizeOfBatch, num_anchors * sp * sp, 6)\n    return converted_boxx.tolist()\n\n#added own imp\nclass YoloLoss(nn.Module):\n  def __init__(self):\n    super(YoloLoss, self).__init__()\n    self.mse = nn.MSELoss() # For bounding box loss\n    self.bce = nn.BCEWithLogitsLoss() # For multi-label prediction: Binary cross entropy\n    self.entropy = nn.CrossEntropyLoss() # For classification\n    self.sigmoid = nn.Sigmoid()\n\n    # Constants for significance of obj, or no obj.\n    self.lambda_class = 1\n    self.lambda_noobj = 10\n    self.lambda_obj = 1\n    self.lambda_box = 10\n\n  def forward(self, predictions, target, anchors):\n    obj = target[..., 0] == 1\n    noobj = target[..., 0] == 0\n\n    no_object_loss = self.bce(\n        (predictions[..., 0:1][noobj]), (target[..., 0:1][noobj])\n    )\n\n    anchors = anchors.reshape(1,3,1,1,2) # Anchors initial shape 3x2 --> 3 anchor boxes each of certain hxw (2)\n\n    # box_preds = [..., sigmoid(x), sigmoid(y), [p_w * exp(t_w)], [p_h * exp(t_h)], ...]\n    box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n\n    # iou between predicted box and target box\n    ious = InterctionOverUnion(box_preds[obj], target[..., 1:5][obj]).detach()\n\n    object_loss = self.bce(\n        (predictions[..., 0:1][obj]), (ious * target[..., 0:1][obj]) # target * iou because only intersected part object loss calc\n    )\n\n    predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3]) # x, y to be between [0,1]\n    target[..., 3:5] = torch.log(\n        (1e-6 + target[..., 3:5] / anchors)\n    ) # Exponential of hxw (taking log because opp. of exp)\n\n    box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n\n    class_loss = self.entropy(\n        (predictions[..., 5:][obj]), (target[..., 5][obj].long())\n    )\n\n    return(\n        self.lambda_box * box_loss\n        + self.lambda_obj * object_loss\n        + self.lambda_noobj * no_object_loss\n        + self.lambda_class * class_loss\n    )\n  \n  #added own imp\ndef plot_image(image, boxes):\n    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n    cmap = plt.get_cmap(\"tab20b\")\n    class_labels = AllClacess\n    colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n    im = np.array(image)\n    height, width, _ = im.shape\n\n    # Create figure and axes\n    fig, ax = plt.subplots(1)\n    # Display the image\n    ax.imshow(im)\n\n    # Create a Rectangle patch\n    for box in boxes:\n        assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n        class_pred = box[0]\n        box = box[2:]\n        UpperLeft_x = box[0] - box[2] / 2\n        UpperLeft_y = box[1] - box[3] / 2\n        rect = patches.Rectangle(\n            (UpperLeft_x * width, UpperLeft_y * height),\n            box[2] * width,\n            box[3] * height,\n            linewidth=2,\n            edgecolor=colors[int(class_pred)],\n            facecolor=\"none\",\n        )\n        # Add the patch to the Axes\n        ax.add_patch(rect)\n        plt.text(\n            UpperLeft_x * width,\n            UpperLeft_y * height,\n            s=class_labels[int(class_pred)],\n            color=\"white\",\n            verticalalignment=\"top\",\n            bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n        )\n\n    plt.show()\n\n# Instantiate the model\nmodel = YOLOv3(NumClasses=NumClasses).to(DEVICE)\n\n# Compile the model\noptimizer = torch.optim.Adam(\n    model.parameters(), lr=RateOfLearning\n)\nloss_fn = YoloLoss()\n\n# Scaler\nscaler = torch.cuda.amp.GradScaler()\n\n# Train-Test Loader\ntrain_loader, test_loader = get_loaders(\n    train_csv_path='/kaggle/input/pascalvoc-yolo/8examples.csv', test_csv_path='/kaggle/input/pascalvoc-yolo/8examples.csv'\n)\n\n# Anchors\nscaled_anchors = (\n    torch.tensor(ANCHORS) * torch.tensor([13,26,52]).unsqueeze(1).unsqueeze(1).repeat(1,3,2)\n).to(DEVICE)\n\nimport torch.optim as optim\n\nfrom tqdm import tqdm\nimport time\n\nhistory_loss = []\n\nfor epoch in tqdm(range(epochsno), desc=\"Epochs\"):\n  model.train()\n\n  losses = []\n\n  start_time = time.time()\n\n  for batch_idx, (x,y) in enumerate(train_loader):\n    x = x.to(DEVICE)\n    y0, y1, y2 = (y[0].to(DEVICE),\n                  y[1].to(DEVICE),\n                  y[2].to(DEVICE))\n\n    with torch.cuda.amp.autocast():\n      out = model(x)\n      loss = (\n          loss_fn(out[0], y0, scaled_anchors[0])\n          + loss_fn(out[1], y1, scaled_anchors[1])\n          + loss_fn(out[2], y2, scaled_anchors[2])\n      )\n\n    losses.append(loss.item())\n\n    optimizer.zero_grad()\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\n  end_time = time.time()\n  epoch_duration = end_time - start_time\n\n  history_loss.append(sum(losses)/len(losses))\n\n  if (epoch+1) % 10 == 0:\n    tqdm.write(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds\")\n\n    print(f\"Epoch [{epoch+1}/{epochsno}], \"\n          f\"Loss: {sum(losses)/len(losses):.4f}\")\n\n    torch.save(model.state_dict(), f'/kaggle/working/Yolov3_epoch{epoch+1}.pth')","metadata":{"execution":{"iopub.status.busy":"2024-08-05T21:53:25.032957Z","iopub.execute_input":"2024-08-05T21:53:25.033326Z","iopub.status.idle":"2024-08-05T21:54:27.558828Z","shell.execute_reply.started":"2024-08-05T21:53:25.033298Z","shell.execute_reply":"2024-08-05T21:54:27.557659Z"},"editable":false,"trusted":true},"execution_count":415,"outputs":[{"name":"stdout","text":"Success!\n","output_type":"stream"},{"name":"stderr","text":"Epochs:   6%|         | 9/150 [00:03<00:44,  3.16it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 10 completed in 0.31 seconds\nEpoch [10/150], Loss: 48.0097\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  13%|        | 19/150 [00:07<00:42,  3.08it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 20 completed in 0.32 seconds\nEpoch [20/150], Loss: 30.2520\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  19%|        | 29/150 [00:10<00:39,  3.06it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 30 completed in 0.31 seconds\nEpoch [30/150], Loss: 25.7399\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  26%|       | 39/150 [00:14<00:35,  3.12it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 40 completed in 0.31 seconds\nEpoch [40/150], Loss: 23.3626\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  33%|      | 49/150 [00:18<00:32,  3.09it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 50 completed in 0.31 seconds\nEpoch [50/150], Loss: 21.7259\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  39%|      | 59/150 [00:21<00:29,  3.07it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 60 completed in 0.31 seconds\nEpoch [60/150], Loss: 20.4254\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  46%|     | 69/150 [00:25<00:26,  3.09it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 70 completed in 0.31 seconds\nEpoch [70/150], Loss: 19.3385\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  53%|    | 79/150 [00:29<00:23,  3.08it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 80 completed in 0.32 seconds\nEpoch [80/150], Loss: 18.4925\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  59%|    | 89/150 [00:32<00:19,  3.06it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 90 completed in 0.32 seconds\nEpoch [90/150], Loss: 17.6992\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  66%|   | 99/150 [00:36<00:16,  3.07it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 100 completed in 0.32 seconds\nEpoch [100/150], Loss: 16.9769\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  73%|  | 109/150 [00:40<00:13,  3.02it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 110 completed in 0.32 seconds\nEpoch [110/150], Loss: 16.3433\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  79%|  | 119/150 [00:43<00:10,  3.08it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 120 completed in 0.33 seconds\nEpoch [120/150], Loss: 15.7888\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  86%| | 129/150 [00:48<00:07,  2.97it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 130 completed in 0.32 seconds\nEpoch [130/150], Loss: 15.3267\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  93%|| 139/150 [00:51<00:03,  3.03it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 140 completed in 0.32 seconds\nEpoch [140/150], Loss: 14.9371\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  99%|| 149/150 [00:55<00:00,  3.01it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 150 completed in 0.32 seconds\nEpoch [150/150], Loss: 14.4735\n","output_type":"stream"},{"name":"stderr","text":"Epochs: 100%|| 150/150 [00:56<00:00,  2.67it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"    \n# Login with the API KEY\nwandb.login(key=\"ab35ea8191eba471c2b58a844910531625b00550\")\n\n# Initialize wandb\nwandb.init(project=\"Untitled10\", entity=\"mblogge785-work\")  # Replace with your wandb username\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T22:17:50.959092Z","iopub.execute_input":"2024-08-05T22:17:50.959798Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016670422333360572, max=1.0","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06b3615a4075469499847fc514453495"}},"metadata":{}}]},{"cell_type":"code","source":"\n# Create WandB tables\npred_table = wandb.Table(columns=[\"Predicted Class\", \"IoU\"])\ntrue_table = wandb.Table(columns=[\"True Class\", \"IoU\"])\nfor pred in results['pred_boxes']:\n    pred_table.add_data(pred[0], pred[1])  # Assuming format [class, IoU]\n\nfor true in results['true_boxes']:\n    true_table.add_data(true[0], true[1])  # Assuming format [class, IoU]\n\n# Log tables and metrics to WandB\nwandb.log({\n    \"Predictions\": pred_table,\n    \"Ground Truth\": true_table,\n    \"Precision\": results['precision'],\n    \"Recall\": results['recall'],\n    \"F1 Score\": results['f1'],\n    \"train_loss\": avg_loss,\n    \"epoch\": epoch + 1,\n    \"epoch_duration_seconds\": epoch_duration\n})","metadata":{"execution":{"iopub.status.busy":"2024-08-05T22:01:32.006945Z","iopub.execute_input":"2024-08-05T22:01:32.007339Z","iopub.status.idle":"2024-08-05T22:01:32.051601Z","shell.execute_reply.started":"2024-08-05T22:01:32.007308Z","shell.execute_reply":"2024-08-05T22:01:32.050055Z"},"editable":false,"trusted":true},"execution_count":423,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[423], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m pred_table \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mTable(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Class\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIoU\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m true_table \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mTable(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue Class\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIoU\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresults\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_boxes\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      5\u001b[0m     pred_table\u001b[38;5;241m.\u001b[39madd_data(pred[\u001b[38;5;241m0\u001b[39m], pred[\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Assuming format [class, IoU]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m true \u001b[38;5;129;01min\u001b[39;00m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_boxes\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n","\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"],"ename":"NameError","evalue":"name 'results' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Assuming `cells_to_boxx` and `non_max_suppression` are defined elsewhere in your code.\n# Ensure `plot_image` is also defined, which plots the image with bounding boxes.\n\ndef load_image(image_path, transform=None):\n    image = Image.open(image_path).convert(\"RGB\")\n    if transform:\n        image = transform(image)\n    return image\n\ndef plot_image(image, boxes):\n    plt.figure(figsize=(12, 8))\n    plt.imshow(image)\n    ax = plt.gca()\n\n    for box in boxes:\n        # Extract the coordinates\n        x1, y1, x2, y2 = box[1:5]  # Adjust index based on your box format\n\n        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n                             fill=False, color='red', linewidth=2)\n        ax.add_patch(rect)\n\n    plt.show()\n\n# Define the image transformation\ntransform = transforms.Compose([\n    transforms.ToTensor()\n])\n\n# Path to the specific image\nimage_path = '/kaggle/input/pascalvoc-yolo/images/000007.jpg'  # Replace with your image path\n\n# Load and preprocess the image\nimage = load_image(image_path, transform).unsqueeze(0).to(DEVICE)\n\n# Perform inference\nmodel.eval()\nwith torch.no_grad():\n    out = model(image)\n    boxx = [[] for _ in range(image.shape[0])]\n    batch_size, A, sp, _, _ = out[0].shape\n    anchor = torch.tensor([*ANCHORS[0]]).to(DEVICE) * sp\n    boxes_scale_i = cells_to_boxx(\n        out[0], anchor, sp=sp, is_preds=True\n    )\n    for idx, (box) in enumerate(boxes_scale_i):\n        boxx[idx] += box\n\n    for i in range(batch_size):\n        nms_boxes = non_max_suppression(\n            boxx[i], iou_threshold=0.5, threshold=0.6, box_format=\"midpoint\",\n        )\n        plot_image(image[0].permute(1, 2, 0).detach().cpu(), nms_boxes)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]}]}